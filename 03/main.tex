\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{2}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4451_f25/}{Mathematical Statistics II}\\ \vspace{2mm}
Maximum Likelihood Estimation}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]




\begin{document}

\maketitle

\mode<article>{\tableofcontents}

% \mode<presentation>{
%   \begin{frame}{Outline}
%     \tableofcontents
%   \end{frame}
% }

\section{Introduction}

\begin{frame}[allowframebreaks]{Overview}
  \begin{itemize}
    \item The next approach we will discuss is Maximum Likelihood Estimation (MLE).
    \item As we will see, the MLE has several desirable properties, and as a result is often favored over approaches like the method of moments.
    \item The material for this section largely comes from Chapter~8.5 of \citet{rice07}, and various sections in \citet{pawitan01}.
    \end{itemize}
\end{frame}

\section{Likelihood: an introduciton}

\begin{frame}[allowframebreaks,fragile]{What is likelihood?}

\begin{itemize}
  \item The term ``likelihood" is often used colloquially to mean something analogous to probability. E.g., ``What is the likelihood that it rains tomorrow?"
  \item When we use this term in statistics / mathematics, we mean something specific that isn't the same thing as probability.
  \item The use of the term ``likelihood" was first made by R. A. Fisher, who was the architect and primary proponent of ``likelihood-based-inference".
  \item We will start with the treatment of likelihood in the text ``In all Likelihood" \citep{pawitan01}, which is a fantastic resource on the subject. (This will lead to some review...)
\end{itemize}

\framebreak

\begin{exampleblock}{Coin Flips}
  We will revisit this example, as it is a great starting point to connect with existing understanding.

  Consider flipping a coin $N = 10$ times.

  \mode<article>{
  \begin{itemize}
    \item In a probability class, we might assume the probability of heads $\theta = p$ is some number, say $p = 0.4$.
    \item If $X$ is the number of heads, then $P_\theta(X = 8) = 0.011$ is something we can calculate exactly. In any interpretation of probability, this means that if we repeat the experiment 10,000 time, we expect around 110 times, the experiment will have 8 heads.
    \item Now what if, in a single experiment, we observe $X = 8$? \textbf{Based on this information alone}, what can we say about the parameter $p$? Information we have about $p$ is \emph{incomplete}(we've already done a pure frequentist interpretation of this, as well as a Bayesian interpretation. The MoM estimate is also easy to compute).
    \item However, we can conclude that $p$ \textbf{cannot be zero or one}. This fact is available \emph{deductively}, since if $p$ were zero or one, then $X = 0$ or $X = 10$.
    \item Similarly, we are fairly certain that $p$ is not close to zero, since observing $X = 10$ would be extremely unlikely in this case. For instance, the probability of observing $X=8$ if $p = 0.10$ is only \ensuremath{3.6\times 10^{-7}}, meaning we would only expect about 1 out of 3 million replicates to give $X=8$ (or similar for observing $X \in \{8, 9, 10\}$).
    \item In contrast, $\theta = 0.6$ or $\theta = 0.7$ are \emph{likely}, since $P_\theta(X = 8)$ would be 0.12, 0.23, respectively.
    \item Thus, we have found a \emph{deductive} way of comparing different values of $p$, based on the observed data: Compare the probability of observed data under different values of $p$.
    \item Concretely, we take the probability model $P_\theta(X = 8)$, and treat it as a function of $\theta$.
    \item Note this is opposite of what we did last semester, where we treat the pmf for a fixed $\theta$ as a function of $x$ (the possible output).
    \item This is how we define the likelihood function (for this model, and more generally, for any pmf):
    $$
    L(\theta) = P_\theta(X = 8) = f(x^*; \theta).
    $$
  \end{itemize}
  }
\end{exampleblock}

\framebreak

\begin{itemize}
  \item For our specific coin-flipping example with $N=10$, $X = 8$, the likelihood function is
  $$
  L(\theta) = P_\theta(X = 8).
  $$
  \item This is plotted in the following way:
\end{itemize}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{x} \hlkwb{<-} \hlkwd{seq}\hldef{(}\hlnum{1e-8}\hldef{,} \hlnum{1}\hlopt{-}\hlnum{1e-8}\hldef{,} \hlkwc{length.out} \hldef{=} \hlnum{1000}\hldef{)}
\hldef{y} \hlkwb{<-} \hlkwd{dbinom}\hldef{(}\hlnum{8}\hldef{,} \hlnum{10}\hldef{, x)}
\hlkwd{plot}\hldef{(}\hlkwc{x} \hldef{= x,} \hlkwc{y} \hldef{= y,} \hlkwc{type} \hldef{=} \hlsng{'l'}\hldef{)}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{tmp/figure/unnamed-chunk-1-1} 

}


\end{knitrout}

\begin{itemize}
  \item From the figure, we see that $p$ is unlikely to be less than $0.5$,or greater than $0.95$.
  \item Given the data alone (no prior), we should prefer a value somewhere in the middle of these values.
  \item We still have some uncertainty about the value of $p$, but the likelihood gives us a numerical way to compare values of $\theta$. Stochastic uncertainty as a result of sampling is captured in the likelihood function $L(\theta)$.
  \item \alert{The likelihood is not a probability}. Though it came from a probability, the likelihood function (a function of $\theta$) does not satisfy the requirements to be a probability. In our previous example, we have:
  $$
  \int_0^1 L(\theta)\, d\theta = 1/11 \neq 1.
  $$
  \item For discrete probability, the likelihood was \alert{continuous}. Discrete likelihoods are possible, arising when we want to select from a list $\{\theta_1, \theta_2, \ldots\}$.

  \framebreak

  \item The idea behind maximum likelihood estimation (MLE) is simple: our estimate is the value of $\theta$ that maximizes the likelihood function $L(\theta)$.
  \item The MLE is considered a \emph{frequentist} approach. Why? It quantifies a maximum belief about a parameter, which is more Bayesian in nature than Frequentist.
  \item As we'll see later, the MLE has nice theoretical Frequentist \emph{properties}, and as a result can be justified via the frequentist paradigm.
  \item Still, it has close connection to Bayesian estimation and interpretation. In fact, we'll discuss connections between the MLE and Bayesian statistics later.

  \framebreak

  \item Often, maximizing the likelihood directly is challenging, so we maximize the log-likelihood instead.
  \item Other times, the likelihood has to be maximized numerically.

\end{itemize}

\begin{exampleblock}{MLE of coin toss problem}
  Suppose we have $N = 10$ total tosses, and $n$ total heads. Find the MLE of $p$, the probability of heads.

  \mode<article>{
    \begin{itemize}
      \item First, describing what the likelihood function is. We already did this for this model, but a quick review, reinforcing common notation.
      \item We'll use a binomial$(N,p)$ model; the parameter of interest is $\theta = p$.
      \item Let $X$ be a random variable, denoting the number of heads. Thus, pmf of the model is:
      $$
      P(X = x) = f(x;\, \theta) = \binom{N}{x}p^{x}(1-p)^{N-x}.
      $$
      \item We assume we observe a specific dataset $x^*$. In this case, we observe $x^* = n$. Fixing the model at the observed data, we can describe the probability of the observed data as a function of $\theta = p$:
      $$
      L(\theta) = f(x^*; \, \theta) = \binom{N}{n}p^{n}(1-p)^{N-n},
      $$
      and the MLE is:
      $$
      \hat{\theta} = \argmax_\theta L(\theta),
      $$
      meaning that our estimate $\hat{\theta}$ is the \emph{argument} that \emph{maximizes} the likelihood function $L(\theta)$.
      \item As often is the case, this will be easiest if we take the natural logarithm, giving the log-likelihood. Note this is a monotonic transformation, so the argument that maximizes the log-likelihood is the same as the argument that maximizes the likelihood.
      \item Typically, we denote the log-likelihood using $\ell(\theta)$. In latex, this symbol is given by \texttt{\textbackslash ell}.
      $$
      \hat{\theta} = \argmax_\theta L(\theta) = \argmax_\theta \log\big(L(\theta)\big) = \argmax_\theta \ell(\theta).
      $$
      \item For our specific model, the log-likelihood is:
      $$
      \ell(\theta) = \log \binom{N}{n}p^{n}(1-p)^{N-n} = \log\bigg(\binom{N}{n}\bigg) + n\log(p) + (N-n)\log(1-p).
      $$
      \item Now we see another recurring theme when finding the MLE: we end up with terms that don't impact the maximization. That is, the binomal term out front does not change the maximum value of $p$, so it can be ignored.
      \item To maximize, we take the derivative, and set equal to zero:
      $$
      \frac{\partial \ell}{\partial \theta} = \frac{n}{p} - \frac{N-n}{1-p} = 0,
      $$
      or
      $$
      \frac{n}{p} = \frac{N-n}{1-p} \implies n-np = Np - np.
      $$
      \item Solving for $p$, we get the MLE:
      $$
      p = \frac{n}{N} \implies \hat{p} = \frac{n}{N} = \argmax_\theta L(\theta).
      $$
      \item A few notes:
      \begin{itemize}
        \item The MLE matches the frequentist-derived estimate. In most models, a first-principle estimate using frequentist interpretation of probability is not available, but the MLE will still match what the estimate \emph{would be} if such an estimate were available.
        \item Setting a derivative equal to zero and solving only gives a critical point. Why did I assume it was a maximum? First, we plotted like likelihood function, and it is clear any critical point will be a maximum. More importantly, many of the classic probability models are part of what is known as the \emph{exponential family}. In this family, all critical points of $L(\theta)$ will correspond to maximums. Next, theory we will establish later shows that as the number of observations increases, the likelihood function will be concave down around the MLE, suggesting that for even complex models, critical points in the region of high-likelihood will always be maximums.
      \end{itemize}
    \end{itemize}
  }

\end{exampleblock}

\end{frame}

\begin{frame}[allowframebreaks]{Continuous models}

\begin{itemize}
  \item The interpretation of the likelihood function as the ``the probability of the observed data $x^*$, considered as a function of $\theta$" makes perfect sense in the discrete model case.
  \item For continuous models, the technical issue arises that the probability of any point value $x$ is zero.
  \item We resolve the problem similar to what was done in Math 4450 and the John Rice text: approximate the probability by discretizing into small, discrete intervals:
  $$
  x^* \in (x^* - \epsilon/2, x^* + \epsilon/2),
  $$
  thus, the probability of observing something $\epsilon$-close to the data is:
  \begin{align*}
  L(\theta) &= P_\theta\big(X \in (x^* - \epsilon/2, x^* + \epsilon/2)\big) \\
  &= \int_{x^* - \epsilon/2}^{x^* + \epsilon/2} f(x; \theta)\, d\theta \approx \epsilon f(x^*;\theta).
  \end{align*}
  \item Then, since the likelihood is only meaningful up to a constant (we will discuss likelihood ratios later), then this has the same behavior as $L(\theta) = f(x^*;\theta)$.
  \item There are more advanced approaches to this problem, but this simple argument justifies the use of the pdf of a continuous random variable as the likelihood $L(\theta)$.
  \item \textbf{Going forward:} we once again will generalize a model $f(x;\theta)$ to mean either the pmf or pdf of a random variable. I will often say ``density" as a blanket term, even if this corresponds to a pmf, not a density.
  \item Further, when we ``integrate" a density, this means either:
  $$
  \int f(x;\theta)\,dx, \quad \text{If continuous}
  $$
  or
  $$
  \sum_x f(x;\theta),\quad \text{If discrete.}
  $$
\end{itemize}

\end{frame}

% \begin{frame}[allowframebreaks,fragile]{Example: partial information}
%   \begin{itemize}
%     \item Using likelihood as a mechanism for parameter estimation can be useful, even in scenarios we only have partial information.
%   \end{itemize}
%
%   \begin{exampleblock}{Extending the binomial example: Seed germination}
%     Suppose $100$ seeds were planted and it is only known that $x\leq 10$ germinated, the exact number unknown.
%     Estimate the probability of seed germination.
%
%     \mode<article>{
%     \begin{itemize}
%       \item This is really an extension of the coin-flipping example, and one reason why the binomial model is important to study: in many cases, we are interested in studying independent trials of ``success" vs ``failure".
%       \item As before, let $X$ denote the random variable of the number of seed germinations. Assuming independence, a natural model is:
%       $$
%       X \sim \text{Binom}(100, p),
%       $$
%       since there were 100 seeds to start.
%       \item The likelihood is determined by the probability of what is known, as a function of $\theta = p$:
%       $$
%       L(p) = P_{p}(X \leq 10) = \sum_{x = 0}^{10} \binom{100}{x} p^{x}(1-p)^{100-x}
%       $$
%     \end{itemize}
%     }
%
%   \end{exampleblock}
%
% \end{frame}

\section{Joint Probabilities}

\begin{frame}[allowframebreaks]{Likelihood with multiple observations}

\begin{itemize}
  \item Often the data we observe is multi-dimensional, rather than summarized as a single observation.

  \item In this case, the likelihood $\theta$ is still determined via the joint model:
  $$
  L(\theta) = f(x^*;\theta) = f_{X_{1:N}}(x_1^*, x_2^*, \ldots, x_N^*;\theta).
  $$
  \item We are mostly focused in this class in the case were the observations are independent, meaning the likelihood factors:
  $$
  L(\theta) = \prod_{i = 1}^N f_{X_i}(x_i^*;\theta).
  $$

  \item We often further simplify this by assuming the data are identically distributed:
  $$
  L(\theta) = \prod_{i = 1}^N f_{X_1}(x_i^*;\theta).
  $$

  \item As we've seen, it's generally easier to maximize the log-likelihood. In the IID case:
  $$
  \ell(\theta) = \log \prod_{i = 1}^N f_{X_1}(x_i^*;\theta) = \sum_{i = 1}^n \log f_{X_1}(x_i^*;\theta).
  $$
\end{itemize}

\end{frame}

\section{Examples}

\begin{frame}[allowframebreaks]{Examples of finding the MLE}

\begin{exampleblock}{Traffic data: Poisson Model}
  Returning to a motivating example, suppose we model traffic accidents in a given week as $X_1, X_2, \ldots, X_N$, where the data are iid Poisson$(\lambda)$. Obtain the MLE for $\lambda$.

  \mode<article>{
    \begin{itemize}
      \item If $X_i$ are iid poisson, then the pmf of a single observation is:
      $$
      f(x;\, \theta) = P_\theta(X_i = x) = \frac{\lambda^xe^{-\lambda}}{x!}.
      $$
      \item In this case, the parameter set of interest, $\theta$ is a single value: $\theta = \lambda$.
      \item Due to the IID assumption, the likelihood of each observation is given by:
      $$
      L(\theta) = f_{X_{1:N}}(x^*_{1:N}; \, \theta) = \prod_{i = 1}^N \frac{\lambda^{x^*_i}e^{-\lambda}}{x^*_i!}.
      $$
      \item We want to maximize this function with respect to $\lambda$; this is easier done after taking the log:
      \begin{align*}
      \ell(\theta) &= \log f_{X_{1:N}}(x^*_{1:N}; \, \theta) \\
                   &= \sum_{i = 1}^N \log f_{X_1}(x^*_i; \, \theta) \\
                   &= \sum_{i = 1}^N \big(x^*_i \log \lambda - \lambda - \log (x^*_i!)\big)\\
                   &= \log \lambda \sum_{i = 1}^N x^*_i - N\lambda - \sum_{i = 1}^N \log (x^*_i!) \\
                   &= \log \lambda \sum_{i = 1}^N x^*_i - N\lambda - c(x^*).
      \end{align*}
      \item In the last step, we replaced the sum of log-factorials with some constant function $c(x^*)$. As a function of $\theta$, the maximum of $\ell(\theta)$ does not change with this constant, so it can be ignored for the purposes of maximization.
      \item Now to find the maximum, we can take the derivative and set equal to zero:
      $$
      \ell'(\theta) = \frac{1}{\lambda}\sum_{i = 1}^N x^*_i - N = 0,
      $$
      Thus,
      $$
      \hat{\lambda} = \frac{1}{N}\sum_{i = 1}^N x^*_i = \bar{x}^*_N.
      $$
      \item We formally need to check if this is indeed a maximum and not just a critical point. The second derivative with respect to $\lambda$ is always negative (since $x_i^*$ is non-negative), implying that it is indeed a maximum.
      \item We'll see later that as $N \rightarrow \infty$, then the likelihood function is \emph{always} concave-down around the maximum, under fairly week conditions.
      \item We could also plot the likelihood / log-likelihood, which is informative on it's own right.
      \item Finally, note that this is the same estimator that was obtained via the MoM approach. This sometimes happens.
    \end{itemize}
  }

\end{exampleblock}

\framebreak

\begin{exampleblock}{Two parameter model: Gaussian model}
  Suppose we model observations $X_1, \ldots, X_N$ as IID $N(\mu, \sigma^2)$ random variables. Find the MLE of $\theta = (\mu, \sigma^2)$.

  \mode<article>{
    \begin{itemize}
      \item Though we have two parameters, the approach for finding the MLE is similar: find the likelihood function of the entire dataset, take the logarithm, and the compute (partial) derivatives and set equal to zero.
      \item In the final step, we might end up with a system of equations rather than just a single equation.
      \item There's also an interesting feature of this example that is applicable elsewhere: should we find the MLE of $\sigma$, or the MLE of $\sigma^2$? Fortunately, you get the same result either way (we'll show this later).
      \item In this case, either approach is straightforward. As practice, you may want to consider finding the MLE of $\sigma^2$. Here, we will treat $\sigma$ as the parameter of interest, though sometimes it's easier to differentiate with respect to something like: $\theta_2 = \sigma^2$.
      \item Because of the IID assumption joint-likelihood function is:
      $$
      L(\mu, \sigma) = f(x^*;\, \mu, \sigma) = \prod_{i = 1}^N \frac{1}{\sigma\sqrt{2\pi}}\exp\Big(-\frac{1}{2}\big((x^*_i - \mu)/\sigma\big)^2\Big),
      $$
      \item The log-likelihood is therefore:
      \begin{align*}
      \ell(\mu, \sigma) &= \sum_{i = 1}^N \Big(\log(1) - \log(\sigma\sqrt{2\pi}) - \frac{1}{2}\big((x^*_i - \mu)/\sigma\big)^2\Big) \\
      &= -n\log(\sigma) - \frac{n}{2}\log 2\pi - \frac{1}{2\sigma^2}\sum_{i = 1}^N (x^*_i - \mu)^2.
      \end{align*}
      \item The partial derivatives are given by:
      \begin{align*}
        \frac{\partial \ell}{\partial \mu} &= \frac{1}{\sigma^2}\sum_{i = 1}^N (x^*_i - \mu) = 0 \\
        \frac{\partial \ell}{\partial \sigma} &= -\frac{n}{\sigma} + \sigma^{-3}\sum_{i = 1}^N (x_i^* - \mu)^2.
      \end{align*}
      \item Because $\sigma \geq 0$ (and zero if and only if the data are the same value), the first equation will only be zero if $\sum_{i = 1}^N (x^*_i - \mu) = 0$, or if $\mu = \bar{x}^*_N$, which is independent of $\sigma$. Thus, the MLE is:
      $$
      \hat{\mu} = \bar{x}^*_N.
      $$
      \item For the second equation, the partial derivative is zero if and only if
      $$
      \frac{n}{\sigma} = \sigma^{-3}\sum_{i = 1}^N (x_i^* - \mu)^2,
      $$
      or if
      $$
      \sigma^2 = \frac{1}{n}\sum_{i = 1}^N (x_i^* - \mu)^2.
      $$
      Since we are looking for the MLE of $\theta = (\theta, \sigma)$, we can now replace $\mu$ with it's maximizer $\hat{\mu} = \bar{x}^*_N$, giving us:
      $$
      \hat{\sigma} = \sqrt{\frac{1}{n}\sum_{i = 1}^N (x_i^* - \hat{\mu})^2} = \sqrt{\frac{1}{n}\sum_{i = 1}^N (x_i^* - \bar{x}^*_N)^2}.
      $$
      \item Like the Poisson example, the MLE for the iid normal model has the same estimator as the MoM procedure.
      \item The sampling distribution for $\theta$ is therefore:
      $$
      \hat{\mu} \sim N(\mu, \sigma^2 / N), \quad N\hat{\sigma}^2/\sigma^2 \sim \chi^2_{N-1},
      $$
      and the two distributions are independent.
    \end{itemize}
  }

\end{exampleblock}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Plotting Normal Likelihood}
  \begin{itemize}
    \item The likelihood function (not just to point estimate) will be used to measure uncertainty.
    \item For models with a single parameter, we often plot the likelihood curve.
    \item With more than one parameter, however, we have a \alert{likelihood surface}.
    \item For the iid Normal$(\mu, \sigma^2)$ model, code for plotting this surface is available with course source-code.
  \end{itemize}

  \framebreak




\begin{figure}[ht]
\includegraphics[width=0.8\textwidth]{bivariateNormal.png}
\caption{Likelihood surface of data generated from normal distribution.}
\end{figure}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Calculating the likelihood in R}
  \begin{itemize}
    \item For standard distributions, the likelihood is easy to calculate in R. Recall the likelihood is:
    $$
    L(\theta) = f(x^*_i; \theta).
    $$
    \item For $X_1, \ldots, X_n$ iid normal, the likelihood can be calculated using \code{dnorm}:
  \end{itemize}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Synthetic data}
\hldef{x_star} \hlkwb{<-} \hlkwd{rnorm}\hldef{(}\hlkwc{n} \hldef{=} \hlnum{100}\hldef{,} \hlkwc{mean} \hldef{=} \hlnum{4}\hldef{,} \hlkwc{sd} \hldef{=} \hlnum{3}\hldef{)}
\hldef{theta} \hlkwb{<-} \hlkwd{c}\hldef{(}\hlnum{0}\hldef{,} \hlnum{1}\hldef{)}  \hlcom{# value of theta}
\hldef{Likelihood} \hlkwb{<-} \hlkwd{prod}\hldef{(}\hlkwd{dnorm}\hldef{(x,} \hlkwc{mean} \hldef{= theta[}\hlnum{1}\hldef{],} \hlkwc{sd} \hldef{= theta[}\hlnum{2}\hldef{]))}
\hldef{loglik} \hlkwb{<-} \hlkwd{sum}\hldef{(}
  \hlkwd{dnorm}\hldef{(x,} \hlkwc{mean} \hldef{= theta[}\hlnum{1}\hldef{],} \hlkwc{sd} \hldef{= theta[}\hlnum{2}\hldef{],} \hlkwc{log} \hldef{=} \hlnum{TRUE}\hldef{)}
\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\section{Numeric Optimization}

\begin{frame}[allowframebreaks]{Numeric Optimization}
  \begin{itemize}
    \item In the previous examples, the MLE was available \emph{analytically}.
    \item In many cases, however, there is no closed-form solution for the MLE, and it must be computed numerically.
    \item The next example demonstrates this, and then we will discuss optimization strategies.
  \end{itemize}

  \framebreak

  \begin{exampleblock}{Example: Gamma likelihood}
    Suppose we want to model data $X_1, X_2, \ldots, X_n$ as iid Gamma$(\alpha, \lambda)$, which has the density function:
    $$
    f(x;\, \alpha, \lambda) = \frac{1}{\Gamma(\alpha)}\lambda^\alpha x^{\alpha - 1}e^{-\lambda x}, \quad 0 \leq x < \infty.
    $$
    Find the MLE of $\theta = (\alpha, \lambda)$.

    \mode<article>{
      \begin{itemize}
        \item The joint likelihood under the IID assumption is:
        $$
        L(\theta) = \prod_{i = 1}^n \frac{1}{\Gamma(\alpha)}\lambda^\alpha {(x^*_i)}^{\alpha - 1}e^{-\lambda x^*_i}, \quad 0 \leq x^*_i < \infty.
        $$
        \item We'll drop the indicator function, since the observations are all bigger than zero (otherwise a Gamma model is a bad choice), so the value is one.
        Taking logarithms, we get:
        \begin{align*}
        \ell(\alpha, \lambda) &= \sum_{i = 1}^n \big(\alpha \log \lambda + (\alpha - 1)\log x_i^* - \lambda x_i^* - \log\Gamma(\alpha)\big) \\
        &= n\alpha\log\lambda + (\alpha - 1)\sum_{i = 1}^n \log x_i^* - \lambda \sum_{i = 1}^n x^*_i - n\log \Gamma(\alpha).
        \end{align*}
        \item The partial derivatives are:
        \begin{align*}
          \frac{\partial \ell}{\partial \alpha} &= n \log \lambda + \sum_{i = 1}^n \log x_i^* - n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)},\\
          \frac{\partial \ell}{\partial \lambda} &= \frac{n\alpha}{\lambda} - \sum_{i = 1}^n x_i^*.
        \end{align*}
        \item The second equation is the easiest to solve. Setting equal to zero, we have:
        $$
        \frac{n \alpha}{\lambda} = \sum_{i = 1}^n x_i^* = n\bar{x}^*_n.
        $$
        Thus, solving for $\lambda$, we get:
        $$
        \lambda = \frac{\alpha}{\bar{x}^*_n}.
        $$
        \item The estimate of $\lambda$ depends on the value of $\alpha$. Since we are looking for a maximum of both, it needs to be a critical point, so we can write the estimate of $\lambda$ as:
        $$
        \hat{\lambda} = \frac{\hat{\alpha}}{\bar{x}^*_n},
        $$
        implying that we need to first maximize the likelihood for $\alpha$.
        \item Plugging in $\hat{\lambda}$ into the second partial derivative, setting equal to zero, gives:
        $$
        n\log \alpha - n \log \bar{x}^*_n + \sum_{i = 1}^n \log x_{i}^* - n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)} = 0.
        $$
        \item This equation \emph{cannot be solved} in closed-form for $\alpha$.
      \end{itemize}
    }

  \end{exampleblock}

  \framebreak

  \begin{itemize}
    \item The previous example leads us to consider numeric techniques for optimization and root finding.
    \item Note that this class is \alert{not} an optimization course, so we'll only cover some of the most basic ideas.
    \item More modern and efficient numeric optimization techniques are readily available in R (or any other statistical software).
    \item For this class, we'll introduce some basic ideas like the Newton-Raphsom approach for root finding and optimization, as well as some other basic methods.
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Newton-Raphsom root-finding algorithm}
  \begin{itemize}
    \item Idea: start at a point $\theta_0$, and approximate find the tangent line of $f(\theta)$ at the point $\theta_0$:
    $$
    y - f(\theta_0) = f'(\theta_0)\big(\theta - \theta_0\big)
    $$
    \item Then, find the root of the tangent line by setting $y = 0$, and solving for $\theta$:
    $$
    \theta = \theta_0 - \frac{f(\theta_0)}{f'(\theta_0)}.
    $$
    \item This root of the tangent line will be closer than our original guess $\theta_0$, so we set:
    $$
    \theta_1 = \theta_0 - \frac{f(\theta_0)}{f'(\theta_0)},
    $$
    and repeat:
    $$
    \theta_{n + 1} = \theta_n - \frac{f(\theta_n)}{f'(\theta_n)}.
    $$
    \item We stop based on some convergence criteria, often something like $|\theta_{n + 1} - \theta_n| < \epsilon$, for a small choice of $\epsilon$.
    \item (In class, check out wikipedia or draw a picture).
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item We need now a starting point $\theta_0$.
    \item Really we can pick anything, but it's best if we are close to the MLE.
    \item For our current problem (Gamma distribution), we could use the MoM estimator:
    $$
    \hat{\alpha}_{\text{MoM}} = \theta_0 = \frac{\big(\bar{x}^*_n\big)^2}{\frac{1}{n}\sum_{i = 1}^n \big(x_i^* - \bar{x}^*_n\big)^2}.
    $$
  \end{itemize}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{NR_root} \hlkwb{<-} \hlkwa{function}\hldef{(}\hlkwc{theta0}\hldef{,} \hlkwc{fn}\hldef{,} \hlkwc{deriv}\hldef{,} \hlkwc{tol} \hldef{=} \hlnum{1e-8}\hldef{,} \hlkwc{maxiter} \hldef{=} \hlnum{1000}\hldef{) \{}
  \hldef{iter} \hlkwb{<-} \hlnum{0}
  \hldef{theta_old} \hlkwb{<-} \hldef{theta0}
  \hldef{theta_new} \hlkwb{<-} \hldef{theta_old} \hlopt{+} \hlnum{10} \hlopt{*} \hldef{tol}

  \hlkwa{while}\hldef{(}\hlkwd{abs}\hldef{(theta_old} \hlopt{-} \hldef{theta_new)} \hlopt{>} \hldef{tol} \hlopt{&&} \hldef{iter} \hlopt{<} \hldef{maxiter) \{}
    \hldef{iter} \hlkwb{<-} \hldef{iter} \hlopt{+} \hlnum{1}
    \hldef{theta_old} \hlkwb{<-} \hldef{theta_new}
    \hldef{theta_new} \hlkwb{<-} \hldef{theta_old} \hlopt{-} \hlkwd{fn}\hldef{(theta_old)} \hlopt{/} \hlkwd{deriv}\hldef{(theta_new)}
  \hldef{\}}
  \hlkwd{cat}\hldef{(}\hlsng{"iters: "}\hldef{, iter,} \hlsng{"\textbackslash{}n"}\hldef{)}
  \hldef{theta_new}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\framebreak

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{alpha_fn} \hlkwb{<-} \hlkwa{function}\hldef{(}\hlkwc{alpha}\hldef{,} \hlkwc{data}\hldef{) \{}
  \hldef{n} \hlkwb{<-} \hlkwd{length}\hldef{(data)}
  \hldef{n} \hlopt{*} \hlkwd{log}\hldef{(alpha)} \hlopt{-} \hldef{n} \hlopt{*} \hlkwd{log}\hldef{(}\hlkwd{mean}\hldef{(data))} \hlopt{+} \hlkwd{sum}\hldef{(}\hlkwd{log}\hldef{(data))} \hlopt{-} \hldef{n} \hlopt{*} \hlkwd{digamma}\hldef{(alpha)}
\hldef{\}}

\hldef{alpha_deriv} \hlkwb{<-} \hlkwa{function}\hldef{(}\hlkwc{alpha}\hldef{,} \hlkwc{data}\hldef{) \{}
  \hldef{n} \hlkwb{<-} \hlkwd{length}\hldef{(data)}
  \hldef{(n}\hlopt{/}\hldef{alpha)} \hlopt{-} \hldef{n} \hlopt{*} \hlkwd{psigamma}\hldef{(alpha,} \hlnum{1}\hldef{)}
\hldef{\}}

\hlkwd{set.seed}\hldef{(}\hlnum{123}\hldef{)}
\hldef{data} \hlkwb{<-} \hlkwd{rgamma}\hldef{(}\hlkwc{n} \hldef{=} \hlnum{23}\hldef{,} \hlkwc{shape} \hldef{=} \hlnum{1}\hldef{,} \hlkwc{rate} \hldef{=} \hlnum{2}\hldef{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\framebreak

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Not the exact MoM estimate, but close enough:}
\hldef{alpha_mom} \hlkwb{<-} \hldef{(}\hlkwd{mean}\hldef{(data)}\hlopt{^}\hlnum{2}\hldef{)} \hlopt{/} \hlkwd{sd}\hldef{(data)}

\hlkwd{NR_root}\hldef{(}
  \hlkwc{theta0} \hldef{= alpha_mom,}
  \hlkwc{fn} \hldef{=} \hlkwa{function}\hldef{(}\hlkwc{x}\hldef{)} \hlkwd{alpha_fn}\hldef{(x,} \hlkwc{data} \hldef{= data),}
  \hlkwc{deriv} \hldef{=} \hlkwa{function}\hldef{(}\hlkwc{x}\hldef{)} \hlkwd{alpha_deriv}\hldef{(x,} \hlkwc{data} \hldef{= data),}
  \hlkwc{tol} \hldef{=} \hlnum{1e-10}
\hldef{)}
\end{alltt}
\begin{verbatim}
iters:  7 
[1] 0.9728019
\end{verbatim}
\end{kframe}
\end{knitrout}




\framebreak

\begin{itemize}
  \item Once the estimate of $\alpha$ is found, we can then plug it in to get the estimate of $\lambda$:
  $$
  \hat{\lambda} = \frac{\hat{\alpha}}{\bar{x}^*_n} = 1.981.
  $$
\end{itemize}

\framebreak




\begin{figure}[ht]
\includegraphics[width=0.8\textwidth]{GammaSurface.png}
\caption{Likelihood surface of data generated from Gamma distribution.}
\end{figure}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Root-find considerations}

\begin{itemize}
  \item The function that we built for solving $f(\theta) = 0$ requires the derivative $f'(\theta)$.
  \item In some cases, the derivative is not readily available. Instead, we can approximate using the definition:
  $$
  f'(\theta) = \lim_{h \rightarrow 0} \frac{f(\theta + h) - f(\theta)}{h} \approx \frac{f(\theta + \Delta) - f(\theta)}{\Delta}.
  $$
  \item We just pick $\Delta$ to be small, and we get a very good approximation of the derivative.
  \item Thus, we don't really need to derivative for uni-variate root-finding.
  \item The same mathematical approach can readily be extended into higher dimensional $\theta$, replacing the derivative with a \alert{gradient}.
  \item An important consideration, however, is that the results may depend on your starting parameter $\theta_0$. In practice, you may want to try multiple values of $\theta_0$.

  \framebreak

  \item In most programming languages, there will be pre-built methods for root-fining and optimization.
  \item Our function we built works, but it doesn't do careful error checking, and it isn't optimized for speed.
  \item For univariate-root finding $f(\theta) = 0$ in R, we can use the \code{uniroot} function, which doesn't require a derivative.
  \item This function is very efficient for solving roots if $\theta \in \R$. For higher dimensions, we need to import a different package.
\end{itemize}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{uniroot}\hldef{(}
  \hlkwa{function}\hldef{(}\hlkwc{x}\hldef{)} \hlkwd{alpha_fn}\hldef{(x,} \hlkwc{data} \hldef{= data),}
  \hlkwc{interval} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{0.5}\hldef{,} \hlnum{2}\hldef{)}
\hldef{)}
\end{alltt}
\begin{verbatim}
$root
[1] 0.9728068

$f.root
[1] -7.754612e-05

$iter
[1] 6

$init.it
[1] NA

$estim.prec
[1] 6.103516e-05
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Direct Numeric Optimization}
  \begin{itemize}
    \item Based on our last example, we solved for one parameter $\lambda$, and numerically found the other, $\alpha$, via root-finding.
    \item In many cases, this is not possible. Instead, we might want to directly optimize both parameters.
    \item We will first introduce this by extending the Newton-Raphson to perform optimization rather than root-finding.

    \framebreak

    \item The idea is simple: local maximum are just the zeros (roots) of the derivative function.
    \item Thus, we still perform Newton-Raphson, but on the derivative rather than the original function.
    \item Starting with initial estimate $\theta_0$, we update following the equations until convergence:
    $$
    \theta_{n + 1} = \theta_{n} - \frac{f'(\theta_n)}{f''(\theta_n)}
    $$

    \framebreak

    \item If $\theta$ is multivariate, then again we use the \alert{gradient}: $\nabla f(\theta)$, which is a vector, and Hessian: $\nabla^2 f(\theta)$, which is a matrix:
    $$
    \theta_{n+1} = \theta_n - \big(\nabla^2 f(\theta_n)\big)^{-1}\nabla \big(f(\theta_n)\big).
    $$
    \item This is the basic approach of many traditional machine learning algorithms:
    \begin{itemize}
      \item Pick a model that depends on $\theta$, and a loss-function $f(\theta)$ that depends on the data and model (here, our loss is the log-likelihood function).
      \item If possible, calculate the derivative of the loss function with respect to $\theta$. If not, approximate numerically.
      \item If possible, calculate the Hessian of the loss function with respect to $\theta$. If not, approximate numerically.
      \item Start with initial guess for $\theta$, take steps the size of the (approximate) hessian of $f(\theta)$, in the direction of the gradient $f(\theta)$.
    \end{itemize}

    \item Many optimization strategies are variants of this basic approach: In practice, Hessians / Gradients may be expensive to compute.
    \item We will primarily focus our attention on pre-existing solutions.
    \item In R, the function we will use is called \code{optim}.
    \item There are several optimization methods available within this function

    \framebreak

    \begin{itemize}
      \item Nelder-Mead: a heuristic, direct search method. Does not require differentiability. Slow, but robust.
      \item BFGS / L-BFGS-B: A Quasi-Newton approach, approximates either (or both) the Hessian and Gradient numerically. Extremely effective for (twice) differentiable functions, but slow as $\theta$ grows in dimension.
      \item SANN: A stochastic approach. Hard to tune, but effective on ``rough" surfaces
      \item CG: Conjugate Gradients. More ``fragile" than BFGS, but require less memory storage so can be useful for large $\theta$.
      \item Brent: Only univariate $\theta$. In these cases, approximates the gradient and hessian, similar to what we proposed. Very effective, but requires univariate $\theta$.
    \end{itemize}

    \framebreak

    \item How it works: Get function $f$, method you want to use, and starting point $\theta_0$. Example:
  \end{itemize}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{gamma_negloglik} \hlkwb{<-} \hlkwa{function}\hldef{(}\hlkwc{theta}\hldef{) \{}
  \hlopt{-}\hlkwd{dgamma}\hldef{(}
    \hlkwc{x} \hldef{= data,}
    \hlkwc{shape} \hldef{= theta[}\hlnum{1}\hldef{],} \hlkwc{rate} \hldef{= theta[}\hlnum{2}\hldef{],}
    \hlkwc{log} \hldef{=} \hlnum{TRUE}
  \hldef{) |>} \hlkwd{sum}\hldef{()}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\framebreak

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{optim}\hldef{(}
  \hlkwd{c}\hldef{(}\hlnum{2}\hldef{,} \hlnum{5}\hldef{),}
  \hlkwc{fn} \hldef{= gamma_negloglik,}
  \hlkwc{method} \hldef{=} \hlsng{'L-BFGS-B'}\hldef{,}  \hlcom{# Constrained theta>0}
  \hlkwc{lower} \hldef{=} \hlkwd{c}\hldef{(}\hlnum{1e-8}\hldef{,} \hlnum{1e-8}\hldef{)}  \hlcom{# Constrained theta>0}
\hldef{)}
\end{alltt}
\begin{verbatim}
$par
[1] 0.9728026 1.9806150

$value
[1] 6.641716

$counts
function gradient 
      19       19 

$convergence
[1] 0

$message
[1] "CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH"
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[allowframebreaks]{Gradient Descent and Variations}

\begin{itemize}

    \item In Newton-Raphson, the Hessian just tells us the \emph{size} of the step.
    \item Often the gradient is available, but not the Hessian (or it is expensive to compute the inverse, since it is a matrix).
    \item In these cases, we often use a different approach called \alert{gradient descent}.
    \item Idea: still step in the direction of the (negative) gradient, but the step size will just be small $\delta_n$, and let $\delta_n$ shrink over-time:
    $$
    \theta_{n+1} = \theta_{n} - \delta_n \nabla f(\theta_n).
    $$
    \item The idea is simple, but a lot of modern optimization techniques and theory are based on this idea, finding various strategies for specifying $\delta_n$.
    \item Ex: BFGS approximates the Hessian, but scales poorly with dimension. In deep learning, $\theta$ has dimension in millions / billions, so BFGS won't work (nor does \code{optim}).
    \item The success of deep-learning and AI is largely due to the advent of \alert{automatic differentiation}: software that calculates the exact gradient of $f(\theta_n)$, after simple calculations.
    \item Auto-diff libraries: PyTorch, TensorFlow, JAX, etc. Mostly available in Python, though some are available in R.

    \framebreak

    \item Final approach we will discuss is \alert{stochastic} gradient descent, which is effectively randomly sampling the data to compute an approximate gradient.
    \item The idea is that, even with auto-diff, gradients (with entire data) are expensive to compute.
    \item Thus, randomly sample data to get a stochastic approximation of the gradient; this is faster to compute, so we get more update-steps.
    \item This is the primary technique used in most deep-learning frameworks.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Maximum Likelihood: continued examples}

\begin{exampleblock}{Muon Decay}
  Returning to the Muon-Decay example we used MoM to solve. The density of iid observations $X_1, \ldots, X_n$ is:
  $$
  f(x; \alpha) = \frac{1 + \alpha x}{2},\quad -1 \leq x \leq 1 \quad -1\leq \alpha \leq 1.
  $$
  Find the MLE of $\alpha$.

  \mode<article>{
    \begin{itemize}
      \item The joint-likelihood function, under IID assumptions and ignoring indicators, is:
      $$
      L(\alpha) = \prod_{i = 1}^n \frac{1 + \alpha x^*_i}{2}.
      $$
      \item The log-likelihood is given by:
      $$
      \ell(\alpha) = \sum_{i = 1}^n \log(1 + \alpha x^*_i) - n \log 2.
      $$
      \item Taking the derivative, we have:
      \begin{align*}
      \ell'(\alpha) &= \frac{\partial}{\partial \alpha} \sum_{i = 1}^n \log(1 + \alpha x^*_i) - \frac{\partial}{\partial \alpha} n\log 2 \\
      &= \sum_{i = 1}^n \frac{\partial}{\partial \alpha} \log(1 + \alpha x_i^*) \\
      &= \sum_{i = 1}^n \frac{1}{1 + \alpha x^*_i}.
      \end{align*}
      \item Setting this equal to zero gives the following equation:
      $$
      \sum_{i = 1}^n \frac{x^*_i}{1 - \alpha x^*_i} = 0.
      $$
      \item As before, there is no closed-form solution to solve this equation for $\alpha$, and we need to take a computational approach. There are a couple of choices:
      \begin{itemize}
        \item Solve for roots using a root finding algorithm, or directly optimize $\ell(\alpha)?$
        \item What optimization procedure to use? Autodiff? Calculate second derivative and use Newton-Raphson? Quasi-newton, like BFGS?
      \end{itemize}
      \item In this case, since a second derivative can readily be calculated, it's probably best to do Newton-Raphson, or a root-finding algorithm. We can use our own approach for root-finding of $\ell(\alpha)$, apply \code{uniroot} to $\ell'(\alpha)$, or \code{optim} to $\ell(\alpha)$.
      \item In class, we will practice at least of of these approaches, but it is recommended you try some of these approaches on your own.
      \item Regardless, the MoM might be a good choice for initializing the value $\alpha_0$.
    \end{itemize}
  }

\end{exampleblock}

  \framebreak
  
  \begin{quote}
    ``If [the likelihood] cannot by maximized analytically, it may be possible to use a computer to maximize [the likelihood] numerically. In fact, this is one of the most important features of the MLE. If a model can be written down, then there is some hope of maximizing it numerically, and, hence, finding MLEs of the parameters." -- \citet[][Section 7.2]{casella24}
  \end{quote}

\end{frame}

\section{Constraint Optimization}

\begin{frame}[allowframebreaks]{Constrained optimization}
  \begin{itemize}
    \item In all of our previous examples, we want to maximize the log-likelihood function $f(x_i^*; \theta)$, and there is some natural constraints of $\theta$.
    \item Examples: In Gamma$(\alpha, \lambda)$, $\theta = (\alpha, \lambda)$, and we are constrained by $\alpha, \lambda > 0$.
    \item In the Gaussian model, $\theta = (\mu, \sigma)$. $\mu$ is unconstrained, but $\sigma \geq 0$.
    \item Similarly, in the Muon-decay example: $\theta = \alpha \in (0, 1)$.
    \item This leads to the issue of \emph{constrained} optimization, and is typically a requirement for maximum likelihood estimation.
    \item There are a few common strategies:
    \framebreak
    \item Limit search region to be within constraints: This is the basic approach used by the \code{uniroot} function. It's also used sometimes in \code{optim}, e.g., the L-BFGS-B method.
    \item Variable transformations. Modify the function so that the search algorithm can try all possible values, but apply a transformation that keeps it in the desired range.
    \begin{itemize}
    \item Example: if $\theta > 0$ is a constraint, optimize over $\alpha$ with $\theta = e^{\alpha}$. Thus, $\alpha < 0$ is no problem, since $\theta > 0$. (\alert{Example on next slide})
    \item Another common example is a logit transformation, which ensures $0 \leq \theta \leq 1$, via $\log (\theta / (1 - \theta)) = \alpha$.
    \end{itemize}
    \item In some cases, the constraint leads to an equation $g(\theta) = 0$, in which case we can apply something like Lagrange-multipliers. We will look at one of these cases as well.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks,fragile]{Example: Gamma likelihood}
  \begin{exampleblock}{MLE of Gamma likelihood using transformations}
    Revisit the numeric optimization of the Gamma likelihood, using variable transformations to deal with constraints.
  \end{exampleblock}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{gamma_negloglik2} \hlkwb{<-} \hlkwa{function}\hldef{(}\hlkwc{alpha}\hldef{) \{}
  \hldef{theta} \hlkwb{<-} \hlkwd{exp}\hldef{(alpha)}
  \hlopt{-}\hlkwd{dgamma}\hldef{(}
    \hlkwc{x} \hldef{= data,}
    \hlkwc{shape} \hldef{= theta[}\hlnum{1}\hldef{],} \hlkwc{rate} \hldef{= theta[}\hlnum{2}\hldef{],}
    \hlkwc{log} \hldef{=} \hlnum{TRUE}
  \hldef{) |>} \hlkwd{sum}\hldef{()}
\hldef{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

\framebreak

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hldef{results} \hlkwb{<-} \hlkwd{optim}\hldef{(}
  \hlkwd{c}\hldef{(}\hlkwd{log}\hldef{(}\hlnum{2}\hldef{),} \hlkwd{log}\hldef{(}\hlnum{5}\hldef{)),}
  \hlkwc{fn} \hldef{= gamma_negloglik2,}
  \hlkwc{method} \hldef{=} \hlsng{'BFGS'} \hlcom{# unconstrained}
\hldef{)}
\hlkwd{exp}\hldef{(results}\hlopt{$}\hldef{par)}  \hlcom{# Theta}
\end{alltt}
\begin{verbatim}
[1] 0.9728029 1.9806131
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[allowframebreaks]{Multinomial Cell probabilities}
  \begin{itemize}
    \item In this example, we are bound by a curve, not a region. Lagrange Multipliers are a good analytic approach.
    \item Consider fitting a multinomial distribution to a frequency table.
    \item We let $X_1, \ldots, X_m$ be the counts in cells $1, \ldots, X_m$, and we assume that $(X_1, \ldots, X_m)$ follows a multinomial distribution.
    \item The distribution has has parameters $n$: total counts, and $p_i$ probability of cell $i$.
    \item The total count $n = \sum_{i = 1}^b X_i$, and $p_i$ being the
    \item In this case, the $X_i$ are \emph{not} independent, so we don't just take product of IID observations.
    \item However, the likelihood can be calculated via the pmf of the multinomial distribution:
    $$
    L(\theta) = f(x^*_i; \theta) = \frac{n!}{\prod_{i = 1}^m x_i!} \prod_{i = 1}^mp_i^{x^*_i}.
    $$
    \item Given $X_1, \ldots, X_m$, the number of trials $n$ is known. The parameter vector of interest is then:
    $$
    \theta = (p_1, \ldots, p_m).
    $$
    \item The log-likelihood is given by:
    $$
    \ell(\theta) = \log n! - \sum_{i = 1}^m \log x_i! + \sum_{i = 1}^m x_i \log p_i.
    $$
    \item Note that we have the constraint $\sum_i p_i = 1$. Thus, we will solve this using the Lagrange-multiplier technique.
    \item Reminder: Suppose we are trying to maximize (or minimize) a function $f(\theta)$, with the constraint $g(\theta) = 0$. That is,
    \begin{align*}
      \max_\theta &\, f(\theta)\\
      \text{subject to} & \, g(\theta) = 0
    \end{align*}
    \item This optimization problem can be solved by introducing the Lagrange function:
    $$
    \mathcal{L}(\theta, \lambda) = f(\theta) + \lambda g(\theta),
    $$
    \item And solving the system of equations that arises from:
    $$
    \nabla_{\theta, \lambda} \mathcal{L}(\theta, \lambda) = 0.
    $$
    \item Recall the partial derivative $\frac{\partial}{\partial \lambda}\mathcal{L}(\theta, \lambda) = g(\theta)$, which must be zero, giving the necessary constraint.
    \item Then, the condition $\nabla_\theta \mathcal{L}(\theta, \lambda) = 0$ ensures that the gradients of $f$ and $g$ are parallel. That is, geometrically, the constraint $g(\theta) = 0$ defines a curve or surface that we are constrained to; on this surface, the max/min value of $f$ occurs when a level-set of $f$ is tangent to the constraint curve $g(\theta) = 0$. This occurs only where $\nabla f(\theta) = \lambda \nabla g(\theta)$, which gives rise to the set of equations defined by $\nabla_\theta \mathcal{L}(\theta, \lambda) = 0$.
    \end{itemize}
    \framebreak

    \begin{exampleblock}{MLE of multinomial cell probabilities}
      Use Lagrange multipliers to find the MLE of a multinomial distribution, with $X_1, \ldots, X_m$.

      \mode<article>{
      \begin{itemize}
        \item The log-likelihood, which we want to maximize, is the function:
        $$
        \ell(\theta) = \log n! - \sum_{i = 1}^m \log x^*_i! + \sum_{i = 1}^m x^*_i \log p_i.
        $$
        \item However, we have the constraint:
        $$
       g(\theta) = \sum_{i = 1}^m p_i - 1 = 0.
        $$
        \item We can find the solution using Lagrange-multipliers. The Lagrangian is:
        $$
    \mathcal{L}(\theta, \lambda) =  \log n! - \sum_{i = 1}^m \log x^*_i! + \sum_{i = 1}^m x^*_i \log p_i + \lambda (\sum_{i = 1}^m p_i - 1).
        $$
        \item Taking partial derivatives with respect to $p_i$ gives:
        $$
        \nabla_{\theta_i}  \mathcal{L}(\theta, \lambda) = \frac{x_i^*}{p_i} + \lambda,
        $$
        Setting equal to zero and solving for $p_i$ gives:
        $$
        p_i = -\frac{x_i^*}{\lambda}.
        $$
        \item Now we still have the constraint function, given by:
        $$
        \frac{\partial}{\partial \lambda} \mathcal{L}(\theta, \lambda) = 0,
        $$
        which gives:
        $$
        \sum_{i = 1}^m p_i = 1.
        $$
        Now we can solve for all of $\theta$ and $\lambda$, by combining these equations:
        $$
        \sum_{i = 1}^m p_i = \sum_{i= 1}^m -\frac{x_i^*}{\lambda} = -\frac{n}{\lambda} = 1,
        $$
        which implies:
        $$
        \lambda = -n,
        $$
        and therefore:
        $$
        p_i = \frac{x^*_i}{n}.
        $$
  \end{itemize}
      }

    \end{exampleblock}

\framebreak

\begin{itemize}
  \item Often, we can describe parameters of a model as functions of other parameters.
  \item Example: in the multinomial model, we have parameters $p_i$ representing probabilities for each cell. We might want to make these a function of $\theta$, such that $p_i(\theta)$; then, we will still want to estimate $\theta$ using the data.
  \item In the case above the log-likelihood is then:
  $$
  \ell(\theta) = \log n! - \sum_{i = 1}^m \log x_i^*! + \sum{i = 1}^m x_i \log p_i(\theta).
  $$
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks,fragile]{Hardy-Weinberg Equilibrium}

  \begin{itemize}
    \item Consider alleles expressed as dominant (A) and recessive (a), and they are expressed with rates $1-\theta$ and $\theta$, respectively.
    \item The Hardy-Weinberg principle states genotypes are expressed in large, randomly mixing populations following the Punnett square in Table~\ref{tab:punnett}.
  \end{itemize}

  \begin{table}[ht]
  \centering
  \begin{tabular}{|c|c|c|}\hline
    & A & a \\\hline
  A & $(1-p)^2$ & $p(1-p)$ \\\hline
  a & $p(1-p)$ & $p^2$\\\hline
  \end{tabular}
  \caption{\label{tab:punnett}Genotype frequencies in large, randomly mixing population.}
  \end{table}

  \framebreak

  \begin{itemize}
    \item Since the order of Aa or aA doesn't matter, this results in frequencies for $AA$, $Aa$, and $aa$: $(1-\theta)^2, 2\theta(1-\theta), \theta^2$.
    \item Supposing that we observe a population with these three possible gene expressions. Then, using population counts, we want to fit a multinomial model with $m = 3$ categories, and probabilities defined as:
    $$
    p_1 = (1-\theta)^2, \quad p_2 = 2\theta(1-\theta), \quad p_3 = \theta^2.
    $$
  \end{itemize}

  \framebreak

  \begin{exampleblock}{Blood Types}
    In a Chinese population of Hong Kong in 1937, blood types occurred with the following frequencies, where $M$ and $N$ are erythrocyte antigens: M (342), MN (500), N (187), for a total of 1029 samples. Use a multinomial model to estimate the frequency of alleles and genotypes.

    \mode<article>{
      \begin{itemize}
        \item A naive approach may be to say that $N$ occurs with frequency $\theta^2$, so our estimate would be $\theta = \sqrt{187/1029}$, or approximately 0.4263. However, this seems to ignore some of the information in the table available from the other cell-counts.
        \item Instead, let's do maximum likelihood.
        \item Recall that the multinomial probability model, where the probabilities $p_1, \ldots, p_m$ are functions of $\theta$ has log-likelihood:
        $$
        \ell(\theta) = \log n! - \sum_{i = 1}^m \log x_i^*! + \sum{i = 1}^m x_i \log p_i(\theta).
        $$
        \item Thus, plugging in the Hardy-Weinberg equilibrium values:
    $$
    p_1 = (1-\theta)^2, \quad p_2 = 2\theta(1-\theta), \quad p_3 = \theta^2,
    $$
    we get the log-likelihood of $\theta$:
    \begin{align*}
    \ell(\theta) &= \log n! - \sum_{i = 1}^3 \log x^*_i! + x_1^*\log(1-\theta)^2 + x_2^* \log 2\theta(1-\theta) + x_3^* \log \theta^2\\
    &= \log n! - \sum_{i = 1}^3 \log x^*_i! + (2x_1^* + x_2^*)\,\log(1 - \theta) + (2x_3^* + x_2^*)\log(\theta) + x_2^* \log 2.
    \end{align*}
    \item In the equation above, we don't need to worry that the probabilities sum to 1, since the equations for $p_i(\theta)$ were explicitly designed so that they do. We should check of course that $0 \leq \theta \leq 1$ (though the only critical point does indeed lie in this region).
    \item Taking the derivative with respect to $\theta$, and setting equal to zero, gives the equation:
    $$
    -\frac{2x_1^* + x_2^*}{1-\theta} + \frac{2x^*_3 + x^*_2}{\theta} = 0,
    $$
    which can be solved to give:
    \begin{align*}
      \theta &= \frac{2x^*_3 + x^*_2}{2x_1^* + 2x_2^* + 2x_3^*} \\
             &= \frac{2x^*_3 + x^*_2}{2n}.
    \end{align*}
    \item Plugging in the values from the problem, we get the MLE:
    $$
    \hat{\theta} = 0.4247
    $$

      \end{itemize}
    }

  \end{exampleblock}
  
\end{frame}



\section{Invariance property of the MLE}

\begin{frame}[allowframebreaks]{Invariance property}
\begin{itemize}
  \item We will next talk about general properties of estimators, and introduce some theory.
  \item Before we do that, we still want to discuss point-estimation following the Bayesian approach.
  \item The order of things is a little tricky, but before moving on we will introduce one property of the MLE that can be used in finding the MLE.
  \item The property is known as \alert{invariance}.
  \item Idea: information we have about $\theta$ should be the same information we have about $g(\theta)$ -- we don't gain or loose information by transforming the variable.
\end{itemize}

\framebreak

\begin{block}{Theorem: Invariance property of the MLE}
  \citep[Theorem~7.2.10,][]{casella24} If $\hat{\theta}$ is the MLE of $\theta$, then for any function $g$, $g(\hat{\theta})$ is the MLE of $g(\theta)$. For instance, if $\hat{\theta} = \bar{x}^*_n$, then the MLE of $\sin (\theta)$ is $\sin (\bar{x}^*_n)$.
  
  \mode<article>{
    \begin{itemize}
      \item The proof for when $g$ is a one-to-one function is fairly straight forward, as we can assume an inverse function exists. 
      \item We'll provide a proof sketch without this, following the proof from \citet{casella24}.
      \item First, we will introduce the idea of the \emph{induced} likelihood. The idea is that if $g$ is not one to one, then there could be more than one way to get a particular outcome.
      \item For example, if $g(\theta) = \theta^2$, then $g(2) = g(-2) = 4$.
      \item Thus, the set $A_\eta = \{\theta: g(\theta) = \eta\}$ may have more than one element.
      \item We define the \emph{induced likelihood function}, as:
      $$
      L^*(\eta) = \sup_{\theta \in A_\eta} L(\theta),
      $$
      where for any value $\eta$, $A_\eta = \{\theta: g(\theta) = \eta\}$ is the pre-image of $\eta$ under the map $g$, and $L(\theta)$ is the typical likelihood, conditioned on the data $x^*$.
      \item Note that if $g$ is one-to-one, the $g^{-1}$ exists, and $A_\eta = \{\theta: g(\theta) = \eta\} = g^{1}(\eta)$, and then the induced likelihood function is:
      $$
      L^*(\eta) = L\big(g^{-1}(\eta)\big)
      $$
      \item In general, we note that the definition of $L^*$ implies that the maximums of $L^*$ and $L$ coincide. Now for the proof:
      \end{itemize}
      \begin{proof}
        Let $\hat{\eta}$ denote the value that maximizes $L^*(\eta)$. Then by definition of the maximum and the function $L^*$,
        \begin{align*}
          L^*(\hat{\eta}) &= \sup_\eta L^*(\eta)\\
                          &= \sup_\eta \sup_{\theta \in A_\eta} L(\theta) \\
                          &= \sup_{\theta} L(\theta) \\
                          &= L(\hat{\theta}),
        \end{align*}
        where $\hat{\theta}$ is the MLE of $\theta$. The third equality is because the iterated maximization is equal to the unconditional maximization over $\theta$. That is, the inner $\sup_{\theta \in A_\eta}$ says that, for any fixed $\eta$, we want to get the maximum $\theta$ value. The outer $\sup_\eta$ now says that we want to pick the $\eta$ value that maximizes the inner condition, so any value of $\eta$ is ``up for grabs", so the $\eta$ value that results in the largest $L(\theta)$ is equivalent to picking the largest $\theta$ for $L(\theta)$.
        % Heuristically, this argues that picking the maximum $\hat{\theta}$ based on the maximizing the induced likelihood $L^*(\eta)$ ends up giving the MLE of $L(\theta)$. Next, we show the opposite direction.
        
        Now, recall that $g(\hat{\theta})$ is some value in the $\eta$ space, and thus we can consider the preimage of $g(\hat{\theta})$ as the set of all $\theta$ where $g(\theta) = g(\hat{\theta})$.
        Since $\hat{\theta}$ maximizes the MLE, we have:
        \begin{align*}
        L(\hat{\theta}) &= \sup_{\theta \in A_{g(\hat{\theta})}} \\
                        &= L^*\big(g(\hat{\theta})\big),
        \end{align*}
        The second equality coming from the definition of $L^*$. Combining the first set of equalities with the second, we get:
        $$
        L^*\big(\hat{\eta}\big) = L^*\big(g(\hat{\theta})\big),
        $$
        from which we conclude that if $\hat{\theta}$ is the MLE of $L(\theta)$, then $g\big(\hat{\theta}\big)$ maximizes $L^*(\eta)$, or that $g\big(\hat{\theta}\big)$ is the MLE of $g(\theta)$.
      \end{proof}
  }
  
\end{block}

\framebreak

\begin{itemize}
  \item Note in the previous theorem, nothing is preventing $\theta$ to be multivariate.
  \item Thus, if $\hat{\theta} = (\hat{\theta}_1, \ldots, \hat{\theta}_k)$ is the MLE of $\theta$,
  then $g(\hat{\theta}) = \Big(g\big(\hat{\theta_1}\big), \ldots, g\big(\hat{\theta_k}\big)\Big)$ is the MLE of $g(\theta)$.
  \item For a more concrete example: suppose that $p$ is the probability of an event, a model parameter that we want to estimate. Then if $\hat{p}$ is the MLE, then we can find the MLE for the odds-ratio or log-odds ratio very easily:
  $$
  \text{MLE of odds ratio} = \frac{\hat{p}}{1 - \hat{p}}, \quad \text{MLE of log-odds ratio} = \log \frac{\hat{p}}{1 - \hat{p}}.
  $$
\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Comments on maximum likelihood}
  \begin{itemize}
    \item There are many advantages of MLE, and is one reason that MLE is one of the gold-standards of statistical estimation (the other dominant approach being Bayesian estimation).
    \item Maximum likelihood is generally considered a frequentist approach due to asymptotic theory results, which we cover later. As presented, however, the MLE has more of a Bayesian flavor to it: the likelihood measures some degree of ``belief" about a parameter, and we estimate the parameter by picking the value that maximizes this belief.
    \item Maximum likelihood is not without it's criticisms. For instance: 
    \begin{itemize}
      \item The MLE may not be unique (i.e., more than one global maximum). For instance, $\hat{\theta}_1$ and $\hat{\theta}_2$ may both maximize the likelihood $L(\theta)$; if they are very different estimates, this could cause a problem. It's also possible for an infinite number of $\theta$ values to maximize the likelihood, which again can be problematic.
      \item The MLE is often biased, and this can be particularly problematic in small sample sizes.
      \item The MLE depends on model assumptions. If they are wrong, the estimates might not make much sense.
      \item The MLE might not even exist! Le Cam, a famous 20th century statistician who was largely a proponent of the MLE approach, wrote a fun paper about some of these weaknesses, and warned against blindly using the MLE \citep{lecam90}:
    \end{itemize}
    \begin{quote}
    ``We present a list of principles leading to the construction of good estimates.The main principle says that one should not believe in principles but study each problem for its own sake."" -- \citep{lecam90}
    \end{quote}
    
    \item Even if likelihoods can be maximized numerically, care is needed to ensure that final estimates correspond to global maximums and not just local solutions.
    
    \item Other difficulties arise in more complex models, such as time-series or spatial data. Here, the data are not IID, and the distribution of each observation changes over times / space, and observations are not independent. As a result, it's generally very difficult to write down the likelihood, or find the MLE. Minimizing a loss-function is generally easier in complex settings, even if it results in a loss of information (discussed later), or is equivalent to unrealistic Gaussian approximations (i.e., minimizing MSE). 
  \end{itemize}
\end{frame}

% \begin{frame}[allowframebreaks]{Example: logistic regression}
%   \begin{itemize}
%     \item TODO.
%   \end{itemize}
% \end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version 4.5.2.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4451_s26/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}

\bibliography{../bib4451}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4451}

}



\end{document}


