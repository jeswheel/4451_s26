\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setcounter{tocdepth}{2}}
\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{3}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Joint Distributions}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]

<<setup,include=FALSE,cache=FALSE,purl=FALSE,child="../setup.Rnw">>=
@

\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Introduction}

\begin{frame}{Introduction}
  \begin{itemize}
    \item This material is based on the textbook by \citet[][Chapter~3]{rice07}.
    \item Our goal is to better understand the joint probability structure of more than one random variable, defined on the same sample space.
    \item One reason that studying joint probabilities is an important topic is that it enables us to use what we know about one variable to study another.
    \item Multivariate calculus is not a formal prerequisite for this course. We'll start by presenting a few details from multivariate calculus, as well as some results from analysis.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Partial Derivatives}

\begin{itemize}
  \item A partial derivative is an extension of a uni-variate derivative, where the function is a function of more than one variable.
\end{itemize}

\begin{block}{Definition: Partial Derivative}
  Let $f(\mathbf{x})$ be a function $f: \, \R^d \rightarrow \R$. The partial derivative of $f$ with respect to $x_i$, where $i \in \{1, 2, \ldots, d\}$ is
  \begin{align*}
  \frac{\partial}{\partial x_i}f(\mathbf{x}) &= \frac{\partial f}{\partial x_i} \\
  &= \lim_{h\rightarrow 0} \frac{f(x_1, x_2, \ldots, x_i + h, \ldots, x_d) - f(x_1, x_2, \ldots, x_d)}{h}.
  \end{align*}
\end{block}

\begin{itemize}
  \item We don't have to worry so much about the definition in this class. The thing to notice is that it's a directional derivative.
  \item Holding all other variables constant, it tells us the slope of the function in the $x_i$ direction.
  \item Because it's definition is a limit, similar to the uni-variate case, the same derivative rules that you are familiar with from calculus also apply to partial derivatives, after treating the remaining variables as constants.
\end{itemize}

\begin{exampleblock}{Example}
  Let $f(x_1, x_2, x_3) = x_1^2x_2 - 10x_2^2x_3^3 + 43x_1 - 7\tan(4x_2)$. Find $ \frac{\partial}{\partial x_i} f(x_1, x_2, x_3)$ for all $i \in \{1, 2, 3\}$.
\end{exampleblock}

\begin{block}{Schwarz's Theorem}
  Let $f: \, \R^d \rightarrow \R$ be twice-differentiable function. Higher order derivatives are written:
  $$
  \frac{\partial^2}{\partial x_i\partial x_j} f = \frac{\partial}{\partial x_i} \left(\frac{\partial}{\partial x_j} f\right).
  $$
  That is, we first take the derivative with respect to $x_j$, and then $x_i$. Shwarz's theorem states that the order can be swapped: 
  $$
  \frac{\partial^2}{\partial x_i\partial x_j} f = \frac{\partial^2}{\partial x_j\partial x_i} f.
  $$
\end{block}

\end{frame}

\begin{frame}[allowframebreaks]{Higher order integrals}
  \begin{itemize}
    \item The next thing that we will review is higher-order integrals.
    \item This is a complex topic that we can't cover in detail in this class, but they do appear in this chapter and the next.
    \item For now, we'll just show some simple results that will be useful for our calculations. 
    
    \framebreak
    
    \item Recall one definition of integrals in the uni-case: calculating the area under the curve using an infinite Reimann approximation.
    \item That is, suppose we want to find the area under the function $f(x)$, on the interval $[a, b]$. We can use small, uniform sized rectangles to approximate the area.
    \item Let each rectangle have width $\Delta x$, with midpoint $x^*_i$, where the height of the $i$th rectangle is $f(x^*_i)$. Then:
    $$
    A \approx f(x^*_1)\Delta x + f(x^*_2)\Delta x + \ldots + f(x^*_n) \Delta x.
    $$
    \item The exact area is equal to the limit as the boxes get smaller (and the number of boxes goes to infinity):
    $$
    A = \int_{a}^b f(x)\, dx = \lim_{n \rightarrow \infty} \sum_{i = 1}^n f(x^*_1)\, \Delta x.
    $$
    \item We now want to do the same thing, but in higher dimensions. Instead of using small rectangles to approximate the area, we now need to use small rectangular prisms.
    \item Let $z = f(x, y)$. Our goal is to find the volume between the function $f$ and the $x, y$ plane.
    \item We construct boxes such that the height of the $i$th box is given by $f(x^*_i, y^*_j)$.
    \item Then, the base of the rectangular prisms has area $\Delta A = \Delta x_i\times\Delta y_i$, and the height is $f(x^*_i, y^*_i)$.
    
\begin{figure}
    \centering
    \includegraphics[width=0.35\linewidth]{doubleInt.png}
    \caption{Credit: Paul's Online Notes: Section 15.1}
    \label{fig:doubleInt}
\end{figure}
    
    \item The total volume is approximately given by summing up the volume of each box: 
    $$
    V \approx \sum_{i = 1}^n \sum_{j = 1}^m f(x^*_i, y^*_j) \Delta A.
    $$
    \item We now can get the exact volume by taking the limit:
    $$
    V = \iint_R f(x, y)\, dA = \lim_{n, m \rightarrow \infty} \sum_{i = 1}^n \sum_{j = 1}^m f(x^*_i, y^*_j) \Delta A
    $$
    \item \emph{Fubini's theorem} gives us the standard approach to calculating these integrals
  \end{itemize}
  
  \begin{block}{Fubini's Theorem}
    If we are evaluating a double integral on a rectangular region $R = X \times Y$, then the integral can be calculated as iterated uni-variate integrals:
    \begin{align*}
    \iint_R f(x, y)\, dA &= \iint_{X \times Y} f(x, y)\, d(x, y) \\
    &= \int_X\left(\int_Y f(x, y)\, dy\right)\, dx \\
    &= \int_Y\left(\int_X f(x, y)\, dx\right)\, dy.
    \end{align*}
  \end{block}
  
  \begin{exampleblock}{example}
    Let $R = [2, 4] \times [1, 2]$. Find
    $$
    \iint_R 6xy^2 \, dA.
    $$
  \end{exampleblock}
  
  \framebreak
  
  \begin{itemize}
    \item One thing to note about higher-order integrals is that the area we are integrating ($dA$) doesn't have to be rectangular, and the limits of integration can be functions themselves.
    \item We'll see a few examples of this later, and I'll try to walk through these examples carefully. However, this isn't a class on multivariate calculus, so you won't be required to do calculations beyond what you can do using Fubini's theorem.
    \item There is a nice example in our textbook for polar-coordinate integration, which is perhaps the most common extension \citep[][Example~A, Chapter 3.6.2]{rice07}.
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Swapping order of limits and integrals}
  \begin{itemize}
    \item An important result from measure theory (often covered in an analysis class) is known as the Dominated Convergence Theorem.
    \item We won't prove the theory here, nor state the complete theory, but we will state parts of it that will be useful to us.
    \item A proof of the parts of the theory that we are interested in can be found in our supplement text \citep[Section~2.4 of][]{casella24}.
    \item A more complete treatment of this idea can be found in \citet[Chapter~5,][]{resnick19}.
    \item The next few theorems are all special cases of the Dominated Convergence Theorem.
    \end{itemize}
  
  \framebreak
    
    \begin{block}{Leibnitz's Rule}
    If $f(x, \theta)$ is differentiable with respect to $\theta$, then
    $$
    \frac{d}{d\theta} \int_{a}^{b} f(x, \theta)\, dx = \int_{a}^b \frac{\partial}{\partial \theta} f(x, \theta)\, dx.
    $$
    \end{block}

    \begin{itemize}
    \item This is actually a special case of Leibnitz's Rule, which gives an expression if the limits of integration $a$, $b$, are actually differentiable functions of $\theta$: $a(\theta), b(\theta)$.
    \item Thus, if the integral has finite range, we can change the order of the integral and derivative.

  \end{itemize}
    
    \framebreak
    
  \begin{itemize}
    \item Next, we consider changing the order of a limit and an integral.
    
  \end{itemize}
  
  \begin{block}{Theorem: Limits and Integrals}
    Suppose the function $h(x, y)$ is continuous at $y_0$ for each $x$, and there exists a function $g(x)$ satisfying
    \begin{enumerate}[(i)]
      \item $|h(x, y)| \leq g(x)$ for all $x$ and $y$. 
      \item $\int_{-\infty}^\infty g(x)\, dx < \infty$.
    \end{enumerate}
    Then
    $$
    \lim_{y \rightarrow y_0} \int_{-\infty}^\infty h(x, y)\, dx = \int_{-\infty}^\infty \lim_{y \rightarrow y_0} h(x, y)\, dx.
    $$
  \end{block}
  
  \framebreak
  
  \begin{itemize}
    \item Note that derivatives are just special types of limits:
    $$
    \frac{\partial}{\partial \theta}f(x, \theta) = \lim_{\delta \rightarrow 0} \frac{f(x, \theta + \delta) - f(x, \theta)}{\delta}.
    $$
    \item Thus, theorems about interchanging integrals and integrals can be worked into a theorem about interchanging integrals and derivatives
  \end{itemize}
  
  \framebreak
  
  \begin{block}{Corollary: Derivatives and Integrals}
    Suppose $f(x, \theta)$ is differentiable in $\theta$ and there exists a function $g(x, \theta)$, and $\delta_0 > 0$ such that
    $$
    \left|\frac{\partial}{\partial \theta} f(x, \theta)\right|_{\theta = \theta'} \leq g(x, \theta), \quad \text{for all } \theta' \text{ such that } |\theta' - \theta| \leq \delta_0,
    $$
    and $\int_{-\infty}^\infty g(x, \theta) \, dx < \infty$. Then,
    $$
    \frac{d}{d\theta} \int_{-\infty}^\infty f(x, \theta)\, d\theta = \int_{-\infty}^\infty \frac{\partial}{\partial \theta} f(x,\theta)dx.
    $$
  \end{block}
  
  \framebreak
  
  % \begin{exampleblock}{Example}
  %   Let $X \sim N(\mu, 1)$. Calculate $\frac{d}{dt} \E[e^{tX}]$, or:
  %   $$
  %   \frac{d}{dt}\left(\frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{tx} e^{(-(x - \mu)^2/2)}\, dx\right)
  %   $$
  % \end{exampleblock}
  % 
  % \framebreak
  
  \begin{itemize}
    \item Similar results hold for interchanging summation, derivatives, and integrals.
    \item For finite sums, changing the order of sums and derivatives is a natural property of derivatives. It's not so simple with infinite sums.
  \end{itemize}
  
  \begin{block}{Exchanging summation and derivatives}
    Suppose that $\sum_{x = 0}^\infty h(\theta,x)$ converges for all $\theta$ in an interval $(a, b)$, and $(i)$ $\frac{\partial}{\partial \theta} h(\theta, x)$ is continuous in $\theta$ for each $x$, and $(ii)$ $\sum_{x = 0}^\infty \frac{\partial}{\partial \theta} h(\theta, x)$ converges uniformly on every closed bounded subinterval of $(a, b)$. Then
    $$
    \frac{d}{d\theta} \sum_{x = 0}^\infty h(\theta, x) = \sum_{x = 0}^\infty \frac{\partial}{\partial \theta} h(\theta, x).
    $$
  \end{block}
  
  \begin{itemize}
    \item The necessary conditions stated above will hold for nearly all functions we'll consider in this class. You won't be asked to verify these conditions.
  \end{itemize}
  
  \begin{block}{Exchaning sums and integrals}
    Suppose the series $\sum_{x = 0}^\infty h(\theta, x)$ converges uniformly on $[a, b]$ and that, for each $x$, $h(\theta, x)$ is a continuous function of $\theta$. Then
    $$
    \int_{a}^{b} \sum_{x = 0}^\infty h(\theta, x)\, d\theta = \sum_{x = 0}^\infty \int_{a}^b h(\theta, x)\, d\theta.
    $$
  \end{block}
  
  \begin{itemize}
    \item Like with differentiation, we can always exchange the order of summation and integration, if the sums are finite.
    \item You will not be asked to verify the conditions above, but I wanted to point out that these operations are theoretically supported when they inevitably come up in this class. 
  \end{itemize}
  
  % TODO: Add example 2.4.5, but don't talk about expectation yet. In fact, maybe this needs to move to Chapter~4? 
  % TODO: Add stuff about partial derivatives, multiple integrals, and at least talk about polar coordinates. 
  
  % \framebreak
    
  %   \begin{block}{Leibnitz's Rule}
  %   If $f(x, \theta)$ is differentiable with respect to $\theta$, then
  %   $$
  %   \frac{d}{d\theta} \int_{a}^{b} f(x, \theta)\, dx = \int_{a}^b \frac{\partial}{\partial \theta} f(x, \theta)\, dx.
  %   $$
  %   \end{block}
  %   
  %   \begin{itemize}
  %   \item This is actually a special case of Leibnitz's Rule, which gives an expression if the limits of integration $a$, $b$, are actually differentiable functions of $\theta$: $a(\theta), b(\theta)$.
  %   \item Thus, if the integral has finite range, we can change the order of the integral and derivative.
  %    
  % \end{itemize}
\end{frame}

\section{Joint Probabilities}

\begin{frame}[allowframebreaks]{Joint cdf}
  \begin{itemize}
    \item Just like the univariate case, the joint behavior of two random variables, $X$ and $Y$, is determined by the cumulative distribution function
    $$
    F(x, y) = P(X \leq x, Y \leq y).
    $$
    \item This is true for both discrete and continuous random variables.
    \item Thus, any set $A \subset \R^2$, the joint cdf can give $P\big((X, Y) \in A\big)$. For continuous random variables:
    $$
    P\big((X, Y) \in A\big) = \iint_A f(x, y)\, dy\,dx.
    $$
  \end{itemize}
  
  \framebreak

  \begin{itemize}
    \item For example, let $A$ be the rectangle defined by $x_1 < X < x_2$, and $y_1 < Y < y_2$. (It helps to draw a picture...)
    \item $F(x_2, y_2)$ gives $P(X < x_2, Y < y_2)$, an area that is too big, so we subtract off pieces
    \begin{itemize}
      \item $F(x_2, y_1) = P(X < x_2, Y < y_1)$ (we already have the area $X < x_2$, but now subtract away the area $Y < y_1$).
      \item $F(x_1, y_2) = P(X < x_1, Y < y_2)$ (Now subtracting the area $X < x_1$)
      \item We have ``double subtracted" the area $\{X < x_1, Y < y_1\}$, so we add it back.
    \end{itemize}
    \end{itemize}
  
  $$P\big((X, Y) \in A\big) = F(x_2, y_2) - F(x_2, y_1) - F(x_1, y_2) + F(x_1, y_1).$$
  
  \framebreak
  
  \begin{itemize}
    \item The definition also applies to more than two random variables. 
    \item Let $X_1, \ldots, X_n$ be jointly distributed random variables defined on the same sample space. Then
    $$
    F(x_1, x_2, \ldots, x_n) = P(X_1 \leq x_1, X_2 \leq x_2, \ldots, X_n \leq x_n).
    $$
    
    \item Like the univariate case, we can also define the pmf and pdf of jointly distributed random variables as well.
    \item For discrete random variables, the pmf just describes the joint frequencies: $P(X = x_i, Y = y_j)$ for all $i, j$.
    
    \framebreak
    
    \item For continuous random variables, a joint pdf of $(X, Y)$ is defined as functions $f: \, \R^2 \rightarrow \R$ that satisfy
    $$
    F(X, y) = \int_{-\infty}^x \int_{-\infty}^y f(s, y)\, ds\, dt.
    $$
    \item The \emph{bivariate}-Fundamental Theorem of Calculus implies:
    $$
    \frac{\partial^2 F(x, y)}{\partial x \,\partial y} = f(x, y).
    $$
    \item Similar ideas hold for higher-dimensions.
  \end{itemize}
  
\end{frame}

\section{Discrete Random Variables}

\begin{frame}[allowframebreaks]{Discrete Random Variables}

  \begin{block}{Definition: Joint pmf}
    Let $X$ and $Y$ be discrete random variables define on the same sample space, and take on values $x_1, x_2, \ldots$ and $y_1, y_2, \ldots$, respectively. The \alert{joint pmf} (or joint frequency function), is
    $$
    p(x_i, y_j) = P(X = x_i, Y = y_j).
    $$
  \end{block}

  \begin{itemize}
    \item For discrete RVs, it's often useful to describe the joint pmf as a frequency table.
  \end{itemize}

  \framebreak

  \begin{itemize}
    \item Suppose a fair coin is tossed 3 times. Let $X$ denote the number of heads on the first toss, and $Y$ the total number of heads.
    \item The sample space is
    $$
    \Omega = \{hhh, hht, hth, thh, htt, tht, tth, ttt\}.
    $$
    \item The joint pmf can be expressed as the frequency table below (Table~\ref{tab:freq}).
  \end{itemize}

  \begin{table}
  \centering
  \begin{tabular}{c|cccc}
    & $y$             &             &                 &              \\\hline
  $x$ &           $0$ &           $1$ &           $2$ &          $3$ \\\hline
  $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ &          $0$ \\
  $1$ &           $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
  \end{tabular}
  \caption{\label{tab:freq}Frequency table for $X$ and $Y$, flipping a fair coin three times.}
  \end{table}
  
  \begin{itemize}
    \item Note that the probabilities in Table~\ref{tab:freq} sum to one.
    \item Using the probability laws we have already learned, we can calculate \alert{marginal} probabilities. 
    
    \framebreak
    
    \begin{align*}
      p_Y(0) &= P(Y = 0) \\
      &= P(Y = 0, X = 0) + P(Y = 0, X = 1) \\
      &= \frac{1}{8} + 0 = \frac{1}{8} \\
      p_Y(1) &= P(Y = 1) \\
      &= P(Y = 1, X = 0) + P(Y = 1, X = 1) \\
      &= \frac{2}{8} + \frac{1}{8} = \frac{3}{8}.
    \end{align*}
    
    \framebreak

    \item In general, to find the frequency function for $Y$ and $X$, we just need to sum the appropriate columns or rows, respectively.
    \item $p_X(x) = \sum_i P(x, y_i)$ and $p_Y(y) = \sum_{j} P(x_j, y)$.
    \item The case with multiple random variables is similar:
    $$
    p_{X_i}(x_i) = \sum_{x_j: j\neq i} p(x_1, x_2, \ldots, x_n).
    $$
    \item We can also get marginal frequencies for more than one variable:
    $$
    p_{X_iX_j}(x_i, x_j) = \sum_{x_k: k \notin \{i, j\}} p(x_1, x_2, \ldots, x_n).
    $$
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Multinomial Distribution}
  
  \begin{itemize}
    \item The \alert{multinomial} distribution is a generalization of the binomial distribution.
    \item Suppose there are $n$ independent trials, each with $r$ possible outcomes, with probabilities $p_1, p_2, \ldots, p_r$, respectively.
    \item Let $N_i$ be the total number of outcomes of type $i$ in the $n$ trials, with $i \in \{1, 2, \ldots, r\}$.
    \item The probability of any particular sequence $(N_1, N_2, \ldots, N_r) = (n_1, n_2, \ldots, n_r)$ is
    $$
    p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
    \item The total number of ways to do this was an identity from Chapter~1 (Proposition~1.3):
    $$
    \binom{n}{n_1\, \cdots\, n_r}.
    $$
    \item Combining this gives us the pmf of the multinomial distribution:
  \end{itemize}
  
  \begin{block}{Multinomial Distribution}
    Let $N_1, N_2, \ldots, N_r$ be random variables that follow a multinomial distribution with parameters $N$ and $(p_1, \ldots, p_r)$. The joint pmf is
    $$
    p(n_1, n_2, \ldots, n_r) = \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}
    $$
  \end{block}
    
    \begin{itemize}
      \item The marginal distribution for any $N_i$ can be found by summing the joint frequency function over the other $n_j$.
      \item While possible, this is a non-trivial algebraic exercise.
      \item The simple alternative is to reframe the problem: Let $N_i$ be the number of successes in $n$ trials, and $\tilde{N}_i = \sum_{j \neq i}N_j$ be the number of failures. The probability of success is still $p_i$, leaving the probability of failure to be $1 - p_i$. 
      \item Thus, we see that the marginal distribution for $N_i$ must follow a binomial distribution:
      \begin{align*}
      p_{N_i}(n_i) &= \sum_{n_j: j \neq i} \binom{n}{n_1\, \cdots\, n_r}p_1^{n_1}p_2^{n_2}\cdots p_r^{n_r}\\
      & = \binom{n}{n_i}p_i^{n_i}(1 - p_i)^{n-n_i}
      \end{align*}
    \end{itemize}
  
\end{frame}

\section{Continuous Random Variables}

\begin{frame}[allowframebreaks]{Continuous Random Variables}
  \begin{itemize}
    \item Let $X, Y$ be continuous random variables with joint cdf $F(x, y)$.
    \item Their \alert{joint density function} is a piecewise continuous function of two variables, $f(x, y)$.
    \item A few properties: 
    \begin{itemize}
      \item $f(x, y) \geq 0$ for all $(x, y) \in \R$ (or the support).
      \item $\int_{-\infty}^\infty\int_{-\infty}^\infty f(x, y) \,dx\,dy = 1$.
      \item For any ``measureable set" $A \subset \R^2$, $P\big((X, Y) \in A\big) = \int\int_A f(x, y) dxdy$
      \item In particular, $F(x, y) = \int_{-\infty}^x\int_{-\infty}^y f(u, v)\,du\,dv$.
    \end{itemize}
    \item From the fundamental theorem of multivariable calculus, it follows that
    $$
    f(x, y) = \frac{\partial^2}{\partial x\partial y}F(x, y),
    $$
    wherever the derivative is defined.
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Finding joint probabilities}
    Let $X, Y$ be jointly defined RVs with pdf
    $$
    f(x, y) = \frac{12}{7}(x^2 + xy), \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.
    $$
    Find $P(X > y)$.
    
    \mode<article>{
    \begin{align*} 
      P(X > Y) &= \frac{12}{7}\int_{0}^1\int_{0}^x (x^2 + xy)dydx \\
      &= \frac{9}{14}.
    \end{align*}
    }
    
  \end{exampleblock}
  
  \mode<presentation>{
    \emph{Solution:}
  }
  
  
\end{frame}

\begin{frame}[allowframebreaks]{Marginal cdf}
    The \alert{marginal cdf} of $X$, denoted $F_X$, is
    \begin{align*}
      F_X(x) &= P(X \leq x) \\
      &= P(X \leq x \cap Y \in \R) = P(X \leq x \cap Y < \infty)\\
      &= \lim_{y \rightarrow \infty} F(x, y) \\
      &= \int_{-\infty}^x\int_{-\infty}^{\infty} f(u, y)\,dy\,du.
    \end{align*}
    By taking the derivative of both sides of the equation, we get the \alert{marginal density} of $X$:
    $$
    f_X(x) = F_{X}'(x) = \int_{-\infty}^\infty f(x, y)\,dy.
    $$
    
    \framebreak
    
    \begin{exampleblock}{Calculating Marginal Densities}
      Using the same joint distribution as the previous example, find the marginal density of $X$.
      
    $$
    f(x, y) = \frac{12}{7}(x^2 + xy), \quad 0 \leq x \leq 1, \quad 0 \leq y \leq 1.
    $$
    
    \mode<article>{
      \begin{align*}
      f_X(x) &= \int_Y f(x, y) dy \\
      &= \frac{12}{7} \int_{0}^1 (x^2 + xy)dy \\
      &= \frac{12}{7} \Big(x^2y + \frac{x}{2}y^2\Big)\Big|^{1}_{0} \\
      &= \frac{12}{7} \Big(x^2 + \frac{x}{2}\Big)
      \end{align*}
    }
    \end{exampleblock}
\end{frame}

\begin{frame}{More than two random variables}
  \begin{itemize}
    \item For several jointly continuous random variables, we can make the obvious generalizations.
    \item That is, to find the \emph{marginal} densities, we need to ``marginalize-" or ``integrate-" out the \alert{nusaince} variables.
    \item This means integrating out any combination of variables that we want.
    \item Example: Let $X$, $Y$, and $Z$ be jointly continuous RVs with pdf $f(x, y, z)$. Then the two-dimensional marginal distribution of $X$ and $Z$ is: 
    $$
    f_{XZ}(x, z) = \int_{-\infty}^\infty f(x, y, z) \,dy.
    $$
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Example: constructing bivariate cdfs}
  \begin{itemize}
    \item Suppose that $F(x)$ and $G(y)$ are cdfs for random variables $X$ and $Y$, resp. 
    \item It can be shown that the following function, $H(x, y)$, is always a bivariate cdf for all $-1 \leq \alpha \leq 1$:
    $$
    H(x, y) = F(x)G(y)\Big(1 + \alpha \big(1 - F(x)\big)\big(1 - G(y)\big)\Big).
    $$
    \item Because $\lim_{x \rightarrow \infty} F(x) = \lim_{y \rightarrow \infty} G(x) = 1$, the marginal distributions are:
    \begin{align*}
      \lim_{y \rightarrow \infty}H(x, y) &= F(x) \\
      \lim_{x \rightarrow \infty}H(x, y) &= G(y)
    \end{align*}
    \item Thus, we can use this approach to build an infinite number of biviariate distributions that have a particular marginal distribution.
    
    \framebreak
    
    \item One important example is when the marginal distributions are uniformly distributed.
    \item Let $F(x) = x, 0 \leq x \leq 1$, and $G(y) = y, 0 \leq y \leq 1$.
    \item By selecting $\alpha = -1$, we have 
      \begin{align*}
        H(x, y) &= xy[1 - (1-x)(1-y)]\\
        &= x^2y + y^2x - x^2y^2, \quad 0\leq x, y \leq 1.
      \end{align*}
    \item The density is
      \begin{align*}
        h(x, y) &= \frac{\partial^2}{\partial x\partial y}H(x, y) \\
        &= 2x + 2y - 4xy, \quad 0 \leq x, y \leq 1.
      \end{align*}
    \item \href{https://www.desmos.com/3d/p0makz897t}{Here is a link} to a 3D rendering of this function.

  \framebreak
  
    \item Now, let's select $\alpha = 1/2$:
    \begin{align*}
      H(x, y) &= xy\Big(1 + \frac{1}{2}\big(1 - F(x)\big)\big(1 - G(y)\big)\Big) \\
      &= \frac{1}{2}x^2y^2 - \frac{1}{2}x^2y - \frac{1}{2}xy^2 + \frac{3}{2}xy.
    \end{align*}
    \item Taking the derivative, we get:
    \begin{align*}
      h(x, y) &= \frac{\partial^2}{\partial x\partial y}H(x, y) \\
      &= 2xy - x - y +\frac{3}{2}, \quad 0 \leq x, y \leq 1.
    \end{align*}
    \item \href{https://www.desmos.com/3d/otohcn69lx}{Here is a link} to a 3D rendering of this function.
  
  \framebreak
  
    \item The last two joint cdfs were examples of a \alert{copula}. 
    
    \begin{block}{Definition: Copulas}
      A copula is a joint cdf that has uniform marginal distributions.
    \end{block}
    
    \item Let $C(u, v)$ be a copula. One immediate consequence of the definition is that if $U$ and $V$ are uniform random variables, then $P(U \leq u) = C(u, 1) = u$, and $P(V \leq v) = C(1, v) = v$. 
    
    \framebreak
    
    \item Let $C(u, v)$ be a copula, we will restrict ourselves to the case where it is twice differentiable, such that $c(u,v) = \frac{\partial^2}{\partial u\partial v} C(u, v) \geq 0$.
    \item let $F_X$ and $F_Y$ be the cdfs of $X$ and $Y$, resp.
    \item Now define $U = F_X(X)$, and $V = F_Y(Y)$. From Proposition~2.2, $U$ and $V$ are uniformly distributed.
    \item Now consider the function $H(x, y) = C(u, v) = C\big((F_X(x), F_Y(y)\big)$.
    \item Thus, by the property that $C(u, 1) = u$ and $C(1, v) = v$, we have
    \begin{align*}
      C\big(F_X(x), 1\big) &= F_X(x) \\
      C\big(1, F_Y(y)\big) &= F_Y(y).
    \end{align*}
    Therefore by definition, $F_{XY}(x, y) = H(x, y) = C\big((F_X(x), F_Y(y)\big)$.
    \item Using the chain rule, we can differentiate to obtain
    $$
    f_{XY}(x, y) = c\big(F_X(x), F_Y(y)\big)f_X(x)f_Y(y).
    $$
    
    \framebreak
    
    \item \alert{Takeaway:} We took arbitrary marginal distributions $F_X$ and $F_Y$, and created a family of joint density functions, defined by \emph{any} copula. Thus: the marginal distributions do not determine the joint distribution.
    \item There is a Theorem known as Sklar's Theorem \citep{wikiSklar} that generalizes this statement: All joint distributions can be expressed using a copula and marginal distributions, \emph{and} the representation is unique.
    \item That is, the copula can be thought of as a way to describe the dependence between the variables in any joint distribution. 
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Uniform on specific region}

\begin{itemize}
  \item So far when we have talked about \emph{uniform distributions}, we think about being uniform over $[0, 1]$, or a higher dimensional box: $[a, b]^d$.
  \item It's often useful to have a uniform distribution for other regions of space.
  \item Let $R\subset \R^2$ be any region of interest. The two-dimensional uniform distribution over $R$ is defined by the probability
  $$
  P\big((X, Y) \in A\big) = \frac{|A|}{|R|},
  $$
  where $|\,|$ denotes the measure of the area.
\end{itemize}

\framebreak

\begin{itemize}
  \item Example: Suppose a point is chosen randomly in a disk of radius $1$.
  \item The area of the disk is $\pi r^2 = \pi$, and therefore the joint pdf for the location $(X, Y)$ is
  $$
  f(x, y) = \begin{cases} \frac{1}{\pi} & x^2 + y^2 \leq 1 \\ 0 & \text{otherwise}\end{cases}
  $$
  \item For practice defining new random variables, let $R$ be the random variable denoting the distance of the point from the origin.
  \item Note that $R \leq r$ if and only if the point lies in a disk of radius $r$. This disk has area $\pi r^2$, and therefore because of uniform probability, the joint probability is
  $$
  P(R \leq r) = \frac{\pi r^2}{\pi} = r^2, \quad 0 \leq r \leq 1.
  $$
  \item Taking a derivative, the corresponding density function is 
  $$f_R(r) = 2r, \quad 0 \leq r \leq 1.$$
  \item Transforming random variables from one coordinate system to another is an important idea, one we will talk more about later.
  
  \framebreak 
  
  \item Now let us compute the marginal density of the $x$ coordinate: 
  
  \mode<article>{
  \begin{align*}
    f_X(x) &= \int_{-\infty}^\infty f(x, y)dy \\
    &= \int_{-\infty}^\infty \frac{1}{\pi}\times 1[x^2 + y^2 \leq 1]dy\\
    &= \int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \frac{1}{\pi}dy \\
    &= \frac{2}{\pi} \sqrt{1 - x^2}, \quad -1 \leq x \leq 1.
  \end{align*}
  }
  
\end{itemize}

\end{frame}

\section{Independent Random Variables}

\begin{frame}[allowframebreaks]{Independence}
  \begin{block}{Definition: Independent Random Variables}
    Random variables $X_1, X_2, \ldots, X_n$ are said to be \alert{independent} if their joint cdf factors into the product of their marginal cdf's:
    $$
    F(x_1, x_2, \ldots, x_n) = F_{X_1}(x_1)F_{X_2}(x_2)\cdots F_{X_n}(x_n)
    $$
    for all $x_1, x_2, \ldots, x_n$.
  \end{block}
  
  \begin{itemize}
    \item This definition holds for both continuous and discrete random variables.
    \item For discrete RVs, it is equivalent to state that their joint pmf factors.
    \item For continous RVs, it is equivalent to state that their joint pdf factors.
    
    \framebreak
    
    \item To see why this is true, consider the case of two RVs, $X, Y$.
    \item From the definition, if they are independent, then $F(x, y) = F_X(x)F_Y(y)$.
    \item Taking the second mixed partial derivative makes it immediately clear that the joint pdf $f(x, y)$ factors (assuming all densities exist).
    
    \framebreak
    
    \item Conversely, suppose that the densities factor. Then by definition:
    
    
    \mode<article>{
    \begin{align*}
      F(x, y) &= \int_{-\infty}^x \int_{-\infty}^y f(u, v) dvdu \\
      &= \int_{-\infty}^x \int_{-\infty}^y f_X(u)f_Y(v) dvdu \\
      &= \Big(\int_{-\infty}^x f_X(u) du\Big)\Big(\int_{-\infty}^y f_Y(v) dv\Big) \\
      &= F_X(x)F_Y(y).
    \end{align*}
    }
    
    \framebreak
    
    \item It can also be shown that the definition implies that if $X$ and $Y$ are independent, then
    $$
    P(X \in A, Y \in B) = P(X \in A)P(Y \in B).
    $$
    \item Furthermore, if $g$ and $h$ are functions on $\R$, then $Z = g(X)$ and $W = h(Y)$ are also independent.
  \end{itemize}
  
\end{frame}

\section{Conditional Distributions}

\subsection{Discrete random variables}

\begin{frame}[allowframebreaks]{Conditional distributions: discrete RVs}
  \begin{itemize}
    \item If $X$ and $Y$ are jointly distributed discrete RVs, the \emph{conditional probability} that $X = x_i$ given that $Y = y_i$ is
    \begin{align*}
      P(X = x_i | Y = y_i) &= \frac{P(X = x_i, Y = y_i)}{P(Y = y_i)} \\
      &= \frac{p_{XY}(x_i, y_i)}{P_Y(y_i)},
    \end{align*}
    \item If $p_Y(y_i) = 0$, the probability above is defined to be zero.
    \item We denote this conditional probability as $p_{X|Y}$. 
    \item It's important to note that the conditional pmf is a genuine pmf, as it is non-negative and sums to one.
    \item If $X$ and $Y$ are independent, $p_{Y|X}(y|x) = p_Y(y)$.
  
  \framebreak
  
    \item Let's return to a previous joint pmf example (Table~\ref{tab:freq2}).
  
  \end{itemize}

  \begin{table}
  \centering
  \begin{tabular}{c|cccc}
    & $y$             &             &                 &              \\\hline
  $x$ &           $0$ &           $1$ &           $2$ &          $3$ \\\hline
  $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$ &          $0$ \\
  $1$ &           $0$ & $\frac{1}{8}$ & $\frac{2}{8}$ & $\frac{1}{8}$
  \end{tabular}
  \caption{\label{tab:freq2}Frequency table for $X$ and $Y$, flipping a fair coin three times.}
  \end{table}
  
  \begin{itemize}
    \item The conditional frequency function of $X$ given $Y = 1$ is:
    \begin{align*}
      p_{X|Y}(0|1) &= \frac{2/8}{3/8} = 2/3 \\
      p_{X|Y}(1|1) &= \frac{1/8}{3/8} = 1/3
    \end{align*}
  \end{itemize}
 
 \framebreak
 
 \begin{itemize}
  \item The definition of the conditional frequency can be reexpressed as
  $$
  p_{XY}(x, y) = p_{X|Y}(x|y)p_Y(y).
  $$
  \item By summing up over all possible values of $y$, we have the following
  $$
  p_X(x) = \sum_y p_{X|Y}(x|y)p_Y(y).
  $$
  \item Both of these identities resemble what we have already seen previously when talking about probabilities: The multiplication principle and the law of total probability.
 \end{itemize}
 
 \framebreak
  
  \begin{exampleblock}{Example: Counting particles}
    Suppose that a particle counter is imperfect; for each particle, it detects the particle with probability $0 < p < 1$. If the number of incoming particles in a unit of time is a Poisson distribution with parameter $\lambda$, what is the distribution of the number of counted particles? 
  \end{exampleblock}
  
  \mode<article>{
  Let $N$ denote the true number of particles, and $X$ the number of counted particles. Because the probability that a particle is counted is independent, then if $N = n$, we have $n$ independent Bernoulli random variables.
  In other words, $X|N = n$ has a binomial distribution with parameters $n$ and $p$. By the law of total probability,
  \begin{align*}
    P(X = k) &= \sum_{n} p_{X|N}(x|n)p_N(n) \\
    &= \sum_{n = k}^\infty \binom{n}{k}p^k(1-p)^{n-k} \frac{\lambda^ne^{-\lambda}}{n!} \\
    &= p^ke^{-\lambda}\sum_{n = k}^\infty \frac{n!}{(n-k)!k!}(1-p)^{n-k} \frac{\lambda^{(n-k)}\lambda^k}{n!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}\sum_{n = k}^\infty \lambda^{n-k}\frac{(1-p)^{n-k}}{(n-k)!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}\sum_{j = 0}^\infty \lambda^{j}\frac{(1-p)^{j}}{j!} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda}e^{\lambda(1-p)} \\
    &= \frac{(\lambda p)^k}{k!}e^{-\lambda p}
  \end{align*}
  And therefore we see that the distribution of $X$ is a Poisson distribution with parameter $\lambda p$. This is a useful derivation for any situation where events occur following a Poisson process, and then with some probability $p$ and additional, conditional event occurs. For instance, $N$ might be the number of traffic accidents in a given time period, and each accident being fatal or non-fatal with probability $p$. Then $X$ would be the number of fatal accidents.
  }
  
\end{frame}

\subsection{Continuous Random Variables}

\begin{frame}[allowframebreaks]{The continuous case}
  \begin{itemize}
    \item Although a formal argument is beyond the scope of this course, the definition for conditional density of $Y|X$ will be analogous to the discrete case.
  \end{itemize}
  
  \begin{block}{Definition: Conditional density}
    Let $X,Y$ be jointly continuous random variables with joint density $f_{XY}(x, y)$ and marginal densities $f_X(x)$ and $f_Y(y)$, respectively. Then the conditional density of $Y$ given $X$ is defined to be
    $$
    f_{Y|X}(y|x) = \frac{f_{XY}(x, y)}{f_X(x)},
    $$
    if $0 < f_X(x) < \infty$, and $0$ otherwise.
  \end{block}
  
  \framebreak
  
  \begin{itemize}
    \item A heuristic argument of why this definition makes sense is provided in \citet[][Section~3.5.2]{rice07} using differentials. 
    \item With our definition, we can express the joint density in terms of the marginal and conditional densities:
    $$
    f_{XY}(x, y) = f_{Y|X}(y | x)f_X(x).
    $$
    \item We often use this expression to find marginal densities, using principles we have already discussed.
    $$
    f_Y(y) = \int_{\R} f_{XY}(x, y) dx = \int_{\R} f_{Y|X}(y|x)f_X(x)dx.
    $$
    \item We can think of the above expression as the law of total probability for the continuous case.
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: finding conditional densities}
  \begin{itemize}
    \item Let $X$ and $Y$ be jointly distributed random variables with joint and marginal densities
    \begin{align*}
      f_{XY}(x, y) &= \lambda^2 e^{-\lambda y}, \quad 0 \leq x \leq y \\
      f_X(x) &= \lambda e^{-\lambda x}, \quad x\geq 0 \\
      f_Y(y) &= \lambda^2 y e^{-\lambda y}, \quad y \geq 0
    \end{align*}
    \item Note that if $x$ is held constant, the joint density decays exponentially in $y$ for $y \geq x$.
    \item If $y$ is held constant, the joint density is constant for $0 \leq x \leq y$. 
    
    \framebreak
    
    \item Find the conditional densities for $Y|X$ and $X|Y$.
  \end{itemize}
  
  \mode<article>{
  $$
  f_{Y|X}(y | x) = \frac{\lambda^2e^{-\lambda y}}{\lambda e^{-\lambda x}} = \lambda e^{-\lambda(y - x)}, \quad y \geq x.
  $$
  This density follows our intuition of what happens when $x$ is fixed, namely that $y$ appeared to decay exponentially. Now we see that $Y|X$ is exponentially distributed on the interval $[x, \infty)$. Now for $X|Y$,
  $$
  f_{X|Y}(x | y) = \frac{\lambda^2 e^{-\lambda y}}{\lambda^2 y e^{-\lambda y}} = 1/y, \quad 0 \leq x \leq y.
  $$
  Thus the conditional density of $X$ given $Y = y$ is uniform on the interval $[0, y]$.
  }
  
  \framebreak
  
  \begin{itemize}
    \item Suppose we wanted to generate samples from the joint distribution $(X, Y)$; how can this be done?
  
    \item  Using what we have found about the conditional distributions, there are two simple ways for this to be done. Recall that the joint density is $f_{XY}(x, y) = f_{X|Y}(x|y)f_Y(y) = f_{Y|X}(y|x)f_X(x)$.
    \begin{enumerate}
      \item We could generate $X$, which is an exponential random variable $(f_X(x))$. Then, we could generate $Y$ conditioned on the simulated value of $X = x$, which follows an exponential distribution on the interval $[x, \infty)$.
      \item Similarly, we can note that $Y$ has a gamma distribution, and therefore generate a $y$ following a gamma distribution, and then generate a value from $X|Y = y$, which is uniform on $[0, y]$.
    \end{enumerate}
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{The rejection method}

\begin{itemize}
  \item We are often interested in generating random variables from a density function.
  \item If we have a closed form of the inverse cdf, we can use the ``inverse cdf method" (Proposition~2.3).
  \item If a closed-form of the inverse cdf is not available, a commonly used approach is known as \alert{rejection sampling}.
  
  \framebreak
  
  \item Setup: let $f$ be a density function we wish to simulate from, that is non-zero on an interval $[a, b]$.
  \item Pick a function $M(X)$ such that $M(x)\geq f(x)$ on $[a, b]$, and let
  $$
  m(x) = \frac{M(x)}{\int_{a}^{b} M(x)dx}.
  $$
  \item Note that $m(x)$, as defined, is a probability density function. Then, to generate RV with density $f$, we can do the following:
  \begin{itemize}
    \item[Step 1:] Generate $T$ with density $m$.
    \item[Step 2:] Generate $U \sim U[0, 1]$ independent of $T$. \alert{If} $U \leq f(T) / M(T)$, then we ``accept" $T$ as a sample $(X = T)$; \alert{otherwise}, we ``reject" and go back to Step 1.
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Rejection Method Figure}

  \begin{itemize}

  \item A geometric justification is randomly throwing a dart (uniformly) at Figure~\ref{fig:reject}.
  
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\textwidth]{reject.png}
    \caption{\label{fig:reject}Illustration of the rejection method, copied from \citet[][Figure~3.15]{rice07}.}
  \end{figure}
  
  \item If the dart lands below the curve $f$, record the $x$ coordinate; otherwise, reject it. With enough throws, the distribution of $x$ coordinates will be proportional to the height of the curve.
  
  \end{itemize}
  
\end{frame}

\begin{frame}{Rejection sampling: proof sketch}
  \begin{itemize}
    \item With a little abuse of notation, we'll do a proof sketch that this works.
    \item Let $I$ be the indicator event such that $I = 1$ if a sample is accepted, $I=0$ otherwise.
    \item We want the distribution of accepted samples to follow $f(x)$, i.e., $f_{T|I=1}(x | I = 1) = f(x)$.
  \mode<article>{
    \item For convenience, let's denote $\bar{M} = \int_a^b M(x) \, dx$, so that $m(x) = M(x) / \bar{M}$. 
    \item First, using conditional probabilities, we have
    $$
    f_X(x) = f_{T | I = 1}(x | I = 1) = \frac{f_{T, I = 1}(x, I = 1)}{P(I = 1)} = \frac{P(I = 1 | T = x)f_T(x)}{P(I = 1)}
    $$
    \item Note in the equation above, I switched from $X$ to $T$; this matches our notation previously used, that we simulate a possible sample $T$, and accept if the particular condition is met, in which case, $X = T$.
    \item $T$ was generated with the standardized function $m$. That is, the density $f_T(x) = m(x)$. Thus, 
    $$
    f_X(x) = f_{T|I=1}(x | I = 1) = \frac{P(I = 1 | T = t)m(x)}{P(I = 1)}.
    $$
    \item We have an expression for $m(x)$ already, now we want to find $P(I = 1)$ and $P(I = 1 | T = x)$.
    \begin{align*}
      P(I = 1) &= \int P(I = 1, T = t) \, dt \\
               &= \int P(I = 1 | T = t) m(t) \, dt.
    \end{align*}
    \item For fixed $T = t$, the probability of accepting the sample (i.e., $I = 1$) is determined by the uniform probability:
    $$
    P(I = 1 | T = t) = P\big(M(t) U \leq f(t)\big) = P\big(U \leq f(t) / M(t) \big) = f(t) / M(t).
    $$
    \item Thus, 
      \begin{align*}
      P(I = 1) &= \int P(I = 1 | T = t) m(t) \, dt \\
               &= \int \frac{m(t) f(t)}{M(t)} \, dt \\
               &= \int \frac{M(t) f(t)}{M(t) \bar{M}}\, dt\\
               &= \frac{1}{\bar{M}} \int f(t) \, dt \\
               &= \frac{1}{\bar{M}}.
      \end{align*}
    \item Therefore, the distribution of the accepted samples is: 
    \begin{align*}
      f_{T|I=1}(x | I = 1) &= \frac{P(I = 1 | T = x)f_T(x)}{P(I = 1)} \\
      &= \bar{M} P(I = 1 | T = x)m(x) \\
      &= \bar{M} \times \frac{f(x)}{M(x)}\times \frac{M(X)}{\bar{M}} \\
      &= f(x).
    \end{align*}
  }
  \end{itemize}
  

  
  % \begin{itemize}
  %   \item A more formal argument using differentials is given in \citet[Example~D][Figure~3.15]{rice07}.
  %   \item In order for the rejection method to be worth-while (computationally efficient), it is important that the algorithm has high-acceptance (good choice of $M$), otherwise you may need a large number of samples because many are being rejected.
  % \end{itemize}
\end{frame}

\section{Functions of Jointly Distributed Random Variables}

\begin{frame}[allowframebreaks]{Convolutions}
  \begin{itemize}
    \item Suppose that $X$ and $Y$ are discrete random variables that take values on the integers and joint pmf $p(x, y)$. 
    \item Find the pmf of $Z = X + Y$. 
    
    \item Note that $Z = z$ only when $X = x$ and $Y = z - x$, whenever $x$ is an integer.
    \item Thus, using the law of total probability, we can write 
      $$
      p_Z(z) = \sum_{x = -\infty}^\infty p(x, z-x).
      $$
    \item If $X$ and $Y$ are independent, then $p(x, y) = p_X(x)P_Y(y)$, and
      $$
      p_Z(z) = \sum_{x = -\infty}^\infty p_X(x)p_Y(z-x).
      $$
    \item This sum is called the \alert{convolution} of the sequences $p_X$ and $p_Y$. 
    
    \item The continuous case is similar. Let $X$ and $Y$ be jointly continuous RVs, and $Z = X + Y$.
    \item If we want to find the cdf of $Z$, then:
    \begin{align*}
      F_Z(z) &= P(Z \leq z) \\
             &= P(X + Y \leq z) \\
             &= \int\int_{\{x + y \leq z\}} f(x, y)\, dy\,dx \\
             &= \int_{-\infty}^\infty \int_{-\infty}^{z-x} f(x, y)\, dy\,dx \\
             &= \int_{-\infty}^\infty \int_{-\infty}^{z} f(x, v-x) \, dv\,dx \\
             &= \int_{-\infty}^z \int_{-\infty}^{\infty} f(x, v-x) \, dx\,dv 
    \end{align*}
    \item Differentiating both sides, the fundamental theorem of calculus (with proper assumptions) gives
    $$
    f_Z(z) = \int_{-\infty}^\infty f(x, z-x) dx
    $$
    
    \item Like in the discrete case, if $X$ and $Y$ are independent, then
    $$
    f_Z(z) = \int_{-\infty}^\infty f_X(x)f_Y(z-x) dx
    $$
    \item This integral is called the \alert{convolution} of the functions $f_X$ and $f_Y$. 
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Example: Sum of Exponential RVs}
    Suppose that the lifetime of an electrical component is exponentially distributed with rate $\lambda$, and that and independent and identical backup is available. If the system operates as long as one of the components is functional, and the components will not be replaced if they fail, what is the distribution of the life of the system?
  \end{exampleblock}

  \emph{Solution:}
  
  \mode<article>{
  \vspace{2mm}
    Let $T_1$ and $T_2$ denote the lifetimes of the two component, respectively. The lifetime of the system is $X = T_1 + T_2$. Thus, the pdf of $X$ can be calculated as the convolution:
    \begin{align*}
    f_X(x) &= \int_{-\infty}^\infty f_X(x)f_Y(z-x)dx \\
    &= \int_{0}^x \lambda e^{-\lambda t} \times \lambda e^{-\lambda (x - t)}dt \\
    &= \lambda^2 \int_{0}^x e^{-\lambda x}dt \\
    &= \lambda^2 x e^{-\lambda x}, \quad x \geq 0.
    \end{align*}
  }

  \framebreak

  \begin{itemize}
    \item In the previous example, note carefully the change in integration:
    \begin{itemize}
      \item The exponential density is only positive when $t > 0$, and zero every where else.
      \item Thus, from $(-\infty, 0)$, the integral is zero.
      \item Similarly, we evaluate the density at $x - t$, and hence when $t > x$, the integral is also zero.
    \end{itemize}
    \item You may notice that the density of $X = T_1 + T_2$ that we calculated is the same as a gamma distribution with parameters $2$ and $\lambda$.
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Quotients of random variables}
  \begin{itemize}
    \item Let $X$ and $Y$ be jointly continuous random variables, and let $Z = Y/X$.
    \item Our derivation for the pdf of $Z$ is similar as what we did with the sum: find the cdf, then take the derivative.
    \item $F_Z(z) = P(Z \leq z) = P(Y/X \leq z)$. Thus, we are interested in the probability of the set $\{x, y: y/x \leq z\}$.
    \item We have to be a little careful about what happens if $X = 0$, so we will split it into two parts: 
    \begin{itemize}
      \item If $x > 0$, then the set is $y \leq xz$.
      \item If $x < 0$, then the set is $y \geq xz$. 
    \end{itemize}
    Thus,
    $$
      F_Z(z) = \int_{-\infty}^0 \int_{xz}^\infty f(x, y)\, dy\, dx + \int_{0}^{\infty} \int_{-\infty}^{xz} f(x, y)\, dy\,dx.
    $$
    \item To remove dependence of the inner integrals on $x$, we make the change of variables $y = xv$:
    \begin{align*}
      F_Z(z) &= \int_{-\infty}^0 \int_{z}^{-\infty} xf(x, xv)\, dv\, dx + \int_{0}^{\infty} \int_{-\infty}^{z} xf(x, xv)\, dv\,dx \\
      &= \int_{-\infty}^0 \int_{-\infty}^{z} (-x)f(x, xv)\, dv\, dx + \int_{0}^{\infty} \int_{-\infty}^{z} xf(x, xv)\, dv\,dx \\
      &= \int_{-\infty}^z \int_{-\infty}^\infty |x| f(x, xv)\, dx\, dv 
    \end{align*}
    \item And differentiating both sides, we obtain
    $$
    f_Z(z) = \int_{-\infty}^\infty |x| f(x, xz)\, dx.
    $$
    \item If $X$ and $Y$ are independent, 
    $$
    f_Z(z) = \int_{-\infty}^{\infty} |x|f_X(x)f_Y(xz) dx.
    $$
  \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Example: Cauchy density}

\begin{exampleblock}{Cauchy density}
  Let $X$ and $Y$ be independent, standard normal random variables. Find the pdf of $Z = X / Y$.
  
  \mode<article>{
    \begin{itemize}
  % \item Let $X$ and $Y$ be independent, standard normal random variables.
  % \item Find the pdf of $Z = Y / X$.
  \item Using the expression we previously derived for the quotient of independent RVs, we have
  $$
  f_Z(z) = \int_{-\infty}^{\infty} \frac{|x|}{2\pi}e^{-x^2/2}e^{-x^2z^2/2} \, dx.
  $$
  \item Because the integrand is symmetric, we can re-express this as
  \begin{align*}
    f_Z(z) &= 2\int_{0}^{\infty} \frac{|x|}{2\pi}e^{-x^2/2}e^{-x^2z^2/2} \,dx \\
     &= \frac{1}{\pi}\int_{0}^\infty x e^{-x^2\big((z^2 + 1)/2\big)} \, dx \\
     &= \frac{1}{2\pi} \int_{0}^\infty e^{-u\big((z^2 + 1) / 2\big)}\, du \\
     &= \frac{1}{2\lambda \pi} \int_{0}^\infty \lambda e^{-u\lambda}\, du \\
     &= \frac{1}{\pi(z^2 + 1)}, \quad -\infty < z < \infty.
  \end{align*}
  \item Here, I made the substitution $\lambda = (z^2 + 1)/2$, and the integral was calculated using the fact that the pdf of the exponential distribution integrates to one: $\int_0^\infty \lambda e^{-\lambda x} dx = 1$.
  \end{itemize}
  }
  
\end{exampleblock}

\framebreak

\begin{itemize}
  \item This density is called the \alert{Cauchy density}.
  \item Like the standard normal, the Cauchy density is symmetric about zero and bell-shaped, but the tails of the Cauchy tend to zero very slowly.
  \item \href{https://www.desmos.com/calculator/tqvayodexb}{Here is a link} showing this comparison.
\end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{The General Case}
  \begin{itemize}
    \item There is also a way to find the pdf of more general cases, though the derivation is outside the scope of this course.
    \item Let $X$ and $Y$ be jointly distributed, continuous RVs, and suppose we are interested in the joint pdf of $U = g_1(X, Y)$, $V = g_2(X, Y)$, where $g_1$ and $g_2$ are invertible functions with continuous partial derivatives.
    \item We will denote the inverse of $g_1$ and $g_2$ as $X = h_1(U, V)$ and $Y = h_2(U, V)$, respectively.
    % \item The \alert{Jacobian} of the transformation $J(x, y)$ is
    % $$
    % J(x, y) = \det \begin{bmatrix}\frac{\partial g_1}{\partial x} & \frac{\partial g_1}{\partial y} \\ \frac{\partial g_2}{\partial x} & \frac{\partial g_2}{\partial y} \end{bmatrix} = \Big(\frac{\partial g_1}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big) - \Big(\frac{\partial g_2}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big).
    % $$
    \item The pdf of $(U, V)$ can be calculated in two ways:
  \end{itemize}
  
  \begin{block}{Proposition 3.1: Multivariate transformations}
    Under the assumptions above, the joint density of $U$ and $V$ is 
    \begin{align*}
    f_{UV}(u, v) &= f_{XY}\big(h_1(u, v), h_2(u, v)\big)\Big|J_h(u, v)\Big| \\
                 &= f_{XY}\big(x, y\big)\Big|J^{-1}_g\big(x, y\big)\Big|
    \end{align*}
    for $(u, v)$ such that $u = g_1(x, y)$ and $v = g_2(x, y)$ for some $(x, y)$, and $0$ otherwise.
  \end{block}
  
  \begin{itemize}
    \item Above, we call $J_f$ the \alert{Jacobian determinant} (or just Jacobian) of $f$. It is equal to the matrix of partial derivatives: 
    \begin{align*}
      J_h &= \det \begin{bmatrix}\frac{\partial h_1}{\partial u} & \frac{\partial h_1}{\partial v} \\ \frac{\partial h_2}{\partial u} & \frac{\partial h_2}{\partial v} \end{bmatrix} = \Big(\frac{\partial h_1}{\partial u}\Big)\Big(\frac{\partial h_2}{\partial v}\Big) - \Big(\frac{\partial h_2}{\partial u}\Big)\Big(\frac{\partial h_2}{\partial v}\Big) \\
      J_g &= \det \begin{bmatrix}\frac{\partial g_1}{\partial x} & \frac{\partial g_1}{\partial y} \\ \frac{\partial g_2}{\partial x} & \frac{\partial g_2}{\partial y} \end{bmatrix} = \Big(\frac{\partial g_1}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big) - \Big(\frac{\partial g_2}{\partial x}\Big)\Big(\frac{\partial g_2}{\partial y}\Big)
    \end{align*}
    \item The reason these two expressions are equal is because $J_h = J^{-1}_g$, and we defined $x = h_1(u, v)$ and $y = h_2(u, v)$; you end up with the same result, but sometimes one might be an easier calculation than the other. Both versions are fairly common in textbooks and courses.
    \item I find the first line of the proposition more intuitive: it's a function of $u$ and $v$, so let's start by calculating the Jacobian of the inverse transformation so that all variables are $u$ and $v$. 
    \item The textbook uses the second line approach. You might find this more intuitive, as it is a generalization of the single dimensional case.
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Polar Coordinates}
  \begin{itemize}
    \item Suppose that $X$ and $Y$ are independent standard normal RVs. Their joint pdf is
    $$
    f_{XY}(x, y) = \frac{1}{2\pi}e^{-(x^2/2) - (y^2/2)}.
    $$
    \item We wish the find the joint pdf of $R = \sqrt{X^2 + Y^2}$, and $\Theta = \text{arctan}(y/x)$.
    \item Thus, we have 
    $$
    \begin{cases}
    g_1(x, y) = \sqrt{x^2 + y^2} = r \\
    g_2(x, y) = \text{arctan}(y/x) = \theta,\, \quad \text{if}\, x \neq 0,\, \text{and}\, \theta = 0 \, \text{o.w.}
    \end{cases}
    $$
    \item The inverse transformations are 
    $$
    \begin{cases}
    h_1(r, \theta) = r\cos \theta = x\\
    h_2(r, \theta) = r \sin \theta = y.
    \end{cases}
    $$
    \item The Jacobian of the inverse transformation $J_h$ is
    \begin{align*}
    J_h(r, \theta) &= \det \begin{bmatrix}\frac{\partial h_1}{\partial r} & \frac{\partial h_1}{\partial \theta} \\ \frac{\partial h_2}{\partial r} & \frac{\partial h_2}{\partial \theta} \end{bmatrix} \\
              &= \det \begin{bmatrix} \cos\theta & -r\sin\theta \\ \sin \theta & r\cos \theta  \end{bmatrix} \\
              &= r \cos^2\theta + r \sin^2\theta = r
    \end{align*}
    \item Therefore, the joint distribution is
    \begin{align*}
      f_{R\Theta}(r, \theta) &= r f_{XY}(r \cos\theta, r \sin \theta) \\
      &= \frac{r}{2\pi}e^{-r^2\cos^2\theta / 2 - r^2\sin^2\theta / 2} \\
      &= \frac{r}{2\pi}e^{-r^2/2}.
    \end{align*}
    \item As always, we can't forget the \alert{support}, or values over which the density is positive. Here, because $(X, Y) \in \R^2$, the transformations imply that $\Theta \in [0, 2\pi]$, and $R \geq 0$. 
  \end{itemize}
\end{frame}

% TODO: Include another example? 
% \begin{frame}{Example: Sum of random variables}
%   \begin{itemize}
%     \item Let $X_1$ and $X_2$ be independent, standard normal variables.
%     \item What is the joint density of $(X_1, X_1 + X_2)$?
%     \item Let's let $U = X_1$, and $V = X_1 + X_2$.
%     \item Our transformations are:
%     
%   \end{itemize}
% \end{frame}

\begin{frame}{Transformations of many variables}
\begin{itemize}
  \item Proposition~3.1 can be generalized to transformations of more than two random variables. If $X_1, \ldots, X_n$ have the joint density function $f_{X_1\cdots X_n}$, and 
  \begin{align*}
  Y_i &= g_i(X_1, \ldots, X_n), \quad i = 1, \ldots, n\\
  X_i &= h_i(Y_1, \ldots, Y_n), \quad i = 1, \ldots, n
  \end{align*}
  And if $J_g$ is the determinant of the matrix with the $ij$th entry $\partial g_i/\partial x_j$, and $J_h$ is the determinant of the matrix with entry $\partial h_i / \partial y_j$, then the joint density of $Y_1, \ldots, Y_n$ is
  \begin{align*}
    &f_{Y_1\cdots Y_n}(y_1, \ldots, y_n) \\
    &= f_{X_1\cdots X_n}(x_1, \ldots, x_n)\big|J^{-1}_g(x_1, \ldots, x_n)\big|\\
    &= f_{X_1\cdots X_n}\big(h_1(y_1, \ldots, y_n), \ldots, h_n(y_1, \ldots, y_n)\big)\big|J_h(y_1, \ldots, y_n)\big|
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Final Comments}
  \begin{itemize}
    \item In the transformation formulas, we always transform $n$ variables to $n$ variables. In practice, you might want to consider a transformation from $n \mapsto m$, with $m \leq n$. In this case, there are two main approaches:
    \begin{itemize}
      \item Start from scratch, just like we did for sums and quotients of random variables.
      \item Create dummy variables to make $m = n$ (i.e., $Y_k = X_k$), calculate Jacobian, and then integrate out the dummy variables.
    \end{itemize}
  \end{itemize}
\end{frame}

\section{Order Statistics}

\begin{frame}[allowframebreaks]{Extrema and Order Statistics}

\begin{itemize}
  \item At times we are interested in properties of ordered collections of random variables.
  \item For instance, if we have $n$ independent and identical random variables, what's the distribution of the \alert{maximum} or \alert{minimum} of this collection of random variables? 
  \item More generally, we might want to find the distribution of the $k$th random variable, with $k \in \{1, 2, \ldots, n\}$.
\end{itemize}
  \framebreak
  
  \begin{exampleblock}{Example: Maximum}
    Let $X_1, X_2, \ldots, X_n$ be independent and idenditcally distributed random variables, with distribution function $F(x)$ and density $f(x)$. Let $X_{(1)}, X_{(2)}, \ldots, X_{(n)}$ be the ordered random variables, such that $X_{(1)}$ is the minimum and $X_{(n)}$ is the maximum.
    Find the density function and pdf of $X_{(n)}$.
    
    \mode<article>{
    \begin{itemize}
      \item To find the distribution function, we want to find $P(X_{(n)} \leq x)$.
      \item If $X_{(n)}$ is the maximum of all $X_i$, then that means $X_{(n)} \leq x$ if and only if $X_{i} \leq x$ for all $i$. By the multiplication principle,
      \begin{align*}
      P(X_{(n)} \leq x) &= P(X_1 \leq x)P(X_2 \leq x)\cdots P(X_n \leq x) \\
                        &= F(x)F(x)\cdots F(x) \\
                        &= F(x)^n = F_{X_{(n)}}(x)
      \end{align*}
      \item To find the density, we differentiate:
      $$
      f_{X_{(n)}}(x) = nF(x)^{n-1}f(x)
      $$
    \end{itemize}
    }
    
  \end{exampleblock}
  
  \framebreak
  
  \begin{exampleblock}{Example: Minimum}
    Using the same setup as above, find the distribution and density functions for the minimum, $X_{(1)}$.
    
    \mode<article>{
    \begin{itemize}
      \item As before, we are looking for the function $F_{X_{(1)}}(x) = P(X_{(1)} \leq x)$.
      \item In this case, it's easier to find $P(X_{(1)} > x)$.
      \item The minimum is bigger than $x$ if and only if all $X_i > x$. Thus
      $$
      1 - F_{X_{(1)}}(x) = P(X_{(1)} > x) = \big(1 - F(x)\big)^n.
      $$
      \item Therefore, by taking the derivative, we find
      \begin{align*}
        F_{X_{(1)}}(x) &= 1 - \big(1 - F(x)\big)^n \\
        f_{X_{(1)}}(x) &= n f(x) \big(1 - F(x))^{n-1}
      \end{align*}
    \end{itemize}
    }
    
  \end{exampleblock}
  
  \framebreak
  
  \begin{block}{Theorem~\CHAPTER.1: Order statistic for continuous random variables}
    Let $X_{1}, \ldots, X_{n}$ be continuous random variables, with cdf and pdf $F$ and $f$, respectively. 
    If the ordered random variables are $X_{(1)}, \ldots, X_{(n)}$, then the cdf of $X_{(j)}$, $j \in \{1, 2, \ldots, n\}$ is
    $$
    F_{X_{(j)}}(x) = \sum_{k = j}^n \binom{n}{k} \big[F(x)\big]^k\big[1-F(x)\big]^{n-k}.
    $$
    
    \mode<article>{
    \begin{proof}
    \begin{itemize}
      \item Let $Y$ be a random variable that counts the number of $X_i$ less than or equal to $x$.
      \item If we define the ``success" as the event that $X_{i}\leq x$, then the probability of success is $p = F(x)$.
      \item Thus, $Y$ is a binomial random variable with $n$ trials, and probability $p = F(x)$.
      \item For $X_{(j)} \leq x$, that means that $j$ of the $X_i$ are less than or equal to $x$.
      \item Using our definition of $Y$, $X_{(j)} \leq x$ if and only if $Y \geq j$. Thus
      \begin{align*}
        F_{X_{(j)}}(x) &= P(Y \geq j) \\
                       &= \sum_{k = j}^n \binom{n}{k}\big[F(x)\big]^k\big[1-F(x)\big]^{n-k}.
      \end{align*}
    \end{itemize}
    \end{proof}
    }
    
  \end{block}
  
  \framebreak
  
  \begin{block}{Theorem~\CHAPTER.2: The pdf of ordered statistics}
    Consider the same setup as Theorem~\CHAPTER.1. That is, $X_{(1)}, \ldots, X_{(n)}$ represent the order statistics of a random sample. Then the pdf of $X_{(j)}$ is
    $$
    f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!}f(x)\big[F(x)\big]^{j-1}\big[1 - F(x)\big]^{n-j}.
    $$
    \mode<article>{
    \begin{proof}
    First, I want to note the following identity that will use
    $$
    \binom{n}{k+1}(k+1) = \frac{n!}{(n-k-1)!(k+1)!}(k+1) = \frac{n!}{(n-k-1)!k!} = \frac{n!}{(n-k)!k!}(n-k) = \binom{n}{k}(n-k)
    $$
    \begin{itemize}
      \item Because we already have the cdf, we just need to differentiate to get the pdf.
    \end{itemize}
    \begin{align*}
      f_{X_{(j)}}(x) &= \frac{d}{dx} F_{X_{(j)}}(x) \\
      &= \frac{d}{dx} \sum_{k = j}^n \binom{n}{k}\big[F(x)\big]^k\big[1-F(x)\big]^{n-k} \\
      &= \sum_{k=j}^n \binom{n}{k}\left[kf(x)\big(F(x)\big)^{k-1}\big(1 - F(x)\big)^{n-k} - (n-k)f(x)\big(F(x)\big)^k\big(1-F(x)\big)^{n-k-1}\right]\\
      &= \sum_{k=j}^n \binom{n}{k}\left[kf(x)\big(F(x)\big)^{k-1}\big(1 - F(x)\big)^{n-k}\right] - \sum_{k = j}^n \binom{n}{k}\left[(n-k)f(x)\big(F(x)\big)^k\big(1-F(x)\big)^{n-k-1}\right]
    \end{align*}
    \begin{itemize}
      \item There are many like terms in these sums, which becomes most evident with the identity we derived.
      \item In the first sum, let's ``pull-out" the first term.
      \item In the second sum, the last term is zero because when $k = n$, we have a product $(n -k)$.
    \end{itemize}
    \begin{align*}
      f_{X_{(j)}}(x) &= \binom{n}{j} j \big(F(x)\big)^{j-1}\big(1 - F(x)\big)^{n-j}f(x) \\
                     & \quad + \sum_{k = j+1}^n \binom{n}{k}\left[kf(x)\big(F(x)\big)^{k-1}\big(1 - F(x)\big)^{n-k}\right] \\
                     & \quad - \sum_{k = j}^{n-1} \binom{n}{k}\left[(n-k)f(x)\big(F(x)\big)^k\big(1-F(x)\big)^{n-k-1}\right].
    \end{align*}
    \begin{itemize}
      \item Now I will expand the factorial in the first term, and do a change of variables for the first summation, shifting the indices down one so that the two sums match.
    \end{itemize}
    \begin{align*}
      f_{X_{(j)}}(x) &= \frac{n!}{(j-1)!(n-j)!} f(x) \big(F(x)\big)^{j-1}\big(1 - F(x)\big)^{n-j} \\
                     & \quad + \sum_{k = j}^{n-1} \binom{n}{k+1}\left[(k+1)f(x)\big(F(x)\big)^{k}\big(1 - F(x)\big)^{n-k-1}\right] \\
                     & \quad - \sum_{k = j}^{n-1} \binom{n}{k}\left[(n-k)f(x)\big(F(x)\big)^k\big(1-F(x)\big)^{n-k-1}\right].
    \end{align*}
    \begin{itemize}
      \item Finally, using the identity that we previously derived,
      $$
      \binom{n}{k+1}(k+1) = \binom{n}{k}(n - k),
      $$
      we see that the two sums are additive inverses of one another and cancel out, thus
    \end{itemize}
    $$
    f_{X_{(j)}}(x) = \frac{n!}{(j-1)!(n-j)!}f(x)\big[F(x)\big]^{j-1}\big[1 - F(x)\big]^{n-j}.
    $$
    \end{proof}
    }
  \end{block}
  
  \framebreak
  
  \begin{exampleblock}{Example: Uniform order statistic pdf}
    Let $X_1, \ldots, X_n$ be independent Uniform$(0, 1)$ random variables, so that $f(x) = 1[0 < x < 1]$, and $F_X(x) = x$. Find the pdf of the $j$th order statistic.
    \mode<article>{
    \begin{itemize}
      \item We can just use the formula we previously derived for order statistics:
      \begin{align*}
        f_{X_{(j)}}(x) &= \frac{n!}{(j-1)!(n-j)!}(x)^{j-1}(1 - x)^{n - j}, \quad x \in (0, 1) \\
        &= \frac{\Gamma(n + 1)}{\Gamma(j)\Gamma(n - j + 1)}x^{j - 1}(1 - x)^{(n - j + 1) - 1}, \quad x \in (0, 1),
      \end{align*}
      \item where the last equation comes from the definition of the gamma-function
      \item This final expression is the pdf of a beta$(j, n-j+1)$ random variable, which could be used to calculate the mean or variance.
    \end{itemize}
    }
  \end{exampleblock}
  
  \framebreak
  
  \begin{itemize}
    \item The joint distribution of two or more order statistics can also be derived, and is useful for deriving the density of some statistics of interest.
  \end{itemize}
  
  \begin{block}{Theorem~\CHAPTER.3: joint pdf of order statistics}
    Consider the same setup as Theorem~\CHAPTER.1. That is, $X_{(1)}, \ldots, X_{(n)}$ represent the order statistics of a random sample. Then the joint pdf of $X_{(i)}, X_{(j)}$, $1 \leq i < j \leq n$ is
    \begin{align*}
      f_{X_{(i)}, X_{(j)}}(u, v) &= \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f(u)f(v)\big(F(u)\big)^{i-1}\\
      &\quad \times \big(F(v) - F(u)\big)^{j - 1 - i}\big(1 - F(v)\big)^{n - j},
    \end{align*}
    for $-\infty < u < v < \infty$.
  \end{block}
  
  \framebreak
  
  \begin{exampleblock}{Example: Midrange and Range}
    Let $X_1, \ldots, X_n$ be independent uniform$(0, a)$ random variables, and denote the $i$th order statistic as $X_{(i)}$. The \alert{range} of a sample is defined as the difference between the largest and smallest observations, $R = X_{(n)} - X_{(1)}$. Define the \alert{midrange} as the midpoint between the minimum and maximum observations, $V = (X_{(1)} + X_{(n)}) / 2$. Find the pdf of both $R$ and $V$.
    
    \mode<article>{
    \begin{itemize}
      \item We first can apply Theorem~\CHAPTER.3 to get the joint density of $X_{(1)}$ and $X_{(n)}$:
      \begin{align*}
        f_{X_{(i)}, X_{(j)}}(x_1, x_n) &= \frac{n!}{(i-1)!(j-1-i)!(n-j)!}f(x_1)f(x_n)\big(F(x_1)\big)^{i-1}\\
      &\quad \times \big(F(x_n) - F(x_1)\big)^{j - 1 - i}\big(1 - F(x_n)\big)^{n - j} \\
      &= \frac{n!}{(1-1)!(n-1-1)!(n-n)!}f(x_1)f(x_n)\big(F(x_1)\big)^{1-1}\\
      &\quad \times \big(F(x_n) - F(x_1)\big)^{n - 1 - 1}\big(1 - F(x_n)\big)^{n - n} \\
      &= \frac{n(n-1)}{a^2}\Big(\frac{x_n}{a} - \frac{x_1}{a}\Big)^{n-2} \\
      &= \frac{n(n-1)(x_n - x_1)^{n-2}}{a^n}, \quad 0 < x_1 < x_n < a.
      \end{align*}
      \item Now we can use the change of variables theorem. We find that $g_1(x_1, x_n) = x_n - x_1$, and $g_2(x_1, x_n) = (x_1 + x_n) / 2$.
      \item Solving for $R$ and $V$ as functions of $X_{(1)}$ and $X_{(n)}$, we find the inverse transformations
      $$
      h_1(r, v) = v - r/2, \quad h_2(r, v) = v + r/2.
      $$
      \item We can now calculate the Jacobian of the transformation:
      $$
      J_h = \begin{vmatrix}
      \frac{\partial h_1}{\partial r} & \frac{\partial h_1}{\partial v} \\
      \frac{\partial h_2}{\partial r} & \frac{\partial h_2}{\partial r}
      \end{vmatrix} = -\frac{1}{2} - \frac{1}{2} = -1.
      $$
      \item We need to calculate the support of $(R, V)$. The transformation $g(x_1, x_n)$ maps the set $\{(x_1, x_n):\, 0 < x_1 < x_n < a\}$ onto the set $\{(r, v):\, 0 < r < a, r/2 < v < a-r/2\}$. To see this, note that $0 < x_n - x_1 < a$, and the lower and upper bounds for $v$ can be found by plugging in $x_1 = h_1(r, v)$ and $x_n = h_2(r, v)$ into the upper and lower inequalities of $0 < x_1 < x_n < a$, respectively.
      \item Thus, we find the joint density of $(R, V)$ as
      \begin{align*}
      f_{R, V}(r, v) &= \frac{n(n-1)\big(h_2(r,v) - h_1(r, v)\big)^{n-2}}{a^n}, \quad 0 < r < a, \,\, r/2 < v < a-r/2\\
      &= \frac{n(n-1)r^{n-2}}{a^n}, \quad 0 < r < a, \,\, r/2 < v < a-r/2.
      \end{align*}
      \item Finally, we can integrate to find the marginal densities:
      \begin{align*}
        f_R(r) &= \int_{\R} f_{(R, V)}(r, v) \,dv \\
        &= \int_{r/2}^{a - r/2} \frac{n(n-1)r^{n-2}}{a^n}\, dv \\
        &= \frac{n(n-1)r^{n-2}(a - r)}{a^n}, \quad 0 < r < a.
      \end{align*}
      \item If $a = 1$, the density of $R$ corresponds to that of a beta$(n-1, 2)$ distribution; alternatively, we can do a quick change of variable to see that $R/a$ has a beta distribution, where $a$ just scales the distribution.
      \item The bounds on the integral for integrating out $r$ is a little bit less straightforward because $r$ appears in both inequalities. In this case, it's helpful to plot the region \url{https://www.desmos.com/calculator/tosungnfnu}.
      \item We see we have a triangle that has a base on the v-axis, with height $a$. The triangle is defined by the equations $r = 2v$ and $r = 2(a - v)$, so the integral is calculated over two parts:
      \begin{align*}
        f_V(v) &= \int_{\R} f_{(R, V)}(r, v) \,dr \\
               &= 1[0 < v \leq a/2]\int_{0}^{2v} \frac{n(n-1)r^{n-2}}{a^n}\, dr + 1[a/2 < v \leq a]\int_{0}^{2(a-b)} \frac{n(n-1)r^{n-2}}{a^n}\, dr \\
               &= \begin{cases} \frac{n(2v)^{n-1}}{a^n}, & 0 < v \leq a/2 \\ \frac{n\big(2(a-v)\big)^{n-1}}{a^n}, & a / 2 < v \leq a \end{cases}
      \end{align*}
    \end{itemize}
    }
    
    %TODO: Finish this example
  \end{exampleblock}
  
  \framebreak
  
  \begin{itemize}
    \item The joint pdf of more than two order statistics can also be found.
    \item Perhaps the most useful is the joint pdf of all of the order statistics:
    $$
    f_{X_{(1)}, \ldots, X_{(n)}}(x_1, \ldots, x_n) = \begin{cases} n!f(x_1)\cdots f(x_n) & -\infty < x_1 < \ldots < x_n < \infty \\ 0 & \text{otherwise}\end{cases}
    $$
    \item The $n!$ arises naturally because it is the number of ways to arrange $n$ items.
    \item Conditional and marginal densities of order statistics can be found using formulas for conditional and marginal densities, as needed.
  \end{itemize}
  
  \framebreak
  
  \begin{itemize}
    \item There are a few more useful theorems / identities that are useful, and there derivation is similar.
    \item I'll just present these theorems without proof for now, but proofs can be found in \citet[][Chapter~5.4]{casella24}.
  \end{itemize}
  
  \framebreak
  
  \begin{block}{Theorem~\CHAPTER.4: Discrete random variables}
    Let $X_1, \ldots, X_n$ be a sample from a discrete distribution with pmf $p(x_i) = p_i$. If we define $P_0 = 0$, and for all $i \geq 1$ $P_i = \sum_{j = 1}^i p_j$, then the cdf and pmf of the order statistic $X_{(j)}$ are given by
    $$
    P(X_{(j)} \leq x_i) = \sum_{k = j}^n \binom{n}{k}P_i^k (1- P_i)^{n-k},
    $$
    and
    $$
    P(X_{(j)} = x) = \sum_{k = j}^{n}\binom{n}{k}\big[P_i^k (1 - P_i)^{n-k} - P_{i - 1}^k(1 - P_{i-1})^{n-k}\big].
    $$
  \end{block}
  
  \framebreak
  
  \begin{block}{Theorem~\CHAPTER.5: Joint distributions}
    If $X_{(1)}, \ldots, X_{(n)}$ denote the order statistics of a random sample from a continuous population with cdf $F(x)$ and pdf $f(x)$, then for any $1 \leq i < j \leq n$, the joint pdf of $X_{(i)}$ and $X_{(j)}$ is 
    \begin{align*}
    f_{X_{(i)}, X_{(j)}}(u, v) &= \frac{n!}{(i - 1)!(j-1-i)!(n-j)!}f(u)f(v) \\
    &\quad \times \big[F(u)\big]^{i-1}\big[F(v)-F(u)\big]^{j-1-i}\big[1-F(v)\big]^{n-j}.
    \end{align*}
  \end{block}
  
\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version \Sexpr{getRversion()}.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.9]{References and Acknowledgements}

\acknowledgments

\framebreak

\bibliography{../bib4450}

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







