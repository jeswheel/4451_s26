\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{2}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4451_f25/}{Mathematical Statistics II}\\ \vspace{2mm}
Maximum Likelihood Estimation}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]

<<setup,include=FALSE,cache=FALSE,purl=FALSE,child="../setup.Rnw">>=
@

\begin{document}

\maketitle

\mode<article>{\tableofcontents}

% \mode<presentation>{
%   \begin{frame}{Outline}
%     \tableofcontents
%   \end{frame}
% }

\section{Introduction}

\begin{frame}[allowframebreaks]{Overview}
  \begin{itemize}
    \item The next approach we will discuss is Maximum Likelihood Estimation (MLE).
    \item As we will see, the MLE has several desirable properties, and as a result is often favored over approaches like the method of moments.
    \item The material for this section largely comes from Chapter~8.5 of \citet{rice07}, and various sections in \citet{pawitan01}.
    \end{itemize}
\end{frame}

\section{Likelihood: an introduciton}

\begin{frame}[allowframebreaks,fragile]{What is likelihood?}

\begin{itemize}
  \item The term ``likelihood" is often used colloquially to mean something analogous to probability. E.g., ``What is the likelihood that it rains tomorrow?"
  \item When we use this term in statistics / mathematics, we mean something specific that isn't the same thing as probability. 
  \item The use of the term ``likelihood" was first made by R. A. Fisher, who was the architect and primary proponent of ``likelihood-based-inference".
  \item We will start with the treatment of likelihood in the text ``In all Likelihood" \citep{pawitan01}, which is a fantastic resource on the subject. (This will lead to some review...)
\end{itemize}

\framebreak

\begin{exampleblock}{Coin Flips}
  We will revisit this example, as it is a great starting point to connect with existing understanding.
  
  Consider flipping a coin $N = 10$ times.
  
  \mode<article>{
  \begin{itemize}
    \item In a probability class, we might assume the probability of heads $\theta = p$ is some number, say $p = 0.4$.
    \item If $X$ is the number of heads, then $P_\theta(X = 8) = 0.011$ is something we can calculate exactly. In any interpretation of probability, this means that if we repeat the experiment 10,000 time, we expect around 110 times, the experiment will have 8 heads.
    \item Now what if, in a single experiment, we observe $X = 8$? \textbf{Based on this information alone}, what can we say about the parameter $p$? Information we have about $p$ is \emph{incomplete}(we've already done a pure frequentist interpretation of this, as well as a Bayesian interpretation. The MoM estimate is also easy to compute).
    \item However, we can conclude that $p$ \textbf{cannot be zero or one}. This fact is available \emph{deductively}, since if $p$ were zero or one, then $X = 0$ or $X = 10$.
    \item Similarly, we are fairly certain that $p$ is not close to zero, since observing $X = 10$ would be extremely unlikely in this case. For instance, the probability of observing $X=8$ if $p = 0.10$ is only \Sexpr{dbinom(8, 10, 0.1) |> round(8)}, meaning we would only expect about 1 out of 3 million replicates to give $X=8$ (or similar for observing $X \in \{8, 9, 10\}$).
    \item In contrast, $\theta = 0.6$ or $\theta = 0.7$ are \emph{likely}, since $P_\theta(X = 8)$ would be \Sexpr{sum(dbinom(8, 10, 0.6)) |> round(2)}, \Sexpr{sum(dbinom(8, 10, 0.7)) |> round(2)}, respectively.
    \item Thus, we have found a \emph{deductive} way of comparing different values of $p$, based on the observed data: Compare the probability of observed data under different values of $p$.
    \item Concretely, we take the probability model $P_\theta(X = 8)$, and treat it as a function of $\theta$. 
    \item Note this is opposite of what we did last semester, where we treat the pmf for a fixed $\theta$ as a function of $x$ (the possible output).
    \item This is how we define the likelihood function (for this model, and more generally, for any pmf):
    $$
    L(\theta) = P_\theta(X = 8) = f(x^*; \theta).
    $$
  \end{itemize}
  }
\end{exampleblock}

\framebreak

\begin{itemize}
  \item For our specific coin-flipping example with $N=10$, $X = 8$, the likelihood function is
  $$
  L(\theta) = P_\theta(X = 8).
  $$
  \item This is plotted in the following way:
\end{itemize}

<<>>=
x <- seq(1e-8, 1-1e-8, length.out = 1000)
y <- dbinom(8, 10, x) 
plot(x = x, y = y, type = 'l')
@
  
\begin{itemize}
  \item From the figure, we see that $p$ is unlikely to be less than $0.5$,or greater than $0.95$.
  \item Given the data alone (no prior), we should prefer a value somewhere in the middle of these values.
  \item We still have some uncertainty about the value of $p$, but the likelihood gives us a numerical way to compare values of $\theta$. Stochastic uncertainty as a result of sampling is captured in the likelihood function $L(\theta)$.
  \item \alert{The likelihood is not a probability}. Though it came from a probability, the likelihood function (a function of $\theta$) does not satisfy the requirements to be a probability. In our previous example, we have:
  $$
  \int_0^1 L(\theta)\, d\theta = 1/11 \neq 1.
  $$
  \item For discrete probability, the likelihood was \alert{continuous}. Discrete likelihoods are possible, arising when we want to select from a list $\{\theta_1, \theta_2, \ldots\}$. 
  
  \framebreak
  
  \item The idea behind maximum likelihood estimation (MLE) is simple: our estimate is the value of $\theta$ that maximizes the likelihood function $L(\theta)$.
  \item The MLE is considered a \emph{frequentist} approach. Why? It quantifies a maximum belief about a parameter, which is more Bayesian in nature than Frequentist. 
  \item As we'll see later, the MLE has nice theoretical Frequentist \emph{properties}, and as a result can be justified via the frequentist paradigm.
  \item Still, it has close connection to Bayesian estimation and interpretation. In fact, we'll discuss connections between the MLE and Bayesian statistics later.

  \framebreak 
  
  \item Often, maximizing the likelihood directly is challenging, so we maximize the log-likelihood instead.
  \item Other times, the likelihood has to be maximized numerically.

\end{itemize}

\begin{exampleblock}{MLE of coin toss problem}
  Suppose we have $N = 10$ total tosses, and $n$ total heads. Find the MLE of $p$, the probability of heads.
  
  \mode<article>{
    \begin{itemize}
      \item First, describing what the likelihood function is. We already did this for this model, but a quick review, reinforcing common notation.
      \item We'll use a binomial$(N,p)$ model; the parameter of interest is $\theta = p$. 
      \item Let $X$ be a random variable, denoting the number of heads. Thus, pmf of the model is:
      $$
      P(X = x) = f(x;\, \theta) = \binom{N}{x}p^{x}(1-p)^{N-x}.
      $$
      \item We assume we observe a specific dataset $x^*$. In this case, we observe $x^* = n$. Fixing the model at the observed data, we can describe the probability of the observed data as a function of $\theta = p$:
      $$
      L(\theta) = f(x^*; \, \theta) = \binom{N}{n}p^{n}(1-p)^{N-n},
      $$
      and the MLE is:
      $$
      \hat{\theta} = \argmax_\theta L(\theta),
      $$
      meaning that our estimate $\hat{\theta}$ is the \emph{argument} that \emph{maximizes} the likelihood function $L(\theta)$.
      \item As often is the case, this will be easiest if we take the natural logarithm, giving the log-likelihood. Note this is a monotonic transformation, so the argument that maximizes the log-likelihood is the same as the argument that maximizes the likelihood.
      \item Typically, we denote the log-likelihood using $\ell(\theta)$. In latex, this symbol is given by \texttt{\textbackslash ell}.
      $$
      \hat{\theta} = \argmax_\theta L(\theta) = \argmax_\theta \log\big(L(\theta)\big) = \argmax_\theta \ell(\theta).
      $$
      \item For our specific model, the log-likelihood is:
      $$
      \ell(\theta) = \log \binom{N}{n}p^{n}(1-p)^{N-n} = \log\bigg(\binom{N}{n}\bigg) + n\log(p) + (N-n)\log(1-p).
      $$
      \item Now we see another recurring theme when finding the MLE: we end up with terms that don't impact the maximization. That is, the binomal term out front does not change the maximum value of $p$, so it can be ignored.
      \item To maximize, we take the derivative, and set equal to zero:
      $$
      \frac{\partial \ell}{\partial \theta} = \frac{n}{p} - \frac{N-n}{1-p} = 0,
      $$
      or
      $$
      \frac{n}{p} = \frac{N-n}{1-p} \implies n-np = Np - np.
      $$
      \item Solving for $p$, we get the MLE:
      $$
      p = \frac{n}{N} \implies \hat{p} = \frac{n}{N} = \argmax_\theta L(\theta).
      $$
      \item A few notes:
      \begin{itemize}
        \item The MLE matches the frequentist-derived estimate. In most models, a first-principle estimate using frequentist interpretation of probability is not available, but the MLE will still match what the estimate \emph{would be} if such an estimate were available.
        \item Setting a derivative equal to zero and solving only gives a critical point. Why did I assume it was a maximum? First, we plotted like likelihood function, and it is clear any critical point will be a maximum. More importantly, many of the classic probability models are part of what is known as the \emph{exponential family}. In this family, all critical points of $L(\theta)$ will correspond to maximums. Next, theory we will establish later shows that as the number of observations increases, the likelihood function will be concave down around the MLE, suggesting that for even complex models, critical points in the region of high-likelihood will always be maximums.
      \end{itemize}
    \end{itemize}
  }
  
\end{exampleblock}

\end{frame}

\begin{frame}[allowframebreaks]{Continuous models}

\begin{itemize}
  \item The interpretation of the likelihood function as the ``the probability of the observed data $x^*$, considered as a function of $\theta$" makes perfect sense in the discrete model case.
  \item For continuous models, the technical issue arises that the probability of any point value $x$ is zero.
  \item We resolve the problem similar to what was done in Math 4450 and the John Rice text: approximate the probability by discretizing into small, discrete intervals:
  $$
  x^* \in (x^* - \epsilon/2, x^* + \epsilon/2),
  $$
  thus, the probability of observing something $\epsilon$-close to the data is:
  \begin{align*}
  L(\theta) &= P_\theta\big(X \in (x^* - \epsilon/2, x^* + \epsilon/2)\big) \\
  &= \int_{x^* - \epsilon/2}^{x^* + \epsilon/2} f(x; \theta)\, d\theta \approx \epsilon f(x^*;\theta).
  \end{align*}
  \item Then, since the likelihood is only meaningful up to a constant (we will discuss likelihood ratios later), then this has the same behavior as $L(\theta) = f(x^*;\theta)$.
  \item There are more advanced approaches to this problem, but this simple argument justifies the use of the pdf of a continuous random variable as the likelihood $L(\theta)$.
  \item \textbf{Going forward:} we once again will generalize a model $f(x;\theta)$ to mean either the pmf or pdf of a random variable. I will often say ``density" as a blanket term, even if this corresponds to a pmf, not a density.
  \item Further, when we ``integrate" a density, this means either:
  $$
  \int f(x;\theta)\,dx, \quad \text{If continuous}
  $$
  or
  $$
  \sum_x f(x;\theta),\quad \text{If discrete.}
  $$
\end{itemize}

\end{frame}

% \begin{frame}[allowframebreaks,fragile]{Example: partial information}
%   \begin{itemize}
%     \item Using likelihood as a mechanism for parameter estimation can be useful, even in scenarios we only have partial information.
%   \end{itemize}
%   
%   \begin{exampleblock}{Extending the binomial example: Seed germination}
%     Suppose $100$ seeds were planted and it is only known that $x\leq 10$ germinated, the exact number unknown. 
%     Estimate the probability of seed germination.
%     
%     \mode<article>{
%     \begin{itemize}
%       \item This is really an extension of the coin-flipping example, and one reason why the binomial model is important to study: in many cases, we are interested in studying independent trials of ``success" vs ``failure".
%       \item As before, let $X$ denote the random variable of the number of seed germinations. Assuming independence, a natural model is:
%       $$
%       X \sim \text{Binom}(100, p), 
%       $$
%       since there were 100 seeds to start.
%       \item The likelihood is determined by the probability of what is known, as a function of $\theta = p$:
%       $$
%       L(p) = P_{p}(X \leq 10) = \sum_{x = 0}^{10} \binom{100}{x} p^{x}(1-p)^{100-x}
%       $$
%     \end{itemize}
%     }
%     
%   \end{exampleblock}
%   
% \end{frame}

\section{Joint Probabilities}

\begin{frame}[allowframebreaks]{Likelihood with multiple observations}

\begin{itemize}
  \item Often the data we observe is multi-dimensional, rather than summarized as a single observation.
  
  \item In this case, the likelihood $\theta$ is still determined via the joint model: 
  $$
  L(\theta) = f(x^*;\theta) = f_{X_{1:N}}(x_1^*, x_2^*, \ldots, x_N^*;\theta).
  $$
  \item We are mostly focused in this class in the case were the observations are independent, meaning the likelihood factors:
  $$
  L(\theta) = \prod_{i = 1}^N f_{X_i}(x_i^*;\theta).
  $$
  
  \item We often further simplify this by assuming the data are identically distributed:
  $$
  L(\theta) = \prod_{i = 1}^N f_{X_1}(x_i^*;\theta).
  $$
  
  \item As we've seen, it's generally easier to maximize the log-likelihood. In the IID case:
  $$
  \ell(\theta) = \log \prod_{i = 1}^N f_{X_1}(x_i^*;\theta) = \sum_{i = 1}^n \log f_{X_1}(x_i^*;\theta).
  $$
\end{itemize}

\end{frame}

\section{Examples}

\begin{frame}[allowframebreaks]{Examples of finding the MLE}

\begin{exampleblock}{Traffic data: Poisson Model}
  Returning to a motivating example, suppose we model traffic accidents in a given week as $X_1, X_2, \ldots, X_N$, where the data are iid Poisson$(\lambda)$. Obtain the MLE for $\lambda$.
  
  \mode<article>{
    \begin{itemize}
      \item If $X_i$ are iid poisson, then the pmf of a single observation is:
      $$
      f(x;\, \theta) = P_\theta(X_i = x) = \frac{\lambda^xe^{-\lambda}}{x!}.
      $$
      \item In this case, the parameter set of interest, $\theta$ is a single value: $\theta = \lambda$.
      \item Due to the IID assumption, the likelihood of each observation is given by: 
      $$
      L(\theta) = f_{X_{1:N}}(x^*_{1:N}; \, \theta) = \prod_{i = 1}^N \frac{\lambda^{x^*_i}e^{-\lambda}}{x^*_i!}.
      $$
      \item We want to maximize this function with respect to $\lambda$; this is easier done after taking the log:
      \begin{align*}
      \ell(\theta) &= \log f_{X_{1:N}}(x^*_{1:N}; \, \theta) \\
                   &= \sum_{i = 1}^N \log f_{X_1}(x^*_i; \, \theta) \\
                   &= \sum_{i = 1}^N \big(x^*_i \log \lambda - \lambda - \log (x^*_i!)\big)\\
                   &= \log \lambda \sum_{i = 1}^N x^*_i - N\lambda - \sum_{i = 1}^N \log (x^*_i!) \\
                   &= \log \lambda \sum_{i = 1}^N x^*_i - N\lambda - c(x^*).
      \end{align*}
      \item In the last step, we replaced the sum of log-factorials with some constant function $c(x^*)$. As a function of $\theta$, the maximum of $\ell(\theta)$ does not change with this constant, so it can be ignored for the purposes of maximization.
      \item Now to find the maximum, we can take the derivative and set equal to zero:
      $$
      \ell'(\theta) = \frac{1}{\lambda}\sum_{i = 1}^N x^*_i - N = 0,
      $$
      Thus,
      $$
      \hat{\lambda} = \frac{1}{N}\sum_{i = 1}^N x^*_i = \bar{x}^*_N.
      $$
      \item We formally need to check if this is indeed a maximum and not just a critical point. The second derivative with respect to $\lambda$ is always negative (since $x_i^*$ is non-negative), implying that it is indeed a maximum.
      \item We'll see later that as $N \rightarrow \infty$, then the likelihood function is \emph{always} concave-down around the maximum, under fairly week conditions.
      \item We could also plot the likelihood / log-likelihood, which is informative on it's own right.
      \item Finally, note that this is the same estimator that was obtained via the MoM approach. This sometimes happens.
    \end{itemize}
  }
  
\end{exampleblock}

\framebreak

\begin{exampleblock}{Two parameter model: Gaussian model}
  Suppose we model observations $X_1, \ldots, X_N$ as IID $N(\mu, \sigma^2)$ random variables. Find the MLE of $\theta = (\mu, \sigma^2)$.
  
  \mode<article>{
    \begin{itemize}
      \item Though we have two parameters, the approach for finding the MLE is similar: find the likelihood function of the entire dataset, take the logarithm, and the compute (partial) derivatives and set equal to zero.
      \item In the final step, we might end up with a system of equations rather than just a single equation.
      \item There's also an interesting feature of this example that is applicable elsewhere: should we find the MLE of $\sigma$, or the MLE of $\sigma^2$? Fortunately, you get the same result either way (we'll show this later).
      \item In this case, either approach is straightforward. As practice, you may want to consider finding the MLE of $\sigma^2$. Here, we will treat $\sigma$ as the parameter of interest, though sometimes it's easier to differentiate with respect to something like: $\theta_2 = \sigma^2$.
      \item Because of the IID assumption joint-likelihood function is:
      $$
      L(\mu, \sigma) = f(x^*;\, \mu, \sigma) = \prod_{i = 1}^N \frac{1}{\sigma\sqrt{2\pi}}\exp\Big(-\frac{1}{2}\big((x^*_i - \mu)/\sigma\big)^2\Big),
      $$
      \item The log-likelihood is therefore:
      \begin{align*}
      \ell(\mu, \sigma) &= \sum_{i = 1}^N \Big(\log(1) - \log(\sigma\sqrt{2\pi}) - \frac{1}{2}\big((x^*_i - \mu)/\sigma\big)^2\Big) \\
      &= -n\log(\sigma) - \frac{n}{2}\log 2\pi - \frac{1}{2\sigma^2}\sum_{i = 1}^N (x^*_i - \mu)^2.
      \end{align*}
      \item The partial derivatives are given by:
      \begin{align*}
        \frac{\partial \ell}{\partial \mu} &= \frac{1}{\sigma^2}\sum_{i = 1}^N (x^*_i - \mu) = 0 \\
        \frac{\partial \ell}{\partial \sigma} &= -\frac{n}{\sigma} + \sigma^{-3}\sum_{i = 1}^N (x_i^* - \mu)^2.
      \end{align*}
      \item Because $\sigma \geq 0$ (and zero if and only if the data are the same value), the first equation will only be zero if $\sum_{i = 1}^N (x^*_i - \mu) = 0$, or if $\mu = \bar{x}^*_N$, which is independent of $\sigma$. Thus, the MLE is:
      $$
      \hat{\mu} = \bar{x}^*_N.
      $$
      \item For the second equation, the partial derivative is zero if and only if
      $$
      \frac{n}{\sigma} = \sigma^{-3}\sum_{i = 1}^N (x_i^* - \mu)^2,
      $$
      or if
      $$
      \sigma^2 = \frac{1}{n}\sum_{i = 1}^N (x_i^* - \mu)^2.
      $$
      Since we are looking for the MLE of $\theta = (\theta, \sigma)$, we can now replace $\mu$ with it's maximizer $\hat{\mu} = \bar{x}^*_N$, giving us:
      $$
      \hat{\sigma} = \sqrt{\frac{1}{n}\sum_{i = 1}^N (x_i^* - \hat{\mu})^2} = \sqrt{\frac{1}{n}\sum_{i = 1}^N (x_i^* - \bar{x}^*_N)^2}.
      $$
      \item Like the Poisson example, the MLE for the iid normal model has the same estimator as the MoM procedure.
      \item The sampling distribution for $\theta$ is therefore:
      $$
      \hat{\mu} \sim N(\mu, \sigma^2 / N), \quad N\hat{\sigma}^2/\sigma^2 \sim \chi^2_{N-1},
      $$
      and the two distributions are independent.
    \end{itemize}
  }
  
\end{exampleblock}

\end{frame}

\section{Numeric Optimization}

\begin{frame}[allowframebreaks]{Numeric Optimization}
  \begin{itemize}
    \item In the previous examples, the MLE was available \emph{analytically}.
    \item In many cases, however, there is no closed-form solution for the MLE, and it must be computed numerically.
    \item The next example demonstrates this, and then we will discuss optimization strategies.
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Example: Gamma likelihood}
    Suppose we want to model data $X_1, X_2, \ldots, X_n$ as iid Gamma$(\alpha, \lambda)$, which has the density function:
    $$
    f(x;\, \alpha, \lambda) = \frac{1}{\Gamma(\alpha)}\lambda^\alpha x^{\alpha - 1}e^{-\lambda x}, \quad 0 \leq x < \infty.
    $$
    Find the MLE of $\theta = (\alpha, \lambda)$.
    
    \mode<article>{
      \begin{itemize}
        \item The joint likelihood under the IID assumption is:
        $$
        L(\theta) = \prod_{i = 1}^n \frac{1}{\Gamma(\alpha)}\lambda^\alpha {(x^*_i)}^{\alpha - 1}e^{-\lambda x^*_i}, \quad 0 \leq x^*_i < \infty.
        $$
        \item We'll drop the indicator function, since the observations are all bigger than zero (otherwise a Gamma model is a bad choice), so the value is one. 
        Taking logarithms, we get:
        \begin{align*}
        \ell(\alpha, \lambda) &= \sum_{i = 1}^n \big(\alpha \log \lambda + (\alpha - 1)\log x_i^* - \lambda x_i^* - \log\Gamma(\alpha)\big) \\
        &= n\alpha\log\lambda + (\alpha - 1)\sum_{i = 1}^n \log x_i^* - \lambda \sum_{i = 1}^n x^*_i - n\log \Gamma(\alpha).
        \end{align*}
        \item The partial derivatives are:
        \begin{align*}
          \frac{\partial \ell}{\partial \alpha} &= n \log \lambda + \sum_{i = 1}^n \log x_i^* - n \frac{\Gamma'(\alpha)}{\Gamma(\alpha)},\\
          \frac{\partial \ell}{\partial \lambda} &= \frac{n\alpha}{\lambda} - \sum_{i = 1}^n x_i^*.
        \end{align*}
        \item The second equation is the easiest to solve. Setting equal to zero, we have:
        $$
        \frac{n \alpha}{\lambda} = \sum_{i = 1}^n x_i^* = n\bar{x}^*_n.
        $$
        Thus, solving for $\lambda$, we get:
        $$
        \lambda = \frac{\alpha}{\bar{x}^*_n}.
        $$
        \item The estimate of $\lambda$ depends on the value of $\alpha$. Since we are looking for a maximum of both, it needs to be a critical point, so we can write the estimate of $\lambda$ as:
        $$
        \hat{\lambda} = \frac{\hat{\alpha}}{\bar{x}^*_n},
        $$
        implying that we need to first maximize the likelihood for $\alpha$. 
        
        TODO: Continue. Plug in $\hat{\lambda}$ to get the equation for $\hat{\alpha}$.
      \end{itemize}
    }
    
  \end{exampleblock}
  
\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version \Sexpr{getRversion()}.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4451_s26/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}

\bibliography{../bib4451}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4451}

}



\end{document}







