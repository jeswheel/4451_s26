\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{2}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4451_f25/}{Mathematical Statistics II}\\ \vspace{2mm}
Introduction to Point Estimation}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]

<<setup,include=FALSE,cache=FALSE,purl=FALSE,child="../setup.Rnw">>=
@

\begin{document}

\maketitle

\mode<article>{\tableofcontents}

% \mode<presentation>{
%   \begin{frame}{Outline}
%     \tableofcontents
%   \end{frame}
% }

\section{Introduction}

\begin{frame}[allowframebreaks]{Overview}
  \begin{itemize}
    \item We will formally introduce the idea of point estimation.
    \item In addition to an introduction, we will introduce the concept of the empirical distribution, as well as methods of moment estimators.
    \item The material for this section largely comes from Chapter~8 of \citet{rice07}.
    \end{itemize}
\end{frame}

\section{Point Estimation: An introduction}

\begin{frame}[allowframebreaks]{Point estimation}

\begin{itemize}
  \item In the previous lecture(s), we provided an example of Bayesian vs Frequentsist point-estimation via first principles.
  \item That is, using the various interpretations, we could reason an estimate for the probability $p$ in a binomial experiment.
  \item We are now interested in studying approaches for more general cases.
  \item Given a dataset and a chosen model, how can we estimate parameters?

\framebreak

  \item We will first start with some notation, and motivating examples.
  \item Term \emph{model} in this class will generally refer to a probability model, and can be based on a discrete or continuous probability measure.
  
  \begin{exampleblock}{Normal Model}
    The Normal (or Gaussian) family of distributions arises often in the real world. Examples include human heights (conditioned on gender), rainfall amounts, and many biological measurements are approximately normal (or log-normal).
    
    Given a set of observations $x_1, x_2, \ldots, x_n$, we may \emph{model} these as iid normal $X_i \sim N(\mu, \sigma^2)$, and our goal being using the data to estimate the values of $\mu$ or $\sigma$.
  \end{exampleblock}
  
  \framebreak
  
  \begin{exampleblock}{Regression}
    Sometimes the probability model is \emph{implicit}, but present. Consider the regression model:
    $$
    Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i.
    $$
    We often think of fitting this regression model by minimizing the average squared-error: $(Y_i - \hat{Y}_i)^2$. However, this approach typically corresponds to an implicit probability model for the error terms $\varepsilon_i$, namely a normal distribution with mean $0$. In this case, we might want to estimate $\beta_0$, $\beta_1$, and $\sigma^2$, which is $\Var(\varepsilon_i)$.
  \end{exampleblock}

  \framebreak
  
  \begin{exampleblock}{Poisson Process}
    Another common example is a Poisson Process model. Many real-world phenomena are well-approximated by a Poisson process, over space or time. Examples include arrival times at a gas station, number of meteors landing in a geographic area, radioactive decay, etc. Here, there is only one parameter we want to estimate using data, namely the rate $\lambda$.
  \end{exampleblock}

\end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]{Parameter Estimation}
  \begin{itemize}
    \item All of the above examples have the common feature that we pick a \emph{model}, and we want to use the model to describe the data-generating process.
    \item More accurately, however, we pick a candidate \alert{family} of models; (Gaussian family, Poisson Family, Linear Regression family, etc).
    \item Generally, the exact model needed within a \emph{family} of models is determined by a few parameters.
    \mode<article>{
    \begin{itemize}
      \item If the family is Gaussian, the model is determined by $\mu$ and $\sigma^2$.
      \item If the family is Poisson, the model is determined by $\lambda$. 
      \item If the family is linear-Gaussian regression, the model is determined by $\beta_0, \beta_1$, and $\sigma^2$. 
    \end{itemize}
    }
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Example: Gamma-Rainfall}
  \begin{itemize}
    \item The Gamma distribution depends on two parameters, $\alpha$ and $\lambda$:
    $$
    f_X(x; \alpha, \lambda) = \frac{1}{\Gamma(\alpha)}\lambda^\alpha x^{\alpha - 1}e^{-\lambda x}.
    $$
    \item The Gamma distribution is quite flexible, and works as a useful model for various situations.
    \item One example is modeling rainfall amounts per-storm under two conditions, cloud seeding vs not cloud seeding (simulated data, couldn't find original data).
    \item A Gamma distribution fits both samples well, but we get different parameters $\alpha$ and $\lambda$ for the two different samples
    \item Differences in the respective distributions are reflected in differences in the parameters $\alpha$ and $\lambda$. 
  \end{itemize}
  
\end{frame}

\begin{frame}[fragile]{Two-sample Rainfall}

<<echo=FALSE,include=FALSE,cache=TRUE>>=
library(tidyverse)
set.seed(123)

rainfall <- data.frame(
  seeded = rgamma(n = 500, shape = 0.6, scale = 15),
  unseeded= rgamma(n = 500, shape = 0.5, scale = 14.5)
) %>% 
  pivot_longer(
    cols = everything(),
    names_to = "class",
    values_to = "rainfall"
  )

rainfallVals <- data.frame(
  x = seq(1, 80, length.out = 1000),
  seeded = 5*500*dgamma(seq(1, 80, length.out = 1000), shape = 0.6, scale = 15),
  unseeded = 5*500*dgamma(seq(1, 80, length.out = 1000), shape = 0.5, scale = 14.5)
) %>% 
  pivot_longer(
    cols = c(seeded, unseeded),
    names_to = "class",
    values_to = "rainfall"
  )

ggplot(rainfall) + 
  geom_histogram(aes(rainfall), color = 'black', fill = 'grey59', breaks = seq(0, 80, 5)) + 
  facet_wrap(~class) + 
  theme_bw() + 
  geom_line(data = rainfallVals, aes(x = x, y = rainfall))

ggsave(filename = 'rainfall.png', width = 8, height = 3.75, units = 'in')
@

\begin{figure}[ht]
  \includegraphics[width=\textwidth]{rainfall.png}
  \caption{Data and model fit to two different Gamma distributions.}
\end{figure}
  
\end{frame}

% \begin{frame}[allowframebreaks]{Example: Radio-active decay}
%   \begin{itemize}
%     \item This example comes from Chapter~8.2 of \citet{rice07}.
%     \item Records of emissions of alpha particles from radioactive sources show that emissions per unit of time is not constant, but fluctuates in a seemingly random way.
%     \item If the emission rate is constant over the observation period (i.e., observation period much smaller than half-life), and there are a large number of independent sources (atoms), a Poisson model is appropriate.
%     \item Data are collected by the National Buereau of Standards.
%     \item The data are counts of alpha particles emitted by americium 241 measured over 10-second intervals.
%   \end{itemize}
%   
%   \framebreak
%   
%   \begin{table}[ht]
%   \centering
%   \begin{tabular}{rr}\hline
%     n & Observed
%   \end{tabular}
%   \end{table}
%   
% \end{frame}

\begin{frame}[allowframebreaks]{Notation and generalizations}
  \begin{itemize}
    \item We will generalize by using the following ideas and notations.
    \item We will denote the \emph{observed data} as $x^*_1, x_2^*, \ldots, x_N^*$, and use the shorthands $x_{1:N}^{*}$ if we emphasize the entire collection, and $x^*$ if the emphasis is not needed.
    \item We assume that the data are realizations of random variables $X_1, X_2, \ldots, X_N$, again using the notation $X_{1:N}$ for the collection of $N$ random variables, or $X$ if this is not needed. 
    \item In general, the data $x_i^*$ and random variables $X_i$ can be multivariate, but focus primarily on the univariate case.
    \item We will be interested in fitting a probabilistic model $f_{X_{1:N}}(x_{1:N};\theta)$ using the data. The model may correspond to a discrete probability, or a continuous probability. In these cases, $f$ is usually a pmf of pdf, respectively.
    \item Subscripts will be dropped occasionally if it is not necessary. For instance, $f(x;\theta)$ is taken to mean the model of all data $x = x_{1:N}$, and would formally be expressed as $f_{X_{1:N}}(x_{1:N}; \theta)$.
    \item This approach is sometimes called ``function overload"; it's not my favorite approach, but it is convenient. The meaning of the function is primarily understood by the arguments and context.
    \item The function $f(x;\theta)$ belongs to a particular \alert{family} of models, indexed by $\theta$, which is generally multivariate.
  \end{itemize}
  
  \framebreak
  
<<echo=FALSE,include=FALSE>>=
set.seed(222)
x <- round(rnorm(5, mean = 2), 2)
@
  
  
  \begin{exampleblock}{Normal model example}
    Suppose we observe the following data: \Sexpr{x}, and we would like to fit a normal model to the data, assuming the data are iid. Then $x_1^* = \Sexpr{x[1]}$, $x_2^* = \Sexpr{x[2]}$, and so forth, and the model family depends on $\theta = (\mu, \sigma^2)$, and the model can be expressed as:
    \begin{align*}
    f(x; \theta) &= f_{X_{1:5}}(x_{1:5}; \mu, \sigma^2) \\
    & = \prod_{i = 1}^5 f_{X_i}(x_i; \mu, \sigma^2) \\
    & = \prod_{i = 1}^5 \frac{1}{\sigma\sqrt{2\pi}}e^{-(x_i-\mu)^2/2\sigma^2}
    \end{align*}
    Our goal is to estimate $\mu$, $\sigma^2$ using the observed data $x^*_{1:5}$.
  \end{exampleblock}
  
  \end{frame}
  
  \begin{frame}{Notation and Generalization (continued)}
  
  \begin{itemize}
    \item Our goal now is to develop general procedures for estimating $\theta$, using observed data $x^*$, and a proposed family of models $f(x;\theta)$.
    \item We will develop three main approaches: (1) Method of Moments (2) Maximum Likelihood Estimation, and (3) Bayesian estimation.
    \item In this section, we will focus only on method of moments estimators.
    \item Once point estimation techniques are developed, we will provide theory about these estimates and their uncertainty; discussing bias, variance, and optimality of estimates.
  \end{itemize}
  
\end{frame}

\section{Empirical distribution}

\begin{frame}[allowframebreaks]{Motivation}
  \begin{itemize}
    \item The Method of Moments (MoM) estimation technique is a simple idea.
    \item Pick a family of models $f(x;\theta)$, and observed data $x^*$.
    \item The family of models will have theoretical moments, i.e., $\text{E}[X^k]$.
    \item Generally, these moments can be expressed in terms of the model parameters, $\theta$.
    \item Thus, we will estimate $\hat{\theta}$ so that the \alert{data moments} match the theoretical moments.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{The empirical distribution}
  \begin{itemize}
    \item One justification of this approach considers the \alert{empirical distribution} of observed data.
  \end{itemize}
\end{frame}

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version \Sexpr{getRversion()}.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4451_s26/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}
  
\bibliography{../bib4451}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4451}

}



\end{document}







