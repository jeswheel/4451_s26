\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{6}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Distributions Derived from the Normal Distribution}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]




\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{$\chi^2$ distributions}

\begin{frame}[allowframebreaks]{Introduction}
  \begin{itemize}
    \item This material comes primarily from \citet[][Chapter~6]{rice07}.
    \item Here, we introduce several important distributions that arise from transformations applied to normal distributions.
    \item Many of these distributions form the basis of traditional statistical inference procedures that are taught in introductory statistics courses.
    \item They are very useful in practice due to the central limit theorem: with enough observations, the limiting behavior of nearly all distributions is normal, so distributions that come from the normal distribution arise in practice as well.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{$\chi_\nu^2$ Distribution}
  \begin{itemize}
    \item The first distribution we will consider is the $\chi^2_1$ (Chi-square with 1 degree of freedom).
  \end{itemize}
  \begin{block}{Definition: $\chi^2_1$ distribution}
    If $Z$ is a standard normal random variable, then $X = Z^2$ is called the chi-square distribution with $1$ degree of freedom.
  \end{block}
  \begin{itemize}
    \item We typically use the notation $X \sim \chi^2_{1}$ (in LaTeX: \texttt{\textbackslash \!chi}).
  \end{itemize}
  \framebreak
  \begin{exampleblock}{The pdf of $\chi^2_1$}
    Let $X$ follow a $\chi^2_1$ distribution. Then, the pdf of $X$ is given by
    $$
    f_X(x) = \frac{1}{\sqrt{2\pi}}x^{-1/2}e^{-x/2}.  
    $$
    
    \mode<article>{
    \begin{proof}
    There are a few ways to show this is the case, and was one of the early examples we saw in Chapter 2. For practice, we repeat this example here.
    \begin{itemize}
      \item By definition, $X$ has the same distribution of $Z^2$, where $Z$ is a standard normal.
      \item Recall the standard normal density is:
      $$
      \phi(z) = \frac{1}{\sqrt{2\pi}}e^{-z^2/2}
      $$
      \item Using the CDF method, we write
      \begin{align*}
        F_X(x) &= P(X \leq x) \\
        &= P(Z^2 \leq x) \\
        &= P(-\sqrt{x} \leq Z \leq \sqrt{x}) \\
        &= P(Z \leq \sqrt{x}) - P(Z \leq -\sqrt{x}) \\
        &= \Phi(\sqrt{x}) - \Phi(-\sqrt{x}),
      \end{align*}
      \item Where $\Phi(z)$ is the cdf of $Z$.
      \item Taking the derivative of both sides of the equation, the chain rule gives us
      \begin{align*}
        f_X(x) &= \frac{1}{2}x^{-1/2}\phi(\sqrt{x}) + \frac{1}{2}x^{-1/2}\phi(-\sqrt{x}) \\
        &= x^{-1/2}\phi(\sqrt{x}),
      \end{align*}
      \item where the last step is a result of the symmetry of $\phi(x)$, noting $\phi(-x) = \phi(x)$ for all $x \in \R$.
      \item Thus, replacing $\phi(\sqrt{x})$ with the definition,
      $$
      f_X(x) = \frac{1}{\sqrt{2\pi}}x^{-1/2}e^{-x/2}
      $$
    \end{itemize}
    
    \end{proof}
    }
  \end{exampleblock}
  \framebreak
  \begin{itemize}
    \item In Chapter~2, we previously noted that that $f_X(x)$ is an example of a Gamma distribution.
    \item Specifically, the \emph{kernel} of the Gamma density is $x$ raised to some power, and $e$ raised to some multiple of $x$: 
    $$
    f_{\text{Gamma}}(x) \propto x^{\alpha - 1}e^{-\lambda x}.
    $$
    \item Thus, ignoring the constant for a moment, if $\alpha = 1/2$, $\lambda = 1/2$, then the pdf of $X \sim \chi^2_1$ is just this Gamma density: 
    $$
    f_{X}(x) \propto x^{-1/2}e^{-x/2} = x^{\alpha - 1}e^{-\lambda x}.
    $$
    \item Since both functions are proper probability density functions, they have to integrate to one, so the normalizing constant \emph{must} be the same.
    \item This is also easily verified. The normalizing constant of the Gamma distribution is $\lambda^\alpha / \Gamma(\alpha)$.
    \item With our specific values of $\lambda = \alpha = 1/2$, and recalling that $\Gamma(1/2) = \sqrt{\pi}$, 
    $$
    \frac{1}{\sqrt{2\pi}} = \frac{(1/2)^{(1/2)}}{\Gamma(1/2)} = \frac{\lambda^\alpha}{\Gamma(\alpha)}
    $$
  \end{itemize}
  \begin{alertblock}{MGF of $\chi^2_1$}
    We previously derived the MGF of a Gamma$(\alpha, \lambda)$ distribution: $M(t) = \big(\lambda / (\lambda - t)\big)^\alpha$. Thus, the MGF of a Chi-square(1) distribution is
    $$
    M(t) = (1 - 2t)^{-1/2}, \quad t < 1/2.
    $$
  \end{alertblock}
  
  \framebreak
  
  \begin{block}{Definition}
    If $U_1, U_2, \ldots, U_n$ are $n$ independent $\chi^2_1$ random variables, then
    $$
    V = U_1 + U_2 + \ldots + U_n
    $$
    then the distribution of $V$ is called the Chi-square distribution with $n$ degrees of freedom, denoted $\chi^2_n$. 
  \end{block}
  
  \begin{itemize}
    \item There are a few different ways of deriving the pdf of a $\chi^2_n$ random variable. Here, we will use the MGF uniqueness theorem.
    \item Let $M_i(t)$ denote the MGF of $U_i$, where $U_i \sim \chi^2_1$. Then, due to independence,
    \begin{align*}
      M_V(t) = M_{\sum_{i} U_i}(t) = \prod_{i = 1}^n M_i(t) = \big(M_t(t)\big)^n = (1 - 2t)^{-n/2}
    \end{align*}
    \item Compare this to the Gamma MGF: $M(t) = \big(\lambda / (\lambda - t)\big)^\alpha$. Then, setting $\lambda = 1/2$, $\alpha = n/2$, we see that $V$ has a Gamma$(n/2, 1/2)$ distribution.
    \item Thus, the pdf of $V$ is given by: 
    $$
    f_V(v) = \frac{1}{2^{n/2}\Gamma(n/2)}v^{(n/2) - 1}e^{-v/2}.
    $$
    \item The expected value and variance of the $\chi^2_n$ distribution can easily be found then by using the fact that it is a special case of a Gamma distribution.
  \end{itemize}
  
\end{frame}

\section{The $t$ and $F$ distributions}

\begin{frame}[allowframebreaks]{The Student's $t$ distributions}
  \begin{block}{The Student's $t$ distribution}
    If $Z \sim N(0, 1)$ and $U \sim \chi^2_n$, and $Z$ and $U$ are independent, then the distribution of $T$, where
    $$
    T = \frac{Z}{\sqrt{U / n}},
    $$
    is called the Student's $t$ distribution (or simply the $t$ distribution) with $n$ degrees of freedom, which is often denoted $t_{n}$
  \end{block}
  
  \begin{itemize}
    \item Students often forget the make sure that $Z$ and $U$ in the definition of the $t$ distribution are independent.
    \item The $t$ distribution is the distribution used to perform the famed ``$t$-test".
  \end{itemize}
  
  \framebreak
  
  \begin{alertblock}{The density of the $t_n$ distribution}
    The pdf of the $t$ distribution with $n$ degrees of freedom is:
    $$
    f(t) = \frac{\Gamma\big((n + 1) / 2\big)}{\sqrt{n\pi}\Gamma(n/2)}\Big(1 + \frac{t^2}{n}\Big)^{-(n+1)/2}
    $$
  \end{alertblock}
  
  \begin{itemize}
    \item The derivation of the pdf of a $t$ distribution is a good practice exercise.
    \item Recall it is defined as the ratio of two independent random variables; in Chapter~3, we derived a formula for computing densities of random variables of this form.
    \item Note that $f(t) = f(-t)$, and so $f$ is symmetric about zero.
    \item It also has a bell-curve shape similar to a normal distribution.
    \framebreak
    \item You can see as $n\rightarrow \infty$, the $t_n$ distribution converges to the standard normal (e.g., use Slutsky's theorem, good practice).
  \end{itemize}
  
\begin{figure}[ht]
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{tmp/figure/tDistFig-1} 

}


\end{knitrout}
\end{figure}
 
  
\end{frame}

\begin{frame}[allowframebreaks]{The $F$ distributions}
  
\end{frame}

\section{Sampling Distributions}

\begin{frame}[allowframebreaks]{The sample mean}

\begin{itemize}
  \item In what follows, we'll assume that we are taking samples $X_1, X_2, \ldots, X_n$ from a larger population.
  \item These samples can be repeated experiments, or repeated observations. However, we will assume in general that the samples are independent and identically distributed, unless stated otherwise.
  \item \alert{For the remainder of the chapter}, we will also assume $X_i \sim N(\mu, \sigma^2)$ for all $i$.

\framebreak

  \item As a reminder from earlier chapters, linear combinations of independent normal random variables are also normally distributed. Thus, if $X_1, X_2, \ldots, X_n$ are iid normal, then $\bar{X}_n$ is also normally distributed.
\end{itemize}

\begin{alertblock}{Sampling distribution of the mean}
  If $X_i$ are iid $N(\mu, \sigma^2)$, then $\bar{X}_n$ is normal, with
  $$
  E\Big[1/n \sum_i X_i\Big] = (1/n)\sum_i \mu = \mu,
  $$
  $$
  \Var\big(1/n \sum_i X_i\big) = 1/n^2 \sum_i \sigma^2 = \sigma^2 / n.
  $$
  Thus, 
  $\bar{X}_n \sim N(\mu, \sigma^2/n).$
\end{alertblock}

\framebreak

\begin{block}{Lemma~\CHAPTER.1: Independent Normal RVs}
  Let $X$ and $Y$ be normally distributed random variables.
  Then $X$ and $Y$ are independent, \alert{if and only if}
  $$
  \Cov(X, Y) = 0.
  $$
\end{block}

\begin{itemize}
  \item The above statement can be proved using the factorization theorem, and considering the MGF or pdf of a bivariate normal distribution. 
  \item Recall that for most distributions, independence implies $\Cov(X, Y) = 0$, but not the other way around.
  \item It turns out that the normal distribution is the only distribution that has this property.
\end{itemize}

\framebreak

\begin{block}{Theorem~\CHAPTER.1: Independence of Deviations}
  Let $X_1, \ldots, X_n$ be iid $N(\mu, \sigma^2)$ random variables.
  Then, $\bar{X}_n$ is independent of the vector of random variables called the \emph{deviations}, $\big(X_i - \bar{X}_n\big)_{i = 1}^n$.
  
  \mode<article>{
    \begin{proof}
      First, note that $\bar{X}_n$ is normally distributed. 
      Using Lemma~\CHAPTER.1, all we need to do is argue that the deviations are normally distributed, and that the covariance between $\bar{X}_n$ and $X_i - \bar{X}_n$ is zero.
      Because $\bar{X}_n$ and $X_i$ are both normally distributed, then for all $i$, $X_i - \bar{X}_n$ is just a linear combination of normally distributed random variables, and as a result is also normally distributed.
      Using the bilinearity of covariance,
      \begin{align*}
        \Cov(\bar{X_n}, X_i - \bar{X_n}) &= \Cov(\bar{X_n}, X_i) - \Cov(\bar{X}_n, \bar{X}_n) \\
        &= \Cov\big(\sum_{j = 1}^n \frac{1}{n}X_j, X_i\big) - \Cov\big(\sum_{i = 1}^n \frac{1}{n}X_i, \sum_{j = 1}^n \frac{1}{n}X_j\big) \\
        &= \frac{1}{n}\Cov(X_i, X_i) - \sum^n_{i = 1}\sum^n_{j = 1}\frac{1}{n^2} \Cov(X_i, X_j) \\
        &= \frac{\sigma^2}{n} - \frac{1}{n^2}\sum^n_{i = j} \Cov(X_i, X_j) \\
        &= \frac{\sigma^2}{n} - \frac{1}{n^2}(n\sigma^2) = 0.
      \end{align*}
      Thus, by Lemma~\CHAPTER.1, $\bar{X}_n$ is independent of $X_i - \bar{X_n}$ for all $i$. 
    \end{proof}
  }
  
\end{block}

\mode<presentation>{
  \emph{Proof.}
}

\framebreak

\begin{block}{Corollary~\CHAPTER.1}
  If the $X_i$ are iid $N(\mu, \sigma^2)$, then $\bar{X}_n$ is independent of the sample variance $S^2$, defined by
  $$
  S^2 = \frac{1}{n - 1}\sum_{i = 1}^n (X_i - \bar{X}_n)^2.
  $$
  
  \mode<article>{
    \begin{proof}
      First recall from one of our homework problems that $S^2$ is an unbiased estimate of $\sigma^2$: $E[S^2] = \sigma^2$ (Note that this does not require the assumption that the $X_i$ are normally distributed, just that they are independent and identically distributed with finite variance $\Var(X_i) = \sigma^2 < \infty$.)
      
      From Theorem~\CHAPTER.1, if the $X_i$ are normally distributed, then $\bar{X}_n$ is independent of the vector $\big(X_i - \bar{X}_n\big)_{i = 1}^n$. Because $S^2$ is just a function of $\big(X_i - \bar{X}_n\big)_{i = 1}^n$, we can immediate conclude that $\bar{X_n}$ is independent of $S^2$. 
    \end{proof}
  }
  
\end{block}

\framebreak

\begin{block}{Theorem~\CHAPTER.2}
  If the $X_i$ are iid normal, then $(n - 1)S^2 / \sigma^2$ has a chi-square distribution with $n-1$ degrees of freedom.
  
  \mode<article>{
  \begin{proof}
    First recall how we defined the $\chi^2_n$ distribution. First, if we square a standard normal $Z \sim N(0, 1)$, then $Z^2 \sim \chi^2_1$.
    Then, the sum of $n$ $\chi^2_1$ random variables is how we define a $\chi^2_n$ random variable, i.e., if $Z_i \overset{iid}{\sim} N(0, 1)$, then $Z_1^2 + \ldots + Z_n^2 \sim \chi^2_n$.
    Thus, because $E[X_i] = \mu$, $\Var(X_i) = \sigma^2$ for all $i$,
    $$
    \Big(\frac{X_i - \mu}{\sigma}\Big) \sim N(0, 1), 
    $$
    and therefore
    $$
    \frac{1}{\sigma^2} \sum_{i = 1}^n (X_i - \mu)^2 = \sum_{i = 1}^n \Big(\frac{X_i - \mu}{\sigma}\Big)^2 \sim \chi^2_n.
    $$
    Now we will expand this same sum by adding and subtracting $\bar{X}_n$:
    $$
    \frac{1}{\sigma^2} \sum_{i = 1}^n (X_i - \mu)^2 = \frac{1}{\sigma^2} \sum_{i = 1}^n \big((X_i - \bar{X}_n) + (\bar{X}_n - \mu)\big)^2.
    $$
    From above, we already know this entire expression results in a random variable with a $\chi^2_n$ distribution. We can now manipulate the right hand side to get the distribution of $(n-1)S^2 / \sigma$, which was our original goal.
    We will now expand the square, and note that $\sum_{i = 1}^n (X_i - \bar{X}) = 0$ to simplify:
    \begin{align*}
    \frac{1}{\sigma^2} \sum_{i = 1}^n (X_i - \mu)^2 &= \frac{1}{\sigma^2}\sum_{i = 1}^n (X_i - \bar{X}_n)^2 + \Big(\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}\Big)^2 \\
    V &= (n-1)S^2/\sigma^2 + U,
    \end{align*}
    where $U\sim \chi^2_1$, since it is the result of squaring a standard normal random variable, and $V \sim \chi^2_n$. 
    Importantly, $U$ is a function of $\bar{X}_n$, which was shown to be independent of $S^2$ in Corollary~\CHAPTER.1.
    As a result, we can use the identity for the MGF of the sum of independent random variables: $M_V(t) = M_{(n-1)S^2/\sigma^2} M_{U}(t)$.
    For the values of $t$ for which it is defined, we can then solve for the MGF of $(n-1)S^2/\sigma^2$, which uniquely determines the distribution of the random variable.
    As a reminder, the MGF of a $\chi^2_n$ random variable is $M(t) = (1 - 2t)^{-n/2}$. Thus,
    \begin{align*}
      M_{(n-1)S^2/\sigma^2} &= M_V(t) / M_U(t) \\
      &= \frac{(1 - 2t)^{-n/2}}{(1-2t)^{-1/2}} \\
      &= (1-2t)^{-(n-1)/2},
    \end{align*}
    which is the MGF of a $\chi^2_{n-1}$ random variable.
  \end{proof}
  }
  
\end{block}

\mode<presentation>{
\emph{Proof.}
}

\framebreak

\begin{block}{Theorem~\CHAPTER.3: $t$ distribution}
  Let $X_i$ be iid Normal$(\mu, \sigma^2)$ random variables, and let $\bar{X}_n$ and $S^2$ denote the sample mean and variance, respectively. Then,
  $$
  \frac{\bar{X}_n - \mu}{S / \sqrt{n}} \sim t_{n - 1}.
  $$
  
  \mode<article>{
  \begin{proof}
    The proof is straightforward application of Theorem~\CHAPTER.2, and the definition of the $t$ distribution with $n$ degrees of freedom. We will rewrite the expression as follows: 
    \begin{align*}
       \frac{\bar{X}_n - \mu}{S / \sqrt{n}} &= \frac{\Big(\frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}}\Big)}{\sqrt{S^2}/\sigma^2}
    \end{align*}
    The numerator is a $N(0, 1)$ random variable. By Corollary~\CHAPTER.1, the numerator is independent of the denominator.
    The denominator is a the square root of an independent random variable with $\chi^2_{n-1}$, divided by its degrees of freedom $(n - 1)$:
    $$
    \frac{(n-1)S^2}{\sigma^2(n-1)}. 
    $$
    By the definition of the $t$ distribution, the ratio has a $t_{n-1}$ distribution.
  \end{proof}
  }
  
\end{block}

\mode<article>{
\emph{Proof.}
}

\end{frame}

\begin{frame}[allowframebreaks]{Final Comments}
  \begin{itemize}
    \item The identity in Theorem~\CHAPTER.3 provides the theoretical justification for a one-sample $t$-test. The justification for a two-sample $t$-test is derived in a similar way.
    \item Theorem~\CHAPTER.3 relies on the $X_i$ coming from a normal population, then this distribution is exact. The normal distribution arises in a large number of real-world applications.
    \item In practice, however, the $t$-test is often used even when the samples $X_i$ do not come from a normal distribution.
    \item The justification of this practice can heuristically be justified by the central limit theorem: Even if the $X_i$ are not normally distributed, $\bar{X}_n - \mu$ will be approximately normally distributed, with even relatively small $n$.
    \item Note, however, that the limiting distribution of the statistic in Theorem~\CHAPTER.3 is normal, not $t$. This can be shown with Slutsky's Theorem, noting that $S_n^2 \plim \sigma^2$ (see CLT example in Chapter~5 slides.)
    \item In practice, the $t$ distribution is often used because it has heavier tails than the normal distribution, and thus leads to conservative estimates of the distribution of the statistic in Theorem~\CHAPTER.3. Simulation studies suggest this approximation is quite good, even when there are large deviations from normality.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{The $F$ distributions (ommited earlier)}
  \begin{itemize}
    \item The $F$ distribution is often used as the null-distribution for hypothesis test.
    \item Used regularly in ANOVA, testing overall significance in a multiple regression model, and comparing population variances.
  \end{itemize}
  \begin{block}{Definition: $F$-distribution}
    Let $U$ and $V$ be independent chi-square random variables with $m$ and $n$ degrees of freedom, respectively. The distribution of
    $$
    W = \frac{U/m}{V/n}
    $$
    is called the $F$ distribution with $m$ and $n$ degrees of freedom, denoted $F_{m,n}$.
  \end{block}
  
  \framebreak
  
  \begin{exampleblock}{Density of the $F$ distribution}
    If $W \sim F_{m, n}$, then the density of $W$ is given by 
    $$
    f(w) = \frac{\Gamma\big((m + n) / 2\big)}{\Gamma(m / 2)\Gamma(n / 2)}\Big(\frac{m}{n}\Big)^{m / 2}w^{m/2 - 1}\Big(1 + \frac{m}{n}w\Big)^{-(m+n)/2}.
    $$
  \end{exampleblock}
  \begin{itemize}
    \item Note that $W$ is just the ratio of two independent random variables, so this can be derived using results from Chapter~3 on the density of the ratio of two independent random variables. This is a good practice exercise.
    \item It can be shown that the square of a $t_n$ random variable follows an $F_{1, n}$ distribution.
  \end{itemize}
  
\end{frame}


% TODO: Add examples of CLT (statistical practice), Delta method, and second-order delta-method.

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version 4.5.2.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}
  
\bibliography{../bib4450}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







