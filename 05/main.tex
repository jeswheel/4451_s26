\input{../header}

% \mode<beamer>{\usetheme{AnnArbor}}
\mode<beamer>{\usetheme{metropolis}}
\mode<beamer>{\metroset{block=fill}}
% \mode<beamer>{\usecolortheme{wolverine}}

\mode<beamer>{\setbeamertemplate{section in toc}[sections numbered]}
\mode<beamer>{\setbeamertemplate{subsection in toc}[subsections numbered indented]}

% \mode<beamer>{\usefonttheme{serif}}
\mode<beamer>{\setbeamertemplate{footline}}
\mode<beamer>{\setbeamertemplate{footline}[frame number]}
\mode<beamer>{\setbeamertemplate{frametitle continuation}[from second][\insertcontinuationcountroman]}
\mode<beamer>{\setbeamertemplate{navigation symbols}{}}

\mode<handout>{\pgfpagesuselayout{2 on 1}[letterpaper,border shrink=5mm]}

\newcommand\CHAPTER{5}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space

\title{\vspace{2mm} \link{https://jeswheel.github.io/4450_f25/}{Mathematical Statistics I}\\ \vspace{2mm}
Chapter \CHAPTER: Limit theorems}
\author{Jesse Wheeler}
\date{}

\setbeamertemplate{footline}[frame number]




\begin{document}

\maketitle

\mode<article>{\tableofcontents}

\mode<presentation>{
  \begin{frame}{Outline}
    \tableofcontents
  \end{frame}
}

\section{Convergence Concepts}

\begin{frame}[allowframebreaks]{Introduction}
  \begin{itemize}
    \item This material comes primarily from \citet[][Chapter~5]{rice07}, but will be supplemented with material from \citet[][Chapter~5]{casella24}.
    \item In this chapter, we are interested in the convergence of sequences of random variables.
    \item For instance, we are interested in the convergence of the sample mean, $\bar{X}_n = (X_1 + X_2 + \ldots + X_n) / n$, as the number of samples $n$ grows.
    \item Because $\bar{X}_n$ is itself a random variable, we have to carefully define what it means for the convergence of a random variable.
    \item In this class, we are mainly concerned with three types of convergence.
    \item Because convergence of random variables is a tricky topic, we will treat them in varying amounts of detail.
  \end{itemize}
\end{frame}

\begin{frame}[allowframebreaks]{Convergence in Probability}
  
  \begin{itemize}
    \item The first type of convergence is one of the weaker types, and is usually easy(ish) to verify.

  \begin{block}{Definition: Convergence in Probability}
    A sequence of random variables $X_1, X_2, \ldots$ \alert{converges in probability} to a random variable $X$ if, for every $\epsilon > 0$,
    $$
    \lim_{n \rightarrow \infty} P\big(|X_n - X| \geq \epsilon \big) = 0
    $$
    or, equivalently,
    $$
    \lim_{n \rightarrow \infty} P\big(|X_n - X| < \epsilon\big)= 1.
    $$
  \end{block}
    
    \item We often use the shorthand $X_n \Plim X$ to denote ``$X_n$ converges in probability to $X$ as $n$ goes to infinity". 
    \item Note that the $X_i$ in the definition above do \emph{not} need to be independent and identically distributed.
    \item The distribution of $X_n$ changes as the subscript changes, and each of the convergence concepts we will discuss will describe different ways in which the distribution of $X_n$ converges to some limiting distribution as the subscript becomes large.
    \item A special case is when the limiting random variable $X$ is a constant.
  \end{itemize}
  
  \begin{block}{Example: The (Weak) Law of Large Numbers}
    Let $X_1, X_2, \ldots$ be iid random variables with $E[X_i] = \mu$ and $\Var(X_i) = \sigma^2$.
    Define $\bar{X}_n = (1/n)\sum^{n}_{i = 1} X_i$. Then $\bar{X}_n \Plim \mu$.
    
    \mode<article>{
    \begin{proof}
    
      The proof is a straightforward application of Chebychev's Inequality.
        
      \begin{itemize}
        \item We want to show that 
        $$\lim_{n \rightarrow \infty} P\big(|\bar{X}_n - \mu| \geq \epsilon \big) = 0.$$ 
        \item For every $\epsilon > 0$, Chebychev's inequality gives us:
        \begin{align*}
        P\big(|\bar{X}_n - \mu| \geq \epsilon \big) &= P\big((\bar{X}_n - \mu)^2 \geq \epsilon^2\big) \\
        &\leq \frac{E\big[(\bar{X}_n - \mu)^2\big]}{\epsilon^2} \\
        &= \frac{\Var(\bar{X}_n)}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2}.
        \end{align*}
        \item Thus, taking the limit, we have $\lim_{n \rightarrow \infty} P\big(|\bar{X}_n - \mu| \geq \epsilon \big) = 0$
      \end{itemize}
    \end{proof}
    }
    
  \end{block}
  
  \mode<presentation>{
  \emph{Proof.}
  }
  
  \framebreak
  
  \begin{itemize}
    \item The WLLN is very elegant; under general conditions, the sample mean of independent random variables approaches the population mean as $n \rightarrow \infty$.
    \item This is also used for proportions, as proportions are just means of indicator random variables.
    \item The WLLN can also be extended to show that the results hold even if the variance is infinite, the only condition needed is that the expectation is finite. However, the proof in this case is beyond the scope of this course.
    \item When a sequence of the ``same" sample quantity approaches a constant, we say that the sample quantity is \emph{consistent}.
  \end{itemize}
  
  \framebreak
  
  % \begin{exampleblock}{Example: Consistency of sample variance}
  %   Suppose $X_1, X_2, \ldots$ are sequence of iid random variables with $E[X_i] = \mu$ and $\Var(X_i) = \sigma^2 < \infty$. Define the sample variance as
  %   $$
  %   S_n^2 = \frac{1}{n-1} \sum_{i = 1}^n (X_i - \bar{X}_n)^2.
  %   $$
  %   Show that $S_n^2 \Plim \sigma^2$.
  %   
  %   \mode<article>{
  %   \begin{proof}
  %     Once again, we will use the Chebychev inequality. We have
  %     $$
  %     P\big(|S_n^2 - \sigma^2| \geq \epsilon\big) \leq \frac{E\big[(S_n^2 - \sigma^2)^2\big]}{\epsilon^2} = \frac{\Var (S_n^2)}{\epsilon^2}.
  %     $$
  %     Thus, a sufficient condition that $S_n^2$ converges in probability to $\sigma^2$ is that $\Var(S_n^2) \rightarrow 0$ as $n \rightarrow \infty$. 
  %   \end{proof}
  %   }
  % \end{exampleblock}
  % 
  % \mode<presentation>{
  % \emph{Proof.}
  % }
  
  \begin{itemize}
    \item A natural extension of the definition of the convergence of probability, is convergence of functions of random variables: $h(X_1), h(X_2), \ldots$. 
  \end{itemize}
  
  \begin{block}{Theorem: Convergence in probability for continuous functions}
    Let $X_1, X_2, \ldots$ be a sequence of random variables that converges in probability to a random variable $X$, and let $h$ be a continuous function.
    
    Then, $h(X_1), h(X_2), \ldots$ converges in probability to $h(X)$.
  \end{block}
  
\end{frame}

\begin{frame}[allowframebreaks]{Almost sure convergence}
  \begin{itemize}
    \item Our next convergence concept is stronger than convergence in probability.
    
    \begin{block}{Definition: Almost Sure Convergence}
      A sequence of random variables $X_1, X_2, \ldots$ converge \alert{almost surely} to a random variable $X$ if, for every $\epsilon > 0$,
      $$
      P\big(\lim_{n\rightarrow \infty} |X_n - X| < \epsilon \big) = 1,
      $$
      or
      $$
      P\big(\lim_{n \rightarrow \infty} X_n = X\big) = 1. 
      $$
    \end{block}
    
    \item Almost sure convergence is often written as $X_n \overset{a.s.}{\rightarrow} X$.
    \item It appears similar to convergence in probability, but they are in fact very different. In particular, almost sure convergence is a stronger concept.
    \item One way to think about this difference is that the probability gives a weight to individual sets.
    \item For convergence in probability, the set where $|X_n - X| > \epsilon$ can have positive probability, but that probability converges to zero for large $n$. 
    \item For almost sure convergence, the set where $|X_n - X| > \epsilon$ has probability zero. This doesn't imply that the set $|X_n - X| > \epsilon$ is empty, but it has zero probability.
    \item Almost sure convergence is very similar to pointwise convergence of a sequence of functions. This is no accident, as random variables \emph{are} functions:
    $$
    P\big(\omega \in \Omega: \lim_{n\rightarrow \infty} X_n(\omega) = X(\omega)\big) = 1.
    $$
    \item In the equivalent definition above, we see we must have point-wise convergence \alert{almost-everywhere}, except for the possibility that for some set $N \subset \Omega$ such that $P(N) = 0$, we allow $s \in N$ to not converge: $\lim_{n\rightarrow \infty} X_n(s) \neq X(s)$. 
  \end{itemize}
  
  \framebreak
  
  \begin{exampleblock}{Example: Convergence in prob, not a.s.}
    Let the sample space $\Omega = [0, 1]$, and assign the uniform probability on this interval. Define the sequence of random variables $X_i$ as:
    % \begin{align*}
      $X_1(s) = s + 1_{[0, 1]}(s), \, X_2(s) = s + 1_{[0, \frac{1}{2}]}(s),\, X_3(s) = s + 1_{[\frac{1}{2}, 1]}(s), 
      X_4(s) = s + 1_{[0, \frac{1}{3}]}(s),\, X_5(s) = s + 1_{[\frac{1}{3}, \frac{2}{3}]}(s),\, X_6(s) = s + 1_{[\frac{2}{3}, 1]}(s), \ldots$, and then define $X(s) = s$. We can see that $X_n \Plim X$. However, $X_n$ does not converge almost surely, because there is \alert{no} values $s \in \Omega$ that satisfy $X_n(s) \rightarrow X(s)$. For every $\omega$, the value of $X_n(s)$ alternates between $s$ and $s + 1$ infinitely often. 
    % \end{align*}
  \end{exampleblock}
  
  \framebreak
  
  \begin{block}{Theorem: almost sure convergence implies convergence in probability}
    If $X_1, X_2, \ldots$ are a sequence of random variables such that $X_n \overset{a.s.}{\rightarrow} X$, for some random variable $X$, then $X_n \Plim X$.
  \end{block}
  
  \begin{itemize}
    \item The converse of the statement above is false. That is, convergence in probability does not imply almost sure convergence.
    \item A proof of the theorem above, as well as additional treatment of the connection between almost sure convergence and convergence in probability is found in \citet[][Chatper~6]{resnick19}.
    \item Note: As stated, the weak-law of large numbers (WLLN) can actually be shown to hold a.s., in which case we call it the strong-law of large numbers (SLLN).
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{Convergence in Distribution}
  \begin{itemize}
    \item The final form of convergence we will consider in this course is convergence in distribution.
  \end{itemize}
  
  \begin{block}{Definition: Convergence in Distribution}
    A sequence of random variables $X_1, X_2, \ldots$ \alert{converges in distribution} to a random variable $X$ if
    $$
    \lim_{n \rightarrow \infty} F_{X_n}(x) = F_X(x)
    $$
    at all points $x$ where $F_X(x)$ is continuous.
  \end{block}
  
  \begin{itemize}
    \item One way to think about convergence in distribution is that it's really a statement about the long-run behavior of a sequence of random variables, as it's a statement about the CDFs. 
    \item This is different from the other types of convergence, which are concerned with the random variable itself.
    \item A quick recap of how the different types of convergence are related:
    \begin{itemize}
      \item a.s. convergence $\implies$ convergence in prob $\implies$ convergence in Distribution.
    \end{itemize}
    \item In a \emph{few} special scenarios, we can talk about more connections between the types of convergence.
    \item One such example is convergence in probability to a constant. \citet[Theorem~5.5.13 of][]{casella24} shows that $X_n \Plim a$ for some constant $a$ if and only if $X_n \dlim a$. 
  \end{itemize}
  
\end{frame}

\begin{frame}[allowframebreaks]{The Central Limit Theorem}
  \begin{itemize}
    \item Next we are going to introduce the Central Limit Theorem (CLT).
    \item The CLT is easily one of the most important theorems across all scientific disciplines, and arguably the most important result to modern science.
    
    \vspace{2mm}
    
    \begin{quote}
      ``I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the [CLT]. The law would have been personified by the Greeks and deified, if they had known of it..." - Sir Francis Galton
    \end{quote}
    
    \item The theory for the CLT was developed over a period of roughly 100 years, done by some of the greatest mathematicians of the 19h and 20th centuries.
    \item The theorem states that, under very weak conditions, the sum of any sequence of iid random variables (with finite mean and variance) converges to a normal distribution.
    \item Here, we are going to work towards a proof of a simple (weak) version of the theorem. 
    
    % \item Add some primer about the CLT
    % \item Prove the CLT. 
  \end{itemize}
  
  \begin{block}{Theorem: Continuity Theorem}
    Let $X_n$ be a sequence of random variables with cdf $F_n(x)$, and let $X$ be a random variable with cdf $F(x)$.
    Furthermore, let $M_n(t)$ be the moment generating function of $X_n$, and $M(t)$ the moment generating function of $X$. 
    
    If $M_n(t) \rightarrow M(t)$ for all $t$ in an open interval containing zero, then $F_n(X) \rightarrow F(x)$ at all continuity points of $F$. That is, $X_n \dlim X$.
  \end{block}
  
  \begin{itemize}
    \item Now, we do a brief reminder about Taylor Series and Taylor's Thereom
  \end{itemize}
  
  \begin{block}{Theorem: Taylor Series}
    If a function $f(x)$ has derivatives of order $k$, that is, $\frac{d^k}{dx^k} f(x)$ exists, then for any constant $a$, the \alert{Taylor Polynomial} of order $k$, centered about $a$, is
    $$
    f(x) = \sum_{n = 0}^k \frac{f^{(n)}(a)}{n!}(x - a)^n + R_k(x),
    $$
    where $R_k(x) = h_k(x)(x - a)^k$, for some $h_k$ such that $\lim_{x \rightarrow a} h_k(x) = 0$.
  \end{block}
  
  \begin{itemize}
    \item In particular, it means that we can use a $k$ order polynomial to approximate a differentiable function, and the remainder term $R_k(x)$ goes to zero at a rate smaller than the rate that $(x - a)^k$ goes to zero.
  \end{itemize}
  
  \begin{block}{Theorem: The (classic) Central Limit Theorem}
    Let $X_1, X_2, \ldots$ be a sequence of independent and identical random variables with mean $E[X_i] = \mu$ and variance $\Var(X_i) = \sigma^2 < \infty$.
    Assume that the mgf of $X_i$ exists and is defined in a neighborood of zero, and denote the cdf and mgf as $F$ and $M$, respectively. Then,
    $$
    \sqrt{n}\big(\bar{X}_n - \mu\big) \dlim N(0, \sigma^2). 
    $$
    
    \mode<article>{
    \begin{proof}
      The idea for this proof is tho show that, for some neighborhood around $t = 0$, the mgf of $\sqrt{n}(\bar{X}_n - \mu) / \sigma$ converges to $e^{t^2/2}$, which is the mgf of a $N(0, 1)$ random variable.
      For simplicity, we will assume that the mgf $M(t)$ exists around a symmetric neighborhood around zero (it could be larger), i.e., it exists if $|t| < h$ for some $h > 0$.
      
      Let $Z_i = (X_i - \mu) / \sigma$. Using the properties of mgf, the mgf of $Z_i$, denoted $Z_i$ exists for $|t| < \sigma t$. The exact form of $M_Z(t)$ can be found using the theorem on the mgf of linear transformations of random variables, but is not needed. 
      Now note that the target RV $\sqrt{n}(\bar{X}_n - \mu) / \sigma$ can be written as:
      $$
      \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} = \frac{1}{\sqrt{n}}\sum_{i = 1}^n Z_i.
      $$
      Thus, using the properties of the mgf, we have
      \begin{align*}
        M_{\sqrt{n}(\bar{X}_n - \mu) / \sigma}(t) &= M_{\sum_{i = 1}^n Z_i / \sqrt{n}}(t) \\
        &= M_{\sum_{i = 1}^n Z_i}\big(t/\sqrt{n}\big) \\
        &= \Big(M_{Z}\big(t/\sqrt{n}\big)\Big)^n.
      \end{align*}
      We now do a second order Taylor-Expansion of $M_Z\big(t/\sqrt{n}\big)$ about $0$:
      \begin{align*}
      M_Z\big(t/\sqrt{n}\big) &= M_Z(0) + M^{(1)}_Z(0)\frac{(t /\sqrt{n})^1}{1!} + M^{(2)}_Z(0)\frac{(t /\sqrt{n})^2}{2!} + R_2\big(t/\sqrt{n}\big)
      \end{align*}
      In particular, we note that $M_Z(0) = E[e^{0X}] = E[1] = 1$, and by how the mgf and $Z$ are defined, we have $M_Z'(0) = E[Z] = 0$, and $M_Z''(0) = E[Z^2] = \Var(Z) = 1$ (since the mean is zero). Therefore
      \begin{align*}
      M_Z\big(t/\sqrt{n}\big) &= 1 + \frac{(t/\sqrt{n})^2}{2!} + R_2\big(t/\sqrt{n}\big) \\
      &= 1 + \frac{t^2}{2n} + R_2\big(t/\sqrt{n}\big)
      \end{align*}
      Now by Taylor's theorem, there exists some function $h_2(x)$ such that
      $$
      h_2(x) = \frac{R_2\big(t/\sqrt{n}\big)}{(t/\sqrt{n})^2},
      $$
      where
      $$
      \lim_{t\rightarrow 0} \frac{R_2\big(t/\sqrt{n}\big)}{(t/\sqrt{n})^2} = 0.
      $$
      or equivalently,
      $$
      \lim_{n\rightarrow\infty} \frac{R_2\big(t/\sqrt{n}\big)}{(t/\sqrt{n})^2} = 0.
      $$
      Since $t$ is fixed, (and because the below equality holds when $t=0$), this implies:
      $$
      \lim_{n \rightarrow \infty} \frac{R_2\big(t/\sqrt{n}\big)}{(1/\sqrt{n})^2} = \lim_{n\rightarrow \infty} nR_2\big(t/\sqrt{n}\big) = 0.
      $$
      Finally, returning to the mgf of $\sqrt{n}\big(\bar{X}_n - \mu\big)$, we take the limit as $n \rightarrow \infty$:
      \begin{align*}
        \lim_{n\rightarrow \infty} M_{\sqrt{n}(\bar{X}_n - \mu) / \sigma}(t) &= \lim_{n\rightarrow\infty} \Big(M_{Z}\big(t/\sqrt{n}\big)\Big)^n \\
        &= \lim_{n\rightarrow \infty} \Big(1 + \frac{t^2}{2n} + R_2\big(t/\sqrt{n}\big)\Big)^n \\
        &= \lim_{n\rightarrow \infty} \Big(1 + \frac{1}{n}\Big[\frac{t^2}{2} + nR_2\big(t/\sqrt{n}\big)\Big]\Big)^n \\
        &= \lim_{n\rightarrow \infty} \Big(1 + \frac{a_n}{n}\Big)^n,
      \end{align*}
      where $a_n = \frac{t^2}{2} + nR_2\big(t/\sqrt{n}\big)$. Here, we note that $a_n \rightarrow \frac{t^2}{2}$ due to the convergence of $nR_2\big(t/\sqrt{n}\big)\rightarrow 0$, and we can apply the theorem \citep[e.g., Lemma~2.3.14 of][]{casella24} that if $a_n \rightarrow a$, then
      $$
      \lim_{n\rightarrow \infty}\Big(1 + \frac{a_n}{n}\Big)^n = e^a.
      $$
      Therefore, we get
      $$
      \lim_{n\rightarrow \infty} M_{\sqrt{n}(\bar{X}_n - \mu) / \sigma}(t) = e^{t^2/2}.
      $$
      Thus, by the continuity theorem,
      $$
      \frac{\sqrt{n}(\bar{X}_n - \mu)}{\sigma} \dlim N(0, 1).
      $$
    \end{proof}
    }
    
  \end{block}
  
  \framebreak
  
  \begin{itemize}
    \item One practical implication of the CLT is that, for large $n$, we can approximate
    $$
    \bar{X}_n \overset{d}{\approx} N\big(\mu, \sigma^2/n\big),
    $$
    if $X_i$ are independent and identically distributed with finite mean and variance.
    \item In practice, $n\approx 30$ has been found to lead to good approximations, but it depends heavily on the distribution of $X_i$. 
    \item A further investigation of the CLT proof shows that the convergence towards the normal distribution happens at a rate of $1/\sqrt{n}$.
    \item If we used a Taylor-series approximation with one additional order, we could derive a more accurate approximation under additional conditions known as the Edgeworth Expansion \citep[See Theorem~19.3 of][]{keener10}.
    These are less commonly used in practice, because you need finite third moments.
  \end{itemize}
  
  \begin{exampleblock}{Example: Binomial-Normal Approximation}
    Let $X\sim \text{Binomial}(n, p)$. For any $k \in \{0, 1, \ldots\}$, approximate $P(X\leq k)$.
    
    \mode<article>{
    \begin{itemize}
      \item Note that if $X$ is binomial distributed, it has the same distribution as the sum of Bernoulli random variables.
      \item Let $X_1, X_2, \ldots, X_n$ be Bernoulli$(p)$ random variables, and then $X \overset{d}{=} \sum_i X_i$.
      \item Recall $E[X_i] = p$, $\Var(X_i) = p(1-p)$. 
      \item By considering $\bar{X}_n = \frac{1}{n}\sum_i X_i$, we can use the CLT to approximate:
      $$
      \sqrt{n}\big(\bar{X}_n - p) \overset{d}{\approx} N\big(0, p(1-p)\big).
      $$
      \item Therefore,
      $$
      \bar{X}_n \overset{d}{\approx} N\big(p, p(1-p)/n\big),
      $$
      and
      $$
      X \overset{d}{=} \sum_i X_i = n\bar{X}_n \overset{d}{\approx} N\big(np, np(1-p)\big).
      $$
      Using this approximation, we have
      $$
      P(X \leq k) \approx \Phi\Big(\frac{k - np}{\sqrt{np(1-p)}}\Big).
      $$
      \item One thing to note is we can make a fairly simple improvement to this approximation by doing a continuity correction. 
      \item Specifically, $X$ only takes on real values, so if we approximate it with a continuous distribution $X \overset{d}{\approx} Y$, we generally get a more accurate approximation if we use $P(X \leq k) \approx P(Y \leq k + 0.5)$.
    \end{itemize}
    }
    
  \end{exampleblock}

\end{frame}

\begin{frame}[allowframebreaks]{Slutsky's Theorem}
  \begin{itemize}
    \item The following theorem is useful for our notes and supporting other ideas we will cover. However, we won't discuss the proof because it relies on other convergence concepts we don't cover in this class
  \end{itemize}
  
  \begin{block}{Theorem: Slutsky's Theorem}
    If $X_n \dlim X$ and $Y_n \plim a$ for some random variable $X$ and constant $a$, then
    \begin{enumerate}
      \item $Y_nX_n \dlim aX$.
      \item $X_n + Y_n \dlim X + a$. 
    \end{enumerate}
  \end{block}
  
  \framebreak
  
  \begin{exampleblock}{Example: CLT with estimated variance}
    (HW problem)? Suppose that $X_i$ are iid $N(\mu, \sigma^2)$ random variables.
    By the CLT, we have
    $$
    \frac{\sqrt{n}\big(\bar{X}_n - \mu\big)}{\sigma} \dlim N(0, 1).
    $$
    The problem with this theorem in practice is that we assume $\sigma$ is known, which is often not practical. If $S^2_n$ is our estimate of the variance, and $S_n^2 \plim \sigma^2$, then it can be shown that $\sigma / S_n \plim 1$. Thus, by Slutsky's Threom:
    $$
    \frac{\sqrt{n}\big(\bar{X}_n - \mu\big)}{S_n} = \frac{\sigma}{S_n}\frac{\sqrt{n}\big(\bar{X}_n - \mu\big)}{\sigma} \dlim N(0, 1)
    $$
  \end{exampleblock}
  
\end{frame}

\begin{frame}[allowframebreaks]{Delta-Method}
  \begin{itemize}
    \item The CLT is useful for determining the limiting distribution of a random variable (particularly, sums of iid random variables).
    \item As we have already discussed, we are often interested in functions of random variables.
    \item This next section gives theorems for the limiting distribution of functions of random variables.
  \end{itemize}
  
  \framebreak
  
  \begin{block}{Theorem: The Delta-Method}
    Let $X_n$ be a sequence of random variables that satisfy
    $$
    \sqrt{n}\big(X_n - \theta\big) \dlim N(0, \sigma^2),
    $$
    where $\theta, \sigma^2 < \infty$.
    Now suppose that $g$ is a function such that it's first derivative $g'$ exists and $g'(\theta) \neq 0$. 
    Then,
    $$
    \sqrt{n}\big(g(X_n) - g(\theta)\big)\dlim N\big(0, \sigma^2 [g'(\theta)]^2\big).
    $$
    
    \mode<article>{
    \begin{proof}
    \begin{itemize}
      \item For brevity, this will be more of a proof sketch than a formal proof. First, we do a Taylor-series expansion around $X_n = \theta$:
      $$
      g(X_n) = g(\theta) + g'(\theta)(X_n - \theta) + \text{Remainder}.
      $$
      \item As previously discussed, we have the remainder going to zero as $n \rightarrow \infty$ when considering the function $g(x)$ (not random).
      \item The statement of the theorem implies that $X_n \plim \theta$ as $n \rightarrow \infty$, and as a consequence, the remainder term also converges to zero in probability. In fact, careful treatment (like was done with the CLT), we have $\sqrt{n}\,\text{Remainder} \plim 0$.
      \item In particular, the remainder is of the form:
      $$
      \text{Remainder} = h(X_n)(X_n - \theta)^2,
      $$
      for some function $h$, that satisfies $\lim_{x \rightarrow \theta} h(x) = 0$.
      \item Since $\sqrt{n}(X_n - \theta)$ has limiting distribution $N(0, \sigma^2)$, we can conclude that $\sqrt{n}(X_n -\theta)^2 = \big(\sqrt{n}(X_n-\theta)\big)^2/\sqrt{n}$ converges in probability to $0 \times Z^2 = 0$. Furthermore, because $h(X_n)$ just converges in probability to a constant $h(\theta)$ (continuous mapping theorem), we can conclude that $\sqrt{n}\text{Remainder}\plim 0$.
      \item Thus,
      \begin{align*}
        g(X_n) - g(\theta) &= g'(\theta)(X_n - \theta) + \text{Remainder} \\
        \sqrt{n}\big(g(X_n) - g(\theta)\big) &= g'(\theta)\sqrt{n}(X_n - \theta) + \sqrt{n}\,\text{Remainder}
      \end{align*}
      \item Our assumption states that $g'(\theta)\sqrt{n}(X_n - \theta) \dlim N\big(0, \sigma^2[g'(\theta)]^2\big)$.
      \item Thus, by Slutsky's theorem, because $\sqrt{n}\,\text{Remainder} \plim 0$, we have the left hand sign converging to the desired result:
      $$
      \sqrt{n}\big(g(X_n) - g(\theta)\big)\dlim N\big(0, \sigma^2 [g'(\theta)]^2\big)
      $$
      \item In this proof, note in the Taylor series expansion, the first order term $g'(\theta)(X_n - \theta)$ dominates. 
      \item However, if $g'(\theta) = 0$, then we run into an issue with this approximation. This leads to the second-order delta method. 
    \end{itemize}
      
    \end{proof}
    }
    
  \end{block}
  
  \framebreak
  
  \begin{block}{Theorem: Second Order Delta Method}
    Let $X_n$ be a sequence of random variables that satisfies $\sqrt{n}(X_n - \theta) \dlim N(0, \sigma^2)$. For a given function $g$ such that the first two derivatives of $g$ exist and $g'(x) = 0$, $g''(x) \neq 0$, then
    $$
    n\big(g(X_n) - g(\theta)\big)\dlim \sigma^2 \frac{g''(\theta)}{2}\chi^2_1,
    $$
    where $\chi^2_1$ is a ``Chi-square" distribution with one degree of freedom.
    
    \mode<article>{
    \begin{proof}
      This proof is just a simple extension of the first-order delta method, so here we give a sketch. Using a second-order Taylor approximation of $g(x)$, we have:
      $$
      g(X_n) = g(\theta) + g'(\theta)(X_n - \theta) + \frac{g''(\theta)}{2}(X_n - \theta)^2 + \text{Remainder}.
      $$
      By assumption, $g'(\theta) = 0$, and therefore
      $$
      g(X_n) - g(\theta) = \frac{g''(\theta)}{2}(X_n - \theta)^2 + \text{Remainder}.
      $$
      As before, we can show that the remainder term converges to zero in probability, at a rate that is faster than $n$. 
      Because the square of a standard normal is Chi-square with one degree of freedom, we have by the continuous mapping theorem
      $$
      \frac{n(X_n - \theta)^2}{\sigma^2} = \left(\frac{\sqrt{n}(X_n - \theta)}{\sigma}\right)^2 \dlim \chi^2_1.
      $$
      Therefore, by multiplying both sides of the Taylor-series expansion by $n$, we have
      $$
      n\big(g(X_n) - g(\theta)\big) = \frac{g''(\theta)}{2}n(X_n - \theta)^2 + n\,\text{Remainder}\dlim \sigma^2 \frac{g''(\theta)}{2}\chi^2_1
      $$
    \end{proof}
    }
    
  \end{block}
  
\end{frame}

% TODO: Add examples of CLT (statistical practice), Delta method, and second-order delta-method.

\newcommand\acknowledgments{
\begin{itemize}
\item   Compiled on {\today} using \Rlanguage version 4.5.2.
\item   \parbox[t]{0.75\textwidth}{Licensed under the \link{http://creativecommons.org/licenses/by-nc/4.0/}{Creative Commons Attribution-NonCommercial license}.
    Please share and remix non-commercially, mentioning its origin.}
    \parbox[c]{1.5cm}{\includegraphics[height=12pt]{../cc-by-nc}}
\item We acknowledge \link{https://jeswheel.github.io/4450_f25/acknowledge.html}{students and instructors for previous versions of this course / slides}.
\end{itemize}
}

\mode<presentation>{
\begin{frame}[allowframebreaks=0.8]{References and Acknowledgements}
  
\bibliography{../bib4450}

\vspace{3mm}

\acknowledgments

\end{frame}
}

\mode<article>{

\newpage

{\bf \Large \noindent Acknowledgments}

\acknowledgments

\newpage

\bibliography{../bib4450}

}



\end{document}







